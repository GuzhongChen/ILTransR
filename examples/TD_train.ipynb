{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T08:50:21.787442Z",
     "start_time": "2021-09-24T08:50:19.265440Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:14:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU\n",
      "[03:14:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from iltransr.models.il_smiles_only import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T08:50:24.017440Z",
     "start_time": "2021-09-24T08:50:24.002441Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_dropout = 0.05\n",
    "batch_size = 128\n",
    "bucket_num, bucket_ratio = 2, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ILNet(dropout=cnn_dropout)\n",
    "net.encoder = model.encoder\n",
    "net.src_embed =  model.src_embed\n",
    "net.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T08:50:24.410440Z",
     "start_time": "2021-09-24T08:50:24.285441Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "TD_database = pd.read_excel(os.path.join(root_path,'datasets/il properties/TDECOMP_V.xlsx'),sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T08:50:25.171439Z",
     "start_time": "2021-09-24T08:50:24.411440Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_smiles = TD_database['IL SMILES'].map(canonical_smile).map(no_split).map(preprocess)\n",
    "train_smiles_lengths = TD_database['IL SMILES'].map(canonical_smile).map(no_split).map(get_length)\n",
    "train_MP =mx.nd.array(TD_database['TDECOMP_Celcius'])\n",
    "train_dataset = gluon.data.SimpleDataset(gluon.data.ArrayDataset(train_smiles,train_MP))\n",
    "train_dataloader = get_train_dataloader(train_dataset,train_smiles_lengths,batch_size=batch_size, bucket_num=bucket_num,bucket_ratio=bucket_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:14:28] ../src/operator/cudnn_ops.cc:292: Auto-tuning cuDNN op, set MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable\n",
      "[03:14:29] ../src/operator/cudnn_ops.cc:292: Auto-tuning cuDNN op, set MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 10/10] avg loss 223.193, throughput 0.0394607K wps\n",
      "[Epoch 0] train avg loss 223.193, train avg r2 -6.13405e+06,throughput 0.0394598K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 1 Batch 10/10] avg loss 75.3483, throughput 0.0624678K wps\n",
      "[Epoch 1] train avg loss 75.3483, train avg r2 -5.96482,throughput 0.0624659K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 2 Batch 10/10] avg loss 65.8754, throughput 0.127358K wps\n",
      "[Epoch 2] train avg loss 65.8754, train avg r2 -4.30314,throughput 0.127347K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 3 Batch 10/10] avg loss 56.9832, throughput 0.121912K wps\n",
      "[Epoch 3] train avg loss 56.9832, train avg r2 -2.11253,throughput 0.121899K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 4 Batch 10/10] avg loss 49.1947, throughput 2.77185K wps\n",
      "[Epoch 4] train avg loss 49.1947, train avg r2 -1.2218,throughput 2.76849K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 5 Batch 10/10] avg loss 44.6037, throughput 2.93262K wps\n",
      "[Epoch 5] train avg loss 44.6037, train avg r2 -0.382827,throughput 2.92935K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 6 Batch 10/10] avg loss 40.5678, throughput 0.0492311K wps\n",
      "[Epoch 6] train avg loss 40.5678, train avg r2 -0.185515,throughput 0.0492302K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 7 Batch 10/10] avg loss 36.0835, throughput 0.863339K wps\n",
      "[Epoch 7] train avg loss 36.0835, train avg r2 0.327152,throughput 0.863051K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 8 Batch 10/10] avg loss 35.5716, throughput 0.914045K wps\n",
      "[Epoch 8] train avg loss 35.5716, train avg r2 0.339683,throughput 0.913847K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 9 Batch 10/10] avg loss 30.6441, throughput 0.880164K wps\n",
      "[Epoch 9] train avg loss 30.6441, train avg r2 0.469425,throughput 0.879764K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 10 Batch 10/10] avg loss 27.9782, throughput 2.8083K wps\n",
      "[Epoch 10] train avg loss 27.9782, train avg r2 0.616103,throughput 2.80508K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 11 Batch 10/10] avg loss 26.1206, throughput 2.8452K wps\n",
      "[Epoch 11] train avg loss 26.1206, train avg r2 0.674982,throughput 2.84277K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 12 Batch 10/10] avg loss 24.5042, throughput 2.80803K wps\n",
      "[Epoch 12] train avg loss 24.5042, train avg r2 0.715442,throughput 2.80081K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 13 Batch 10/10] avg loss 23.3995, throughput 0.116698K wps\n",
      "[Epoch 13] train avg loss 23.3995, train avg r2 0.737554,throughput 0.116692K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 14 Batch 10/10] avg loss 23.3282, throughput 0.237454K wps\n",
      "[Epoch 14] train avg loss 23.3282, train avg r2 0.739615,throughput 0.237424K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 15 Batch 10/10] avg loss 23.8678, throughput 0.0777146K wps\n",
      "[Epoch 15] train avg loss 23.8678, train avg r2 0.752901,throughput 0.0777115K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 16 Batch 10/10] avg loss 22.4293, throughput 2.72845K wps\n",
      "[Epoch 16] train avg loss 22.4293, train avg r2 0.775086,throughput 2.72501K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 17 Batch 10/10] avg loss 21.361, throughput 2.75596K wps\n",
      "[Epoch 17] train avg loss 21.361, train avg r2 0.804264,throughput 2.75294K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 18 Batch 10/10] avg loss 22.2887, throughput 2.68756K wps\n",
      "[Epoch 18] train avg loss 22.2887, train avg r2 0.785086,throughput 2.68456K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 19 Batch 10/10] avg loss 28.2412, throughput 2.79364K wps\n",
      "[Epoch 19] train avg loss 28.2412, train avg r2 0.650961,throughput 2.79076K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 20 Batch 10/10] avg loss 25.8399, throughput 2.69269K wps\n",
      "[Epoch 20] train avg loss 25.8399, train avg r2 0.728078,throughput 2.68922K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 21 Batch 10/10] avg loss 18.0292, throughput 2.71161K wps\n",
      "[Epoch 21] train avg loss 18.0292, train avg r2 0.855628,throughput 2.70868K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 22 Batch 10/10] avg loss 16.7033, throughput 0.369681K wps\n",
      "[Epoch 22] train avg loss 16.7033, train avg r2 0.862162,throughput 0.36963K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 23 Batch 10/10] avg loss 16.1445, throughput 2.69028K wps\n",
      "[Epoch 23] train avg loss 16.1445, train avg r2 0.874759,throughput 2.6856K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 24 Batch 10/10] avg loss 15.541, throughput 2.81731K wps\n",
      "[Epoch 24] train avg loss 15.541, train avg r2 0.878771,throughput 2.81094K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 25 Batch 10/10] avg loss 15.2889, throughput 2.85502K wps\n",
      "[Epoch 25] train avg loss 15.2889, train avg r2 0.883707,throughput 2.85192K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 26 Batch 10/10] avg loss 13.9624, throughput 2.85988K wps\n",
      "[Epoch 26] train avg loss 13.9624, train avg r2 0.895744,throughput 2.85699K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 27 Batch 10/10] avg loss 13.8234, throughput 2.76323K wps\n",
      "[Epoch 27] train avg loss 13.8234, train avg r2 0.897305,throughput 2.76049K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 28 Batch 10/10] avg loss 13.5947, throughput 2.85401K wps\n",
      "[Epoch 28] train avg loss 13.5947, train avg r2 0.903422,throughput 2.85145K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 29 Batch 10/10] avg loss 13.2855, throughput 2.94765K wps\n",
      "[Epoch 29] train avg loss 13.2855, train avg r2 0.90298,throughput 2.94384K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 30 Batch 10/10] avg loss 12.27, throughput 2.86539K wps\n",
      "[Epoch 30] train avg loss 12.27, train avg r2 0.91318,throughput 2.86197K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 31 Batch 10/10] avg loss 12.1309, throughput 2.88901K wps\n",
      "[Epoch 31] train avg loss 12.1309, train avg r2 0.917667,throughput 2.88421K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 32 Batch 10/10] avg loss 12.1676, throughput 2.79607K wps\n",
      "[Epoch 32] train avg loss 12.1676, train avg r2 0.916654,throughput 2.79305K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 33 Batch 10/10] avg loss 11.0635, throughput 2.80961K wps\n",
      "[Epoch 33] train avg loss 11.0635, train avg r2 0.921512,throughput 2.8061K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 34 Batch 10/10] avg loss 10.7604, throughput 2.92631K wps\n",
      "[Epoch 34] train avg loss 10.7604, train avg r2 0.926028,throughput 2.92169K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 35 Batch 10/10] avg loss 10.6681, throughput 2.79288K wps\n",
      "[Epoch 35] train avg loss 10.6681, train avg r2 0.925525,throughput 2.78973K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 36 Batch 10/10] avg loss 10.9256, throughput 2.85498K wps\n",
      "[Epoch 36] train avg loss 10.9256, train avg r2 0.922938,throughput 2.8521K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 37 Batch 10/10] avg loss 11.4074, throughput 2.85442K wps\n",
      "[Epoch 37] train avg loss 11.4074, train avg r2 0.926489,throughput 2.85287K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 38 Batch 10/10] avg loss 10.7225, throughput 2.74395K wps\n",
      "[Epoch 38] train avg loss 10.7225, train avg r2 0.926871,throughput 2.7413K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 39 Batch 10/10] avg loss 10.7153, throughput 2.7577K wps\n",
      "[Epoch 39] train avg loss 10.7153, train avg r2 0.929904,throughput 2.75477K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 40 Batch 10/10] avg loss 10.121, throughput 2.72275K wps\n",
      "[Epoch 40] train avg loss 10.121, train avg r2 0.932607,throughput 2.71796K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 41 Batch 10/10] avg loss 9.53666, throughput 2.76067K wps\n",
      "[Epoch 41] train avg loss 9.53666, train avg r2 0.933228,throughput 2.75711K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 42 Batch 10/10] avg loss 9.29, throughput 2.89925K wps\n",
      "[Epoch 42] train avg loss 9.29, train avg r2 0.934451,throughput 2.89265K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 43 Batch 10/10] avg loss 9.22133, throughput 2.76877K wps\n",
      "[Epoch 43] train avg loss 9.22133, train avg r2 0.936704,throughput 2.76621K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 44 Batch 10/10] avg loss 9.2367, throughput 2.80643K wps\n",
      "[Epoch 44] train avg loss 9.2367, train avg r2 0.937661,throughput 2.80344K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 45 Batch 10/10] avg loss 8.91064, throughput 2.85508K wps\n",
      "[Epoch 45] train avg loss 8.91064, train avg r2 0.93794,throughput 2.85177K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 46 Batch 10/10] avg loss 8.89763, throughput 2.86346K wps\n",
      "[Epoch 46] train avg loss 8.89763, train avg r2 0.939202,throughput 2.85898K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 47 Batch 10/10] avg loss 8.72236, throughput 2.84998K wps\n",
      "[Epoch 47] train avg loss 8.72236, train avg r2 0.937813,throughput 2.8486K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 48 Batch 10/10] avg loss 8.57672, throughput 2.89211K wps\n",
      "[Epoch 48] train avg loss 8.57672, train avg r2 0.939469,throughput 2.88869K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 49 Batch 10/10] avg loss 8.45647, throughput 2.80305K wps\n",
      "[Epoch 49] train avg loss 8.45647, train avg r2 0.940446,throughput 2.80054K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 50 Batch 10/10] avg loss 8.22424, throughput 2.86987K wps\n",
      "[Epoch 50] train avg loss 8.22424, train avg r2 0.940519,throughput 2.86719K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 51 Batch 10/10] avg loss 8.15145, throughput 2.88658K wps\n",
      "[Epoch 51] train avg loss 8.15145, train avg r2 0.940304,throughput 2.8814K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 52 Batch 10/10] avg loss 8.2182, throughput 2.83035K wps\n",
      "[Epoch 52] train avg loss 8.2182, train avg r2 0.942636,throughput 2.82751K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 53 Batch 10/10] avg loss 8.05038, throughput 2.81492K wps\n",
      "[Epoch 53] train avg loss 8.05038, train avg r2 0.944218,throughput 2.81205K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 54 Batch 10/10] avg loss 8.04998, throughput 2.79722K wps\n",
      "[Epoch 54] train avg loss 8.04998, train avg r2 0.943051,throughput 2.79476K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 55 Batch 10/10] avg loss 7.99872, throughput 2.79309K wps\n",
      "[Epoch 55] train avg loss 7.99872, train avg r2 0.942369,throughput 2.79072K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 56 Batch 10/10] avg loss 7.94118, throughput 2.86065K wps\n",
      "[Epoch 56] train avg loss 7.94118, train avg r2 0.944417,throughput 2.8579K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 57 Batch 10/10] avg loss 7.85388, throughput 2.84673K wps\n",
      "[Epoch 57] train avg loss 7.85388, train avg r2 0.943547,throughput 2.84399K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 58 Batch 10/10] avg loss 7.73947, throughput 2.86389K wps\n",
      "[Epoch 58] train avg loss 7.73947, train avg r2 0.94345,throughput 2.86134K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 59 Batch 10/10] avg loss 7.7333, throughput 2.80235K wps\n",
      "[Epoch 59] train avg loss 7.7333, train avg r2 0.943559,throughput 2.79981K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 60 Batch 10/10] avg loss 7.63284, throughput 2.79686K wps\n",
      "[Epoch 60] train avg loss 7.63284, train avg r2 0.944719,throughput 2.79505K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 61 Batch 10/10] avg loss 7.60954, throughput 2.71307K wps\n",
      "[Epoch 61] train avg loss 7.60954, train avg r2 0.944887,throughput 2.71006K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 62 Batch 10/10] avg loss 7.57685, throughput 2.66431K wps\n",
      "[Epoch 62] train avg loss 7.57685, train avg r2 0.942889,throughput 2.66175K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 63 Batch 10/10] avg loss 7.54645, throughput 2.76348K wps\n",
      "[Epoch 63] train avg loss 7.54645, train avg r2 0.944907,throughput 2.75855K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 64 Batch 10/10] avg loss 7.52852, throughput 2.82792K wps\n",
      "[Epoch 64] train avg loss 7.52852, train avg r2 0.944058,throughput 2.82449K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 65 Batch 10/10] avg loss 7.47647, throughput 2.76564K wps\n",
      "[Epoch 65] train avg loss 7.47647, train avg r2 0.94343,throughput 2.76207K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 66 Batch 10/10] avg loss 7.50003, throughput 2.77305K wps\n",
      "[Epoch 66] train avg loss 7.50003, train avg r2 0.944174,throughput 2.76924K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 67 Batch 10/10] avg loss 7.58003, throughput 2.88849K wps\n",
      "[Epoch 67] train avg loss 7.58003, train avg r2 0.945041,throughput 2.882K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 68 Batch 10/10] avg loss 7.50254, throughput 2.89816K wps\n",
      "[Epoch 68] train avg loss 7.50254, train avg r2 0.945108,throughput 2.8968K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 69 Batch 10/10] avg loss 7.40373, throughput 2.79465K wps\n",
      "[Epoch 69] train avg loss 7.40373, train avg r2 0.944438,throughput 2.78969K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 70 Batch 10/10] avg loss 7.37859, throughput 2.69702K wps\n",
      "[Epoch 70] train avg loss 7.37859, train avg r2 0.944673,throughput 2.69375K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 71 Batch 10/10] avg loss 7.36127, throughput 2.7057K wps\n",
      "[Epoch 71] train avg loss 7.36127, train avg r2 0.944196,throughput 2.70284K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 72 Batch 10/10] avg loss 7.3494, throughput 2.70595K wps\n",
      "[Epoch 72] train avg loss 7.3494, train avg r2 0.946239,throughput 2.7034K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 73 Batch 10/10] avg loss 7.31112, throughput 2.64947K wps\n",
      "[Epoch 73] train avg loss 7.31112, train avg r2 0.943548,throughput 2.64612K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 74 Batch 10/10] avg loss 7.32136, throughput 2.56576K wps\n",
      "[Epoch 74] train avg loss 7.32136, train avg r2 0.944736,throughput 2.56267K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 75 Batch 10/10] avg loss 7.33196, throughput 2.61268K wps\n",
      "[Epoch 75] train avg loss 7.33196, train avg r2 0.94429,throughput 2.60974K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 76 Batch 10/10] avg loss 7.35237, throughput 2.70417K wps\n",
      "[Epoch 76] train avg loss 7.35237, train avg r2 0.944704,throughput 2.70139K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 77 Batch 10/10] avg loss 7.3868, throughput 2.65208K wps\n",
      "[Epoch 77] train avg loss 7.3868, train avg r2 0.945877,throughput 2.64737K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 78 Batch 10/10] avg loss 7.32517, throughput 2.8399K wps\n",
      "[Epoch 78] train avg loss 7.32517, train avg r2 0.945962,throughput 2.83434K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 79 Batch 10/10] avg loss 7.25924, throughput 2.81147K wps\n",
      "[Epoch 79] train avg loss 7.25924, train avg r2 0.945953,throughput 2.80896K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 80 Batch 10/10] avg loss 7.23983, throughput 2.85279K wps\n",
      "[Epoch 80] train avg loss 7.23983, train avg r2 0.94728,throughput 2.8469K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 81 Batch 10/10] avg loss 7.23129, throughput 2.82965K wps\n",
      "[Epoch 81] train avg loss 7.23129, train avg r2 0.945458,throughput 2.82551K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 82 Batch 10/10] avg loss 7.21928, throughput 2.81547K wps\n",
      "[Epoch 82] train avg loss 7.21928, train avg r2 0.946363,throughput 2.81273K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 83 Batch 10/10] avg loss 7.20666, throughput 2.87651K wps\n",
      "[Epoch 83] train avg loss 7.20666, train avg r2 0.947502,throughput 2.87204K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 84 Batch 10/10] avg loss 7.19148, throughput 2.75194K wps\n",
      "[Epoch 84] train avg loss 7.19148, train avg r2 0.945449,throughput 2.74903K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 85 Batch 10/10] avg loss 7.17731, throughput 0.125395K wps\n",
      "[Epoch 85] train avg loss 7.17731, train avg r2 0.94524,throughput 0.125387K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 86 Batch 10/10] avg loss 7.17144, throughput 2.47901K wps\n",
      "[Epoch 86] train avg loss 7.17144, train avg r2 0.945369,throughput 2.4755K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 87 Batch 10/10] avg loss 7.16795, throughput 2.78221K wps\n",
      "[Epoch 87] train avg loss 7.16795, train avg r2 0.947521,throughput 2.77726K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 88 Batch 10/10] avg loss 7.16427, throughput 2.63036K wps\n",
      "[Epoch 88] train avg loss 7.16427, train avg r2 0.946065,throughput 2.62536K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 89 Batch 10/10] avg loss 7.16002, throughput 2.78255K wps\n",
      "[Epoch 89] train avg loss 7.16002, train avg r2 0.9466,throughput 2.77957K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 90 Batch 10/10] avg loss 7.14924, throughput 2.79388K wps\n",
      "[Epoch 90] train avg loss 7.14924, train avg r2 0.942584,throughput 2.79008K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 91 Batch 10/10] avg loss 7.13973, throughput 2.78773K wps\n",
      "[Epoch 91] train avg loss 7.13973, train avg r2 0.946687,throughput 2.78379K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 92 Batch 10/10] avg loss 7.13191, throughput 2.75872K wps\n",
      "[Epoch 92] train avg loss 7.13191, train avg r2 0.945369,throughput 2.75564K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 93 Batch 10/10] avg loss 7.12954, throughput 2.73435K wps\n",
      "[Epoch 93] train avg loss 7.12954, train avg r2 0.946932,throughput 2.7315K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 94 Batch 10/10] avg loss 7.1261, throughput 2.81791K wps\n",
      "[Epoch 94] train avg loss 7.1261, train avg r2 0.945929,throughput 2.81479K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 95 Batch 10/10] avg loss 7.11721, throughput 2.65802K wps\n",
      "[Epoch 95] train avg loss 7.11721, train avg r2 0.944718,throughput 2.65503K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 96 Batch 10/10] avg loss 7.11571, throughput 2.67063K wps\n",
      "[Epoch 96] train avg loss 7.11571, train avg r2 0.946887,throughput 2.66784K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 97 Batch 10/10] avg loss 7.1141, throughput 2.7224K wps\n",
      "[Epoch 97] train avg loss 7.1141, train avg r2 0.945973,throughput 2.71765K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 98 Batch 10/10] avg loss 7.11737, throughput 2.78487K wps\n",
      "[Epoch 98] train avg loss 7.11737, train avg r2 0.947063,throughput 2.7814K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 99 Batch 10/10] avg loss 7.10868, throughput 2.83361K wps\n",
      "[Epoch 99] train avg loss 7.10868, train avg r2 0.946049,throughput 2.83197K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 100 Batch 10/10] avg loss 7.10552, throughput 2.95159K wps\n",
      "[Epoch 100] train avg loss 7.10552, train avg r2 0.945699,throughput 2.94713K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 101 Batch 10/10] avg loss 7.09901, throughput 2.76365K wps\n",
      "[Epoch 101] train avg loss 7.09901, train avg r2 0.945443,throughput 2.75649K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 102 Batch 10/10] avg loss 7.09732, throughput 2.85168K wps\n",
      "[Epoch 102] train avg loss 7.09732, train avg r2 0.947035,throughput 2.84552K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 103 Batch 10/10] avg loss 7.09586, throughput 0.0749082K wps\n",
      "[Epoch 103] train avg loss 7.09586, train avg r2 0.945605,throughput 0.0749053K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 104 Batch 10/10] avg loss 7.09464, throughput 2.33042K wps\n",
      "[Epoch 104] train avg loss 7.09464, train avg r2 0.94664,throughput 2.3283K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 105 Batch 10/10] avg loss 7.09107, throughput 2.69839K wps\n",
      "[Epoch 105] train avg loss 7.09107, train avg r2 0.94713,throughput 2.69291K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 106 Batch 10/10] avg loss 7.08954, throughput 2.62956K wps\n",
      "[Epoch 106] train avg loss 7.08954, train avg r2 0.947126,throughput 2.62569K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 107 Batch 10/10] avg loss 7.09059, throughput 2.50019K wps\n",
      "[Epoch 107] train avg loss 7.09059, train avg r2 0.945035,throughput 2.49506K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 108 Batch 10/10] avg loss 7.08752, throughput 2.55729K wps\n",
      "[Epoch 108] train avg loss 7.08752, train avg r2 0.946709,throughput 2.55459K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 109 Batch 10/10] avg loss 7.09225, throughput 2.48192K wps\n",
      "[Epoch 109] train avg loss 7.09225, train avg r2 0.946528,throughput 2.47911K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 110 Batch 10/10] avg loss 7.08706, throughput 2.50341K wps\n",
      "[Epoch 110] train avg loss 7.08706, train avg r2 0.946067,throughput 2.50094K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 111 Batch 10/10] avg loss 7.08463, throughput 2.41987K wps\n",
      "[Epoch 111] train avg loss 7.08463, train avg r2 0.946093,throughput 2.41735K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 112 Batch 10/10] avg loss 7.08291, throughput 2.45517K wps\n",
      "[Epoch 112] train avg loss 7.08291, train avg r2 0.945333,throughput 2.4525K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 113 Batch 10/10] avg loss 7.08153, throughput 2.53662K wps\n",
      "[Epoch 113] train avg loss 7.08153, train avg r2 0.948664,throughput 2.53235K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 114 Batch 10/10] avg loss 7.08106, throughput 2.71596K wps\n",
      "[Epoch 114] train avg loss 7.08106, train avg r2 0.947455,throughput 2.71454K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 115 Batch 10/10] avg loss 7.07912, throughput 2.78802K wps\n",
      "[Epoch 115] train avg loss 7.07912, train avg r2 0.946776,throughput 2.78491K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 116 Batch 10/10] avg loss 7.07734, throughput 2.67019K wps\n",
      "[Epoch 116] train avg loss 7.07734, train avg r2 0.946019,throughput 2.66667K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 117 Batch 10/10] avg loss 7.0757, throughput 2.69974K wps\n",
      "[Epoch 117] train avg loss 7.0757, train avg r2 0.945188,throughput 2.69712K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 118 Batch 10/10] avg loss 7.0766, throughput 2.78646K wps\n",
      "[Epoch 118] train avg loss 7.0766, train avg r2 0.946336,throughput 2.78259K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 119 Batch 10/10] avg loss 7.07568, throughput 2.77668K wps\n",
      "[Epoch 119] train avg loss 7.07568, train avg r2 0.944087,throughput 2.7737K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 120 Batch 10/10] avg loss 7.07351, throughput 2.84949K wps\n",
      "[Epoch 120] train avg loss 7.07351, train avg r2 0.946499,throughput 2.84662K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 121 Batch 10/10] avg loss 7.07457, throughput 2.8734K wps\n",
      "[Epoch 121] train avg loss 7.07457, train avg r2 0.944838,throughput 2.86617K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 122 Batch 10/10] avg loss 7.07174, throughput 2.849K wps\n",
      "[Epoch 122] train avg loss 7.07174, train avg r2 0.946277,throughput 2.84552K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 123 Batch 10/10] avg loss 7.07156, throughput 2.8011K wps\n",
      "[Epoch 123] train avg loss 7.07156, train avg r2 0.94482,throughput 2.79834K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 124 Batch 10/10] avg loss 7.07195, throughput 2.78263K wps\n",
      "[Epoch 124] train avg loss 7.07195, train avg r2 0.944243,throughput 2.77974K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 125 Batch 10/10] avg loss 7.07266, throughput 2.90539K wps\n",
      "[Epoch 125] train avg loss 7.07266, train avg r2 0.946023,throughput 2.90075K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 126 Batch 10/10] avg loss 7.07288, throughput 2.73387K wps\n",
      "[Epoch 126] train avg loss 7.07288, train avg r2 0.945962,throughput 2.7308K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 127 Batch 10/10] avg loss 7.07111, throughput 2.80019K wps\n",
      "[Epoch 127] train avg loss 7.07111, train avg r2 0.946481,throughput 2.7964K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 128 Batch 10/10] avg loss 7.0715, throughput 2.69851K wps\n",
      "[Epoch 128] train avg loss 7.0715, train avg r2 0.946119,throughput 2.69363K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 129 Batch 10/10] avg loss 7.06896, throughput 2.74078K wps\n",
      "[Epoch 129] train avg loss 7.06896, train avg r2 0.947637,throughput 2.73678K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 130 Batch 10/10] avg loss 7.06917, throughput 2.75439K wps\n",
      "[Epoch 130] train avg loss 7.06917, train avg r2 0.947211,throughput 2.75283K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 131 Batch 10/10] avg loss 7.0686, throughput 2.88865K wps\n",
      "[Epoch 131] train avg loss 7.0686, train avg r2 0.946687,throughput 2.88503K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 132 Batch 10/10] avg loss 7.07152, throughput 2.68827K wps\n",
      "[Epoch 132] train avg loss 7.07152, train avg r2 0.945875,throughput 2.68265K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 133 Batch 10/10] avg loss 7.06868, throughput 2.75652K wps\n",
      "[Epoch 133] train avg loss 7.06868, train avg r2 0.94757,throughput 2.75363K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 134 Batch 10/10] avg loss 7.06815, throughput 2.78744K wps\n",
      "[Epoch 134] train avg loss 7.06815, train avg r2 0.947168,throughput 2.78334K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 135 Batch 10/10] avg loss 7.06777, throughput 2.77944K wps\n",
      "[Epoch 135] train avg loss 7.06777, train avg r2 0.946979,throughput 2.77397K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 136 Batch 10/10] avg loss 7.0682, throughput 2.81932K wps\n",
      "[Epoch 136] train avg loss 7.0682, train avg r2 0.946987,throughput 2.81614K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 137 Batch 10/10] avg loss 7.06849, throughput 2.84302K wps\n",
      "[Epoch 137] train avg loss 7.06849, train avg r2 0.947567,throughput 2.83957K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 138 Batch 10/10] avg loss 7.06836, throughput 2.89635K wps\n",
      "[Epoch 138] train avg loss 7.06836, train avg r2 0.947625,throughput 2.89278K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 139 Batch 10/10] avg loss 7.0687, throughput 2.73974K wps\n",
      "[Epoch 139] train avg loss 7.0687, train avg r2 0.945223,throughput 2.73607K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 140 Batch 10/10] avg loss 7.06802, throughput 2.77682K wps\n",
      "[Epoch 140] train avg loss 7.06802, train avg r2 0.948937,throughput 2.76964K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 141 Batch 10/10] avg loss 7.06624, throughput 2.7015K wps\n",
      "[Epoch 141] train avg loss 7.06624, train avg r2 0.946397,throughput 2.69847K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 142 Batch 10/10] avg loss 7.06729, throughput 2.65805K wps\n",
      "[Epoch 142] train avg loss 7.06729, train avg r2 0.943976,throughput 2.65515K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 143 Batch 10/10] avg loss 7.06687, throughput 2.68375K wps\n",
      "[Epoch 143] train avg loss 7.06687, train avg r2 0.945515,throughput 2.68012K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 144 Batch 10/10] avg loss 7.06706, throughput 2.83309K wps\n",
      "[Epoch 144] train avg loss 7.06706, train avg r2 0.946485,throughput 2.82983K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 145 Batch 10/10] avg loss 7.06633, throughput 2.71849K wps\n",
      "[Epoch 145] train avg loss 7.06633, train avg r2 0.945436,throughput 2.71493K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 146 Batch 10/10] avg loss 7.06797, throughput 2.7556K wps\n",
      "[Epoch 146] train avg loss 7.06797, train avg r2 0.946344,throughput 2.75182K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 147 Batch 10/10] avg loss 7.06912, throughput 2.74849K wps\n",
      "[Epoch 147] train avg loss 7.06912, train avg r2 0.946003,throughput 2.74476K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 148 Batch 10/10] avg loss 7.06635, throughput 2.77296K wps\n",
      "[Epoch 148] train avg loss 7.06635, train avg r2 0.948717,throughput 2.76839K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 149 Batch 10/10] avg loss 7.06658, throughput 2.85365K wps\n",
      "[Epoch 149] train avg loss 7.06658, train avg r2 0.945511,throughput 2.8499K wps\n",
      "learning rate: 6.103515625e-08\n",
      "Total time cost 185.24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.066577627536028, 0.9455105937460294)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs, learning_rate = 150,0.001                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "max_len = 100\n",
    "save_name = 'TD_best.params'\n",
    "train(net, train_dataloader, batch_size, learning_rate, ctx, epochs, save_name=save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
