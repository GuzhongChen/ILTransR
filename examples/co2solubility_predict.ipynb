{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:26.719687Z",
     "start_time": "2021-09-24T01:28:24.164690Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon,nd,autograd,npx\n",
    "import gluonnlp as nlp\n",
    "import nmt\n",
    "from gluonnlp.model.transformer import ParallelTransformer, get_transformer_encoder_decoder\n",
    "import pandas as pd \n",
    "nlp.utils.check_version('0.7.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:26.735687Z",
     "start_time": "2021-09-24T01:28:26.720688Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "random.seed(101)\n",
    "mx.random.seed(10001)\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "# parameters for dataset\n",
    "dataset = 'pubchem'\n",
    "src_lang, tgt_lang = 'random_smiles', 'rdkit_canonical_smiles'\n",
    "src_max_len, tgt_max_len = 100, 100\n",
    "\n",
    "# parameters for model\n",
    "num_units=128\n",
    "hidden_size=1024\n",
    "dropout=0\n",
    "epsilon=0.1\n",
    "num_layers=3\n",
    "num_heads=4\n",
    "scaled=True\n",
    "share_embed=True\n",
    "embed_size=128\n",
    "tie_weights=True\n",
    "embed_initializer=None\n",
    "magnitude = 3.0\n",
    "lr_update_factor = 0.5\n",
    "param_file = 'co2/co2_best.params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:26.751687Z",
     "start_time": "2021-09-24T01:28:26.736690Z"
    }
   },
   "outputs": [],
   "source": [
    "def _load_vocab(file_path, **kwargs):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return nlp.Vocab.from_json(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:26.767689Z",
     "start_time": "2021-09-24T01:28:26.752689Z"
    }
   },
   "outputs": [],
   "source": [
    "src_vocab = _load_vocab('pubchem/vocab.random_smiles.json')\n",
    "tgt_vocab = _load_vocab('pubchem/vocab.rdkit_canonical_smiles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:26.798687Z",
     "start_time": "2021-09-24T01:28:26.768690Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:01:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder3, decoder3, one_step_ahead_decoder3 = get_transformer_encoder_decoder(\n",
    "    units=num_units,\n",
    "    hidden_size=hidden_size,\n",
    "    dropout=dropout,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    max_src_length=src_max_len,\n",
    "    max_tgt_length=tgt_max_len,\n",
    "    scaled=scaled,prefix='transformer_3')\n",
    "\n",
    "model3 = nlp.model.translation.NMTModel(src_vocab=src_vocab,\n",
    "                 tgt_vocab=tgt_vocab,\n",
    "                 encoder=encoder3,\n",
    "                 decoder=decoder3,\n",
    "                 one_step_ahead_decoder=one_step_ahead_decoder3,\n",
    "                 embed_size=num_units,\n",
    "                 embed_initializer=None,\n",
    "                 prefix='transformer_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:49:19.490024Z",
     "start_time": "2020-09-02T08:49:19.487020Z"
    }
   },
   "source": [
    "model.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:49:19.498031Z",
     "start_time": "2020-09-02T08:49:19.491025Z"
    }
   },
   "source": [
    "def encode(model, src_seq, src_vocab,ctx):\n",
    "    src_sentence = src_vocab[src_seq.split()]\n",
    "    src_sentence.append(src_vocab[src_vocab.eos_token])\n",
    "    src_npy = np.array(src_sentence, dtype=np.int32)\n",
    "    src_nd = mx.nd.array(src_npy)\n",
    "    src_nd = src_nd.reshape((1, -1)).as_in_context(ctx)\n",
    "    src_valid_length = mx.nd.array([src_nd.shape[1]]).as_in_context(ctx)\n",
    "    enc_outputs = model.encode(src_nd,valid_length=src_valid_length)\n",
    "    return enc_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:49:19.523053Z",
     "start_time": "2020-09-02T08:49:19.499032Z"
    }
   },
   "source": [
    "for sentence in ['c 1 ( N = C ( N ) N ) s c c ( - c 2 c c ( C ) n ( C ) c 2 ) n 1', 'C ( C ( c 1 c c c ( C ( N O ) = O ) c c 1 ) C C ) C']:\n",
    "    e = encode(model, sentence,src_vocab,ctx)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.874687Z",
     "start_time": "2021-09-24T01:28:28.843688Z"
    }
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "def canonical_smile(sml):\n",
    "    try:\n",
    "        m = Chem.MolFromSmiles(sml)\n",
    "        #return Chem.MolToSmiles(m, canonical=True,isomericSmiles=False)\n",
    "        return Chem.MolToSmiles(m, canonical=True,isomericSmiles=True)\n",
    "    except:\n",
    "        print(sml)\n",
    "        return float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.890687Z",
     "start_time": "2021-09-24T01:28:28.875688Z"
    }
   },
   "outputs": [],
   "source": [
    "def no_split(sm):\n",
    "    arr = []\n",
    "    i = 0\n",
    "    try:\n",
    "        len(sm)\n",
    "    except:\n",
    "        print(sm)\n",
    "    while i < len(sm)-1:\n",
    "        arr.append(sm[i])\n",
    "        i += 1\n",
    "    if i == len(sm)-1:\n",
    "        arr.append(sm[i])\n",
    "    return ' '.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.906687Z",
     "start_time": "2021-09-24T01:28:28.891688Z"
    }
   },
   "outputs": [],
   "source": [
    "length_clip = nlp.data.ClipSequence(100)\n",
    "# Helper function to preprocess a single data point\n",
    "def preprocess(data):\n",
    "    # A token index or a list of token indices is\n",
    "    # returned according to the vocabulary.\n",
    "    src_sentence = src_vocab[length_clip(data.split())]\n",
    "    src_sentence.append(src_vocab[src_vocab.eos_token])\n",
    "    src_npy = np.array(src_sentence, dtype=np.int32)\n",
    "    src_nd = mx.nd.array(src_npy)\n",
    "    return src_nd\n",
    "\n",
    "# Helper function for getting the length\n",
    "def get_length(x):\n",
    "    return float(len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.922687Z",
     "start_time": "2021-09-24T01:28:28.907688Z"
    }
   },
   "outputs": [],
   "source": [
    "dropout = 0\n",
    "train_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.938687Z",
     "start_time": "2021-09-24T01:28:28.923688Z"
    }
   },
   "outputs": [],
   "source": [
    "class ILNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self,\n",
    "                 dropout,\n",
    "                 src_vocab=src_vocab,\n",
    "                 embed_size=embed_size,\n",
    "                 output_size=1,\n",
    "                 num_filters=(100, 200, 200, 200, 200, 100,100),\n",
    "                 ngram_filter_sizes=(1, 2, 3, 4, 5, 6,7),\n",
    "                 IL_num_filters=(100, 200, 200, 200, 200, 100, 100, 100, 100,100, 160),\n",
    "                 IL_ngram_filter_sizes=(1, 2, 3,4, 5, 6, 7, 8, 9, 10, 15),\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(ILNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            \n",
    "            self.num_filters = num_filters\n",
    "            self.IL_num_filters = IL_num_filters\n",
    "            '''\n",
    "            self.cation_src_embed = None\n",
    "            self.cation_encoder = None\n",
    "            self.cation_textcnn = nlp.model.ConvolutionalEncoder(\n",
    "                embed_size=embed_size,\n",
    "                num_filters=num_filters,\n",
    "                ngram_filter_sizes=ngram_filter_sizes,\n",
    "                conv_layer_activation='relu',\n",
    "                num_highway=1)\n",
    "            self.cation_dropout = gluon.nn.Dropout(dropout)\n",
    "\n",
    "            self.anion_src_embed = None\n",
    "            self.anion_encoder = None\n",
    "            self.anion_textcnn = nlp.model.ConvolutionalEncoder(\n",
    "                embed_size=embed_size,\n",
    "                num_filters=num_filters,\n",
    "                ngram_filter_sizes=ngram_filter_sizes,\n",
    "                conv_layer_activation='relu',\n",
    "                num_highway=1)\n",
    "            self.anion_dropuot = gluon.nn.Dropout(dropout)\n",
    "            '''\n",
    "            self.IL_src_embed = None\n",
    "            self.IL_encoder = None\n",
    "            self.IL_textcnn = nlp.model.ConvolutionalEncoder(\n",
    "                embed_size=embed_size,\n",
    "                num_filters=IL_num_filters,\n",
    "                ngram_filter_sizes=IL_ngram_filter_sizes,\n",
    "                conv_layer_activation='relu',\n",
    "                num_highway=1)\n",
    "            #self.IL_batchnorm = gluon.nn.BatchNorm()\n",
    "            '''\n",
    "            self.mlp = gluon.nn.HybridSequential()\n",
    "            with self.mlp.name_scope():\n",
    "                #self.mlp.add(gluon.nn.Dropout(dropout))\n",
    "                self.mlp.add(gluon.nn.Dense(1024))\n",
    "                #self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                #self.mlp.add(gluon.nn.Dropout(dropout))\n",
    "                \n",
    "                self.mlp.add(gluon.nn.Dense(2048))\n",
    "                self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                \n",
    "                self.mlp.add(gluon.nn.Dense(1024))\n",
    "                self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                \n",
    "                self.mlp.add(gluon.nn.Dense(512))\n",
    "                self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                self.mlp.add(gluon.nn.Dense(256))\n",
    "                self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                '''\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                #self.output.add(gluon.nn.Dropout(dropout))\n",
    "                #self.output.add(gluon.nn.Dense(2048))\n",
    "                #self.output.add(gluon.nn.Activation('relu'))\n",
    "                #self.output.add(gluon.nn.Dropout(dropout))\n",
    "                self.output.add(gluon.nn.Dense(1024))\n",
    "                self.output.add(gluon.nn.Activation('relu'))\n",
    "                self.output.add(gluon.nn.Dropout(dropout))\n",
    "                #self.output.add(gluon.nn.Dropout(dropout))\n",
    "                self.output.add(gluon.nn.Dense(512))\n",
    "                self.output.add(gluon.nn.Activation('relu'))\n",
    "                #self.output.add(gluon.nn.Dropout(dropout))\n",
    "                #self.output.add(gluon.nn.Dense(256))\n",
    "                #self.output.add(gluon.nn.Activation('relu'))\n",
    "                #self.output.add(gluon.nn.Dropout(dropout))\n",
    "                \n",
    "                \n",
    "                self.output.add(gluon.nn.Dense(output_size, flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F, IL_src_nd, IL_valid_length, T, P):  # pylint: disable=arguments-differ\n",
    "        '''\n",
    "        cation_src_embed_ = self.cation_src_embed(cation_src_nd)\n",
    "        cation_encoded, _ = self.cation_encoder(\n",
    "            cation_src_embed_,\n",
    "            valid_length=cation_valid_length)  # Shape(T, N, C)\n",
    "        cation_textcnn = self.cation_textcnn(\n",
    "            F.transpose(cation_encoded, axes=(1, 0, 2)))\n",
    "        cation_textcnn = self.cation_dropout(cation_textcnn)\n",
    "\n",
    "        anion_src_embed_ = self.anion_src_embed(anion_src_nd)\n",
    "        anion_encoded, _ = self.anion_encoder(\n",
    "            anion_src_embed_,\n",
    "            valid_length=anion_valid_length)  # Shape(T, N, C)\n",
    "        anion_textcnn = self.anion_textcnn(\n",
    "            F.transpose(anion_encoded, axes=(1, 0, 2)))\n",
    "        anion_textcnn = self.anion_dropuot(anion_textcnn)\n",
    "        '''\n",
    "        IL_src_embed_ = self.IL_src_embed(IL_src_nd)\n",
    "        IL_encoded, _ = self.IL_encoder(\n",
    "            IL_src_embed_,\n",
    "            valid_length=IL_valid_length)  # Shape(T, N, C)\n",
    "        IL_textcnn = self.IL_textcnn(\n",
    "            F.transpose(IL_encoded, axes=(1, 0, 2)))\n",
    "        #IL_textcnn = self.IL_batchnorm(IL_textcnn)\n",
    "        \n",
    "        T_ = F.reshape(T, shape=(-1, 1))\n",
    "        P_ = F.reshape(P, shape=(-1, 1))\n",
    "        \n",
    "        input_vecs = mx.symbol.concat(\n",
    "            F.reshape(IL_textcnn,\n",
    "                      shape=(-1, sum(self.IL_num_filters))),T_, P_)\n",
    "        \n",
    "        #mlp_out = self.mlp(IL_textcnn)\n",
    "\n",
    "        \n",
    "\n",
    "        #add_temp_press = mx.symbol.concat(IL_textcnn, T_, P_)\n",
    "        out = self.output(input_vecs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T08:19:36.073306Z",
     "start_time": "2021-03-19T08:19:36.058309Z"
    }
   },
   "source": [
    "def get_residual_block(prefix='res_block_', hidden=64):\n",
    "    block = gluon.nn.HybridSequential(prefix=prefix)\n",
    "    with block.name_scope():\n",
    "        block.add(gluon.nn.Dense(hidden, activation='relu', prefix='d1_'),\n",
    "                  gluon.nn.Dropout(0, prefix='dropout_'),\n",
    "                  gluon.nn.Dense(hidden, prefix='d2_'))\n",
    "    return block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T08:19:36.088307Z",
     "start_time": "2021-03-19T08:19:36.074309Z"
    }
   },
   "source": [
    "class ILNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self,\n",
    "                 dropout,\n",
    "                 src_vocab=src_vocab,\n",
    "                 embed_size=embed_size,\n",
    "                 output_size=1,\n",
    "                 IL_num_filters=(100, 200, 200, 200, 200, 100, 100, 100, 100,100, 160, 160),\n",
    "                 IL_ngram_filter_sizes=(1, 2, 3,4, 5, 6, 7, 8, 9, 10, 15, 20),\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(ILNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            \n",
    "            self.IL_num_filters = IL_num_filters\n",
    "\n",
    "            self.IL_src_embed = None\n",
    "            self.IL_encoder = None\n",
    "            self.IL_textcnn = nlp.model.ConvolutionalEncoder(\n",
    "                embed_size=embed_size,\n",
    "                num_filters=IL_num_filters,\n",
    "                ngram_filter_sizes=IL_ngram_filter_sizes,\n",
    "                conv_layer_activation='relu',\n",
    "                num_highway=1)\n",
    "            #self.IL_batchnorm = gluon.nn.BatchNorm()\n",
    "            \n",
    "            self.IL_block1 = get_residual_block('IL_block1_',sum(self.IL_num_filters)+2)\n",
    "            self.IL_dropout = gluon.nn.Dropout(0)\n",
    "            self.IL_block2 = get_residual_block('IL_block2_', sum(self.IL_num_filters)+2)\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                #self.output.add(gluon.nn.Dense(512))\n",
    "                #self.output.add(gluon.nn.Activation('relu'))\n",
    "                #self.output.add(gluon.nn.Dropout(dropout))\n",
    "                       \n",
    "                self.output.add(gluon.nn.Dense(output_size, flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F, IL_src_nd, IL_valid_length, T, P):  # pylint: disable=arguments-differ\n",
    "\n",
    "        IL_src_embed_ = self.IL_src_embed(IL_src_nd)\n",
    "        IL_encoded, _ = self.IL_encoder(\n",
    "            IL_src_embed_,\n",
    "            valid_length=IL_valid_length)  # Shape(T, N, C)\n",
    "        IL_textcnn = self.IL_textcnn(\n",
    "            F.transpose(IL_encoded, axes=(1, 0, 2)))\n",
    "        #IL_textcnn = self.IL_batchnorm(IL_textcnn)\n",
    "        \n",
    "        T_ = F.reshape(T, shape=(-1, 1))\n",
    "        P_ = F.reshape(P, shape=(-1, 1))\n",
    "        \n",
    "        input_vecs = mx.symbol.concat(\n",
    "            F.reshape(IL_textcnn,\n",
    "                      shape=(-1, sum(self.IL_num_filters))),T_, P_)\n",
    "        \n",
    "        il_block1 = self.IL_block1(input_vecs)\n",
    "        il1 = (input_vecs + il_block1).relu()\n",
    "\n",
    "        il2 = self.IL_dropout(il1)\n",
    "        il_block2 = self.IL_block2(il2)\n",
    "        il_transformed = (il2 + il_block2).relu()\n",
    "        \n",
    "        out = self.output(il_transformed)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.954687Z",
     "start_time": "2021-09-24T01:28:28.939689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILNet(\n",
      "  (IL_src_embed): HybridSequential(\n",
      "    (0): Embedding(72 -> 128, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (IL_encoder): TransformerEncoder(\n",
      "    (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): TransformerEncoderCell(\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 128, linear)\n",
      "          (proj_key): Dense(None -> 128, linear)\n",
      "          (proj_value): Dense(None -> 128, linear)\n",
      "        )\n",
      "        (proj): Dense(None -> 128, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 1024, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 128, linear)\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "      )\n",
      "      (1): TransformerEncoderCell(\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 128, linear)\n",
      "          (proj_key): Dense(None -> 128, linear)\n",
      "          (proj_value): Dense(None -> 128, linear)\n",
      "        )\n",
      "        (proj): Dense(None -> 128, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 1024, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 128, linear)\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "      )\n",
      "      (2): TransformerEncoderCell(\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 128, linear)\n",
      "          (proj_key): Dense(None -> 128, linear)\n",
      "          (proj_value): Dense(None -> 128, linear)\n",
      "        )\n",
      "        (proj): Dense(None -> 128, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 1024, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 128, linear)\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (IL_textcnn): ConvolutionalEncoder(\n",
      "    (_convs): HybridConcurrent(\n",
      "      (0): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(1,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (1): HybridSequential(\n",
      "        (0): Conv1D(128 -> 200, kernel_size=(2,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (2): HybridSequential(\n",
      "        (0): Conv1D(128 -> 200, kernel_size=(3,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (3): HybridSequential(\n",
      "        (0): Conv1D(128 -> 200, kernel_size=(4,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (4): HybridSequential(\n",
      "        (0): Conv1D(128 -> 200, kernel_size=(5,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (5): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(6,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (6): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(7,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (7): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(8,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (8): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(9,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (9): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(10,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (10): HybridSequential(\n",
      "        (0): Conv1D(128 -> 160, kernel_size=(15,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "    )\n",
      "    (_highways): Highway(\n",
      "      (hnet): HybridSequential(\n",
      "        (0): Dense(1560 -> 3120, linear)\n",
      "      )\n",
      "      (_activation): Activation(relu)\n",
      "    )\n",
      "  )\n",
      "  (output): HybridSequential(\n",
      "    (0): Dense(None -> 1024, linear)\n",
      "    (1): Activation(relu)\n",
      "    (2): Dropout(p = 0, axes=())\n",
      "    (3): Dense(None -> 512, linear)\n",
      "    (4): Activation(relu)\n",
      "    (5): Dense(None -> 1, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ILNet(dropout=dropout)\n",
    "#net.cation_encoder = model1.encoder\n",
    "#net.cation_src_embed =  model1.src_embed\n",
    "\n",
    "#net.anion_encoder = model2.encoder\n",
    "#net.anion_src_embed =  model2.src_embed\n",
    "\n",
    "net.IL_encoder = model3.encoder\n",
    "net.IL_src_embed =  model3.src_embed\n",
    "net.hybridize()\n",
    "print(net)\n",
    "#net.textcnn.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "#net.output.initialize(mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:01:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU\n"
     ]
    }
   ],
   "source": [
    "net.load_parameters(param_file,ctx=ctx)\n",
    "net.initialize(init=mx.init.Xavier(magnitude=magnitude), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, dataloader,context):\n",
    "    out = []\n",
    "    for i, ((IL_data, IL_length), T,P) in enumerate(dataloader):\n",
    "        IL_data = IL_data.as_in_context(context)\n",
    "        IL_length = IL_length.as_in_context(context).astype(np.float32)\n",
    "        T = T.as_in_context(context)\n",
    "        P = P.as_in_context(context)\n",
    "        output = net(IL_data, IL_length,T,P)\n",
    "        out= out+[f for f in output.asnumpy()]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:29.205687Z",
     "start_time": "2021-09-24T01:28:29.191690Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloader(train_dataset):\n",
    "\n",
    "    # Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, pad_val=0, ret_length=True),\n",
    "        nlp.data.batchify.Stack(dtype='float32'),nlp.data.batchify.Stack(dtype='float32'))\n",
    "\n",
    "    \n",
    "    # Construct a DataLoader object for both the training and test data\n",
    "    train_dataloader = gluon.data.DataLoader(dataset=train_dataset,\n",
    "                                             batchify_fn=batchify_fn,batch_size = train_batch_size)\n",
    "\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:32.331699Z",
     "start_time": "2021-09-24T01:28:30.834687Z"
    }
   },
   "outputs": [],
   "source": [
    "co2_database = pd.read_excel('smiles.xlsx',sheet_name='To_be_calculated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_IL_smiles = co2_database['SMILES'].map(canonical_smile).map(no_split).map(preprocess)\n",
    "train_T =co2_database['normalized_T_co2']\n",
    "train_P = co2_database['normalized_P_co2']\n",
    "train_dataset = gluon.data.SimpleDataset(gluon.data.ArrayDataset(train_IL_smiles,train_T,train_P))\n",
    "predict_dataloader = get_dataloader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T02:48:36.705010Z",
     "start_time": "2021-09-24T01:28:32.333699Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:01:55] ../src/operator/cudnn_ops.cc:292: Auto-tuning cuDNN op, set MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable\n",
      "[20:01:56] ../src/operator/cudnn_ops.cc:292: Auto-tuning cuDNN op, set MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable\n"
     ]
    }
   ],
   "source": [
    "predicted = predict(net, predict_dataloader,ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0\n",
      "0     0.013988\n",
      "1     0.018730\n",
      "2     0.017335\n",
      "3     0.015624\n",
      "4     0.014398\n",
      "...        ...\n",
      "5513  0.051523\n",
      "5514  0.012942\n",
      "5515  0.088111\n",
      "5516  0.014189\n",
      "5517  0.014892\n",
      "\n",
      "[5518 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(predicted)\n",
    "df.to_csv('predicted_co2.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross valid avg train loss 0.016455638436026907\n",
    "K-fold cross valid avg train r2 0.9453854297535305\n",
    "K-fold cross valid avg test loss 0.03263264888688952\n",
    "K-fold cross valid avg test r2 0.8201571726037102\n",
    "\n",
    "K-fold cross valid avg train loss 0.0171923089297629\n",
    "K-fold cross valid avg train r2 0.9405329953971364\n",
    "K-fold cross valid avg test loss 0.03243816043205651\n",
    "K-fold cross valid avg test r2 0.8260102191026778\n",
    "\n",
    "K-fold cross valid avg train loss 0.027404396245807428\n",
    "K-fold cross valid avg train r2 0.8223432808939917\n",
    "K-fold cross valid avg test loss 0.06469722731355083\n",
    "K-fold cross valid avg test r2 -0.3676552963800685"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
