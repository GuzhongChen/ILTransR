{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:20.744886Z",
     "start_time": "2021-05-08T05:59:18.275888Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon,nd,autograd,npx\n",
    "import gluonnlp as nlp\n",
    "import nmt\n",
    "from gluonnlp.model.transformer import ParallelTransformer, get_transformer_encoder_decoder\n",
    "import pandas as pd \n",
    "nlp.utils.check_version('0.7.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:20.760887Z",
     "start_time": "2021-05-08T05:59:20.745888Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "# parameters for dataset\n",
    "dataset = 'pubchem'\n",
    "src_lang, tgt_lang = 'random_smiles', 'rdkit_canonical_smiles'\n",
    "src_max_len, tgt_max_len = 100, 100\n",
    "\n",
    "# parameters for model\n",
    "num_units=128\n",
    "hidden_size=1024\n",
    "dropout=0.1\n",
    "epsilon=0.1\n",
    "num_layers=3\n",
    "num_heads=4\n",
    "scaled=True\n",
    "share_embed=True\n",
    "embed_size=128\n",
    "tie_weights=True\n",
    "embed_initializer=None\n",
    "magnitude = 3.0\n",
    "lr_update_factor = 0.5\n",
    "param_file = 'C:\\\\Users\\\\QI_LAB\\\\Desktop\\\\IL-PROPERTY-PREDICT-PUBCHEM\\\\smiles_transformer_128_1024\\\\valid_best.params'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:20.775887Z",
     "start_time": "2021-05-08T05:59:20.761887Z"
    }
   },
   "outputs": [],
   "source": [
    "def _load_vocab(file_path, **kwargs):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return nlp.Vocab.from_json(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:20.791886Z",
     "start_time": "2021-05-08T05:59:20.776888Z"
    }
   },
   "outputs": [],
   "source": [
    "src_vocab = _load_vocab('C:\\\\Users\\\\QI_LAB\\\\Desktop\\\\IL-PROPERTY-PREDICT-PUBCHEM\\\\datasets\\\\pubchem\\\\vocab.random_smiles.json')\n",
    "tgt_vocab = _load_vocab('C:\\\\Users\\\\QI_LAB\\\\Desktop\\\\IL-PROPERTY-PREDICT-PUBCHEM\\\\datasets\\\\pubchem\\\\vocab.rdkit_canonical_smiles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:20.822886Z",
     "start_time": "2021-05-08T05:59:20.792887Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "encoder1, decoder1, one_step_ahead_decoder1 = get_transformer_encoder_decoder(\n",
    "    units=num_units,\n",
    "    hidden_size=hidden_size,\n",
    "    dropout=dropout,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    max_src_length=src_max_len,\n",
    "    max_tgt_length=tgt_max_len,\n",
    "    scaled=scaled, prefix='transformer_1')\n",
    "\n",
    "encoder2, decoder2, one_step_ahead_decoder2 = get_transformer_encoder_decoder(\n",
    "    units=num_units,\n",
    "    hidden_size=hidden_size,\n",
    "    dropout=dropout,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    max_src_length=src_max_len,\n",
    "    max_tgt_length=tgt_max_len,\n",
    "    scaled=scaled,prefix='transformer_2')\n",
    "'''\n",
    "encoder3, decoder3, one_step_ahead_decoder3 = get_transformer_encoder_decoder(\n",
    "    units=num_units,\n",
    "    hidden_size=hidden_size,\n",
    "    dropout=dropout,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    max_src_length=src_max_len,\n",
    "    max_tgt_length=tgt_max_len,\n",
    "    scaled=scaled,prefix='transformer_3')\n",
    "'''\n",
    "model1 = nlp.model.translation.NMTModel(src_vocab=src_vocab,\n",
    "                 tgt_vocab=tgt_vocab,\n",
    "                 encoder=encoder1,\n",
    "                 decoder=decoder1,\n",
    "                 one_step_ahead_decoder=one_step_ahead_decoder1,\n",
    "                 embed_size=num_units,\n",
    "                 embed_initializer=None,\n",
    "                 prefix='transformer_1')\n",
    "model2 = nlp.model.translation.NMTModel(src_vocab=src_vocab,\n",
    "                 tgt_vocab=tgt_vocab,\n",
    "                 encoder=encoder2,\n",
    "                 decoder=decoder2,\n",
    "                 one_step_ahead_decoder=one_step_ahead_decoder2,\n",
    "                 embed_size=num_units,\n",
    "                 embed_initializer=None,\n",
    "                 prefix='transformer_2')\n",
    "'''\n",
    "model3 = nlp.model.translation.NMTModel(src_vocab=src_vocab,\n",
    "                 tgt_vocab=tgt_vocab,\n",
    "                 encoder=encoder3,\n",
    "                 decoder=decoder3,\n",
    "                 one_step_ahead_decoder=one_step_ahead_decoder3,\n",
    "                 embed_size=num_units,\n",
    "                 embed_initializer=None,\n",
    "                 prefix='transformer_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:20.838886Z",
     "start_time": "2021-05-08T05:59:20.823889Z"
    }
   },
   "outputs": [],
   "source": [
    "#model.initialize(init=mx.init.Xavier(magnitude=magnitude), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:23.201886Z",
     "start_time": "2021-05-08T05:59:20.840887Z"
    }
   },
   "outputs": [],
   "source": [
    "#model1.load_parameters(param_file,ctx=ctx)\n",
    "#model2.load_parameters(param_file,ctx=ctx)\n",
    "model3.load_parameters(param_file,ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:49:19.490024Z",
     "start_time": "2020-09-02T08:49:19.487020Z"
    }
   },
   "source": [
    "model.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:49:19.498031Z",
     "start_time": "2020-09-02T08:49:19.491025Z"
    }
   },
   "source": [
    "def encode(model, src_seq, src_vocab,ctx):\n",
    "    src_sentence = src_vocab[src_seq.split()]\n",
    "    src_sentence.append(src_vocab[src_vocab.eos_token])\n",
    "    src_npy = np.array(src_sentence, dtype=np.int32)\n",
    "    src_nd = mx.nd.array(src_npy)\n",
    "    src_nd = src_nd.reshape((1, -1)).as_in_context(ctx)\n",
    "    src_valid_length = mx.nd.array([src_nd.shape[1]]).as_in_context(ctx)\n",
    "    enc_outputs = model.encode(src_nd,valid_length=src_valid_length)\n",
    "    return enc_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:49:19.523053Z",
     "start_time": "2020-09-02T08:49:19.499032Z"
    }
   },
   "source": [
    "for sentence in ['c 1 ( N = C ( N ) N ) s c c ( - c 2 c c ( C ) n ( C ) c 2 ) n 1', 'C ( C ( c 1 c c c ( C ( N O ) = O ) c c 1 ) C C ) C']:\n",
    "    e = encode(model, sentence,src_vocab,ctx)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:23.233886Z",
     "start_time": "2021-05-08T05:59:23.202887Z"
    }
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "def canonical_smile(sml):\n",
    "    try:\n",
    "        m = Chem.MolFromSmiles(sml)\n",
    "        #return Chem.MolToSmiles(m, canonical=True,isomericSmiles=False)\n",
    "        return Chem.MolToSmiles(m, canonical=True,isomericSmiles=True)\n",
    "    except:\n",
    "        print(sml)\n",
    "        return float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:23.249888Z",
     "start_time": "2021-05-08T05:59:23.234887Z"
    }
   },
   "outputs": [],
   "source": [
    "def no_split(sm):\n",
    "    arr = []\n",
    "    i = 0\n",
    "    try:\n",
    "        len(sm)\n",
    "    except:\n",
    "        print(sm)\n",
    "    while i < len(sm)-1:\n",
    "        arr.append(sm[i])\n",
    "        i += 1\n",
    "    if i == len(sm)-1:\n",
    "        arr.append(sm[i])\n",
    "    return ' '.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:23.265887Z",
     "start_time": "2021-05-08T05:59:23.250887Z"
    }
   },
   "outputs": [],
   "source": [
    "length_clip = nlp.data.ClipSequence(100)\n",
    "# Helper function to preprocess a single data point\n",
    "def preprocess(data):\n",
    "    # A token index or a list of token indices is\n",
    "    # returned according to the vocabulary.\n",
    "    src_sentence = src_vocab[length_clip(data.split())]\n",
    "    src_sentence.append(src_vocab[src_vocab.eos_token])\n",
    "    src_npy = np.array(src_sentence, dtype=np.int32)\n",
    "    src_nd = mx.nd.array(src_npy)\n",
    "    return src_nd\n",
    "\n",
    "# Helper function for getting the length\n",
    "def get_length(x):\n",
    "    return float(len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:23.281886Z",
     "start_time": "2021-05-08T05:59:23.266887Z"
    }
   },
   "outputs": [],
   "source": [
    "dropout = 0.05\n",
    "train_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:23.297887Z",
     "start_time": "2021-05-08T05:59:23.282889Z"
    }
   },
   "outputs": [],
   "source": [
    "class ILNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self,\n",
    "                 dropout,\n",
    "                 src_vocab=src_vocab,\n",
    "                 embed_size=embed_size,\n",
    "                 output_size=1,\n",
    "                 num_filters=(100, 200, 200, 200, 200, 100,100),\n",
    "                 ngram_filter_sizes=(1, 2, 3, 4, 5, 6,7),\n",
    "                 IL_num_filters=(100, 200, 200, 200, 200, 100, 100, 100, 100,100, 160,160),\n",
    "                 IL_ngram_filter_sizes=(1, 2, 3,4, 5, 6, 7, 8, 9, 10, 15,20),\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(ILNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            \n",
    "            self.num_filters = num_filters\n",
    "            self.IL_num_filters = IL_num_filters\n",
    "            '''\n",
    "            self.cation_src_embed = None\n",
    "            self.cation_encoder = None\n",
    "            self.cation_textcnn = nlp.model.ConvolutionalEncoder(\n",
    "                embed_size=embed_size,\n",
    "                num_filters=num_filters,\n",
    "                ngram_filter_sizes=ngram_filter_sizes,\n",
    "                conv_layer_activation='relu',\n",
    "                num_highway=1)\n",
    "            #self.cation_dropout = gluon.nn.BatchNorm()\n",
    "            self.cation_dropout = gluon.nn.Dropout(dropout)\n",
    "\n",
    "            self.anion_src_embed = None\n",
    "            self.anion_encoder = None\n",
    "            self.anion_textcnn = nlp.model.ConvolutionalEncoder(\n",
    "                embed_size=embed_size,\n",
    "                num_filters=num_filters,\n",
    "                ngram_filter_sizes=ngram_filter_sizes,\n",
    "                conv_layer_activation='relu',\n",
    "                num_highway=1)\n",
    "            #self.anion_dropout = gluon.nn.BatchNorm()\n",
    "            self.anion_dropout = gluon.nn.Dropout(dropout)\n",
    "            '''\n",
    "            self.IL_src_embed = None\n",
    "            self.IL_encoder = None\n",
    "            self.IL_textcnn = nlp.model.ConvolutionalEncoder(\n",
    "                embed_size=embed_size,\n",
    "                num_filters=IL_num_filters,\n",
    "                ngram_filter_sizes=IL_ngram_filter_sizes,\n",
    "                conv_layer_activation='relu',\n",
    "                num_highway=1)\n",
    "            #self.IL_dropout = gluon.nn.BatchNorm()\n",
    "            #self.IL_dropout = gluon.nn.Dropout(dropout)\n",
    "            '''\n",
    "            self.mlp = gluon.nn.HybridSequential()\n",
    "            with self.mlp.name_scope():\n",
    "                #self.mlp.add(gluon.nn.Dropout(dropout))\n",
    "                self.mlp.add(gluon.nn.Dense(4096))\n",
    "                #self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                #self.mlp.add(gluon.nn.Dropout(dropout))\n",
    "                \n",
    "                self.mlp.add(gluon.nn.Dense(2048))\n",
    "                #self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                \n",
    "                self.mlp.add(gluon.nn.Dense(1024))\n",
    "                #self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                \n",
    "                self.mlp.add(gluon.nn.Dense(512))\n",
    "                self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                self.mlp.add(gluon.nn.Dense(256))\n",
    "                self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "            '''\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                self.output.add(gluon.nn.Dense(1024))\n",
    "                self.output.add(gluon.nn.Activation('relu'))\n",
    "                self.output.add(gluon.nn.Dropout(dropout))\n",
    "                self.output.add(gluon.nn.Dense(512))\n",
    "                self.output.add(gluon.nn.Activation('relu'))\n",
    "                self.output.add(gluon.nn.Dense(output_size, flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F,IL_src_nd, IL_valid_length, T):  # pylint: disable=arguments-differ\n",
    "        '''\n",
    "        cation_src_embed_ = self.cation_src_embed(cation_src_nd)\n",
    "        cation_encoded, _ = self.cation_encoder(\n",
    "            cation_src_embed_,\n",
    "            valid_length=cation_valid_length)  # Shape(T, N, C)\n",
    "        cation_textcnn = self.cation_textcnn(\n",
    "            F.transpose(cation_encoded, axes=(1, 0, 2)))\n",
    "        cation_textcnn = self.cation_dropout(cation_textcnn)\n",
    "\n",
    "        anion_src_embed_ = self.anion_src_embed(anion_src_nd)\n",
    "        anion_encoded, _ = self.anion_encoder(\n",
    "            anion_src_embed_,\n",
    "            valid_length=anion_valid_length)  # Shape(T, N, C)\n",
    "        anion_textcnn = self.anion_textcnn(\n",
    "            F.transpose(anion_encoded, axes=(1, 0, 2)))\n",
    "        anion_textcnn = self.anion_dropout(anion_textcnn)\n",
    "        '''\n",
    "        IL_src_embed_ = self.IL_src_embed(IL_src_nd)\n",
    "        IL_encoded, _ = self.IL_encoder(\n",
    "            IL_src_embed_,\n",
    "            valid_length=IL_valid_length)  # Shape(T, N, C)\n",
    "        IL_textcnn = self.IL_textcnn(\n",
    "            F.transpose(IL_encoded, axes=(1, 0, 2)))\n",
    "        #IL_textcnn = self.IL_dropout(IL_textcnn)\n",
    "        \n",
    "        T_ = F.reshape(T, shape=(-1, 1))\n",
    "        \n",
    "        input_vecs = mx.symbol.concat(\n",
    "            F.reshape(IL_textcnn,\n",
    "                      shape=(-1, sum(self.IL_num_filters))),T_)\n",
    "        \n",
    "        #mlp_out = self.mlp(input_vecs)\n",
    "\n",
    "        \n",
    "\n",
    "        #add_temp_press = mx.symbol.concat(mlp_out, T_)\n",
    "        #add_temp_press = mx.symbol.concat(input_vecs, T_)\n",
    "        out = self.output(input_vecs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:23.313887Z",
     "start_time": "2021-05-08T05:59:23.298887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILNet(\n",
      "  (IL_src_embed): HybridSequential(\n",
      "    (0): Embedding(72 -> 128, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (IL_encoder): TransformerEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(128 -> 128, linear)\n",
      "          (proj_key): Dense(128 -> 128, linear)\n",
      "          (proj_value): Dense(128 -> 128, linear)\n",
      "        )\n",
      "        (proj): Dense(128 -> 128, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(128 -> 1024, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(1024 -> 128, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "      )\n",
      "      (1): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(128 -> 128, linear)\n",
      "          (proj_key): Dense(128 -> 128, linear)\n",
      "          (proj_value): Dense(128 -> 128, linear)\n",
      "        )\n",
      "        (proj): Dense(128 -> 128, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(128 -> 1024, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(1024 -> 128, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "      )\n",
      "      (2): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(128 -> 128, linear)\n",
      "          (proj_key): Dense(128 -> 128, linear)\n",
      "          (proj_value): Dense(128 -> 128, linear)\n",
      "        )\n",
      "        (proj): Dense(128 -> 128, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(128 -> 1024, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(1024 -> 128, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (IL_textcnn): ConvolutionalEncoder(\n",
      "    (_convs): HybridConcurrent(\n",
      "      (0): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(1,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (1): HybridSequential(\n",
      "        (0): Conv1D(128 -> 200, kernel_size=(2,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (2): HybridSequential(\n",
      "        (0): Conv1D(128 -> 200, kernel_size=(3,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (3): HybridSequential(\n",
      "        (0): Conv1D(128 -> 200, kernel_size=(4,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (4): HybridSequential(\n",
      "        (0): Conv1D(128 -> 200, kernel_size=(5,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (5): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(6,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (6): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(7,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (7): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(8,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (8): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(9,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (9): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(10,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (10): HybridSequential(\n",
      "        (0): Conv1D(128 -> 160, kernel_size=(15,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (11): HybridSequential(\n",
      "        (0): Conv1D(128 -> 160, kernel_size=(20,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "    )\n",
      "    (_highways): Highway(\n",
      "      (hnet): HybridSequential(\n",
      "        (0): Dense(1720 -> 3440, linear)\n",
      "      )\n",
      "      (_activation): Activation(relu)\n",
      "    )\n",
      "  )\n",
      "  (output): HybridSequential(\n",
      "    (0): Dense(None -> 1024, linear)\n",
      "    (1): Activation(relu)\n",
      "    (2): Dropout(p = 0.05, axes=())\n",
      "    (3): Dense(None -> 512, linear)\n",
      "    (4): Activation(relu)\n",
      "    (5): Dense(None -> 1, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ILNet(dropout=dropout)\n",
    "#net.cation_encoder = model1.encoder\n",
    "#net.cation_src_embed =  model1.src_embed\n",
    "\n",
    "#net.anion_encoder = model2.encoder\n",
    "#net.anion_src_embed =  model2.src_embed\n",
    "\n",
    "net.IL_encoder = model3.encoder\n",
    "net.IL_src_embed =  model3.src_embed\n",
    "net.hybridize()\n",
    "print(net)\n",
    "#net.textcnn.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "#net.output.initialize(mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:23.505886Z",
     "start_time": "2021-05-08T05:59:23.474887Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def get_r2(label, pred, multioutput='uniform_average'):\n",
    "    label = label.asnumpy()\n",
    "    pred = pred.asnumpy()\n",
    "    r2 = metrics.r2_score(label,pred,multioutput=multioutput)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:23.521887Z",
     "start_time": "2021-05-08T05:59:23.506887Z"
    }
   },
   "outputs": [],
   "source": [
    "home_dir = 'C:\\\\Users\\\\QI_LAB\\\\Desktop\\\\IL-PROPERTY-PREDICT-PUBCHEM\\\\viscosity'\n",
    "save_dir = os.path.join(home_dir,'textcnn')\n",
    "\n",
    "def train(net, train_data, batch_size, learning_rate, context, epochs,log_interval=10, dev_data=None, fold=None ):\n",
    "    start_pipeline_time = time.time()\n",
    "    #net.cation_textcnn.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    #net.anion_textcnn.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    net.IL_textcnn.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    #net.cation_dropout.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    #net.anion_dropout.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    #net.IL_dropout.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    #net.mlp.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    net.output.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    num_epoch_lr = 10\n",
    "    factor = 0.5\n",
    "    schedule = mx.lr_scheduler.FactorScheduler(base_lr = learning_rate, step=len(train_data)* num_epoch_lr,factor=factor)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'lr_scheduler': schedule})\n",
    "    #trainer = gluon.Trainer(net.collect_params(), 'adam',{'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.L2Loss()\n",
    "    # Training/Testing.\n",
    "    best_epoch_L = 100\n",
    "    for epoch in range(epochs):\n",
    "        # Epoch training stats.\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_r2 = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        r2_num = 0\n",
    "        epoch_wc = 0\n",
    "        # Log interval training stats.\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "        for i, ((IL_data, IL_length), T,label) in enumerate(train_data):\n",
    "            #cation_data = cation_data.as_in_context(context)\n",
    "            #cation_length = cation_length.as_in_context(context).astype(np.float32)\n",
    "            #anion_data = anion_data.as_in_context(context)\n",
    "            #anion_length = anion_length.as_in_context(context).astype(np.float32)\n",
    "            IL_data = IL_data.as_in_context(context)\n",
    "            IL_length = IL_length.as_in_context(context).astype(np.float32)\n",
    "            T = T.as_in_context(context)\n",
    "            label = label.as_in_context(context)\n",
    "            wc = max_len\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += label.shape[0]\n",
    "            epoch_sent_num += label.shape[0]\n",
    "            with autograd.record():\n",
    "                output = net(IL_data, IL_length,T)\n",
    "                L = loss(output, label).sum()\n",
    "                r2 = get_r2(output,label)\n",
    "            L.backward()\n",
    "            # Update parameter.\n",
    "            trainer.step(batch_size)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            epoch_r2+=r2\n",
    "            r2_num+=1\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print('[Epoch %d Batch %d/%d] avg loss %g, throughput %gK wps' % (\n",
    "                    epoch, i + 1, len(train_data),\n",
    "                    log_interval_L / log_interval_sent_num,\n",
    "                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n",
    "                # Clear log interval training stats.\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "        if  (epoch_L/ epoch_sent_num) < best_epoch_L:\n",
    "            best_epoch_L = epoch_L\n",
    "            save_path = os.path.join(save_dir, 'viscosity_best.params')\n",
    "            net.save_parameters(save_path)\n",
    "        \n",
    "        print('[Epoch %d] train avg loss %g, train avg r2 %g,'\n",
    "              'throughput %gK wps' % (\n",
    "                  epoch, epoch_L / epoch_sent_num, epoch_r2 / r2_num,\n",
    "                  epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))\n",
    "        print('learning rate:',trainer.learning_rate)\n",
    "        '''\n",
    "        if epoch + 1 >= (epochs * 2) // 3:\n",
    "            new_lr = trainer.learning_rate * lr_update_factor\n",
    "            trainer.set_learning_rate(new_lr)\n",
    "        '''\n",
    "    print('Total time cost %.2fs'%(time.time()-start_pipeline_time))\n",
    "    return epoch_L / epoch_sent_num, epoch_r2 / r2_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T05:59:23.537886Z",
     "start_time": "2021-05-08T05:59:23.522887Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloader(train_dataset):\n",
    "\n",
    "    # Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, pad_val=0, ret_length=True),\n",
    "        nlp.data.batchify.Stack(dtype='float32'),nlp.data.batchify.Stack(dtype='float32'))\n",
    "\n",
    "    # Construct a DataLoader object for both the training and test data\n",
    "    train_dataloader = gluon.data.DataLoader(dataset=train_dataset,\n",
    "                                             batchify_fn=batchify_fn,batch_size = train_batch_size)\n",
    "\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T06:00:42.190898Z",
     "start_time": "2021-05-08T05:59:23.586888Z"
    }
   },
   "outputs": [],
   "source": [
    "viscosity_database = pd.read_excel('viscosity_P.xlsx',sheet_name='viscosity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T06:08:54.186910Z",
     "start_time": "2021-05-08T06:08:49.052900Z"
    }
   },
   "outputs": [],
   "source": [
    "train_IL_smiles = viscosity_database['IL SMILES'].map(canonical_smile).map(no_split).map(preprocess)\n",
    "train_T =viscosity_database['normalized_T']\n",
    "train_lngamma = viscosity_database['lnÎ·']\n",
    "train_dataset = gluon.data.SimpleDataset(gluon.data.ArrayDataset(train_IL_smiles,train_T,train_lngamma))\n",
    "train_dataloader= get_dataloader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T06:31:04.869914Z",
     "start_time": "2021-05-08T06:11:01.338913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 10/121] avg loss 2.56781, throughput 0.0148947K wps\n",
      "[Epoch 0 Batch 20/121] avg loss 0.99412, throughput 0.0361598K wps\n",
      "[Epoch 0 Batch 30/121] avg loss 1.94689, throughput 0.0189595K wps\n",
      "[Epoch 0 Batch 40/121] avg loss 0.754829, throughput 0.0185543K wps\n",
      "[Epoch 0 Batch 50/121] avg loss 0.752657, throughput 0.0439522K wps\n",
      "[Epoch 0 Batch 60/121] avg loss 0.850871, throughput 0.022079K wps\n",
      "[Epoch 0 Batch 70/121] avg loss 1.34137, throughput 0.0246409K wps\n",
      "[Epoch 0 Batch 80/121] avg loss 1.06576, throughput 0.0714387K wps\n",
      "[Epoch 0 Batch 90/121] avg loss 0.770288, throughput 0.0170806K wps\n",
      "[Epoch 0 Batch 100/121] avg loss 0.536686, throughput 0.0707263K wps\n",
      "[Epoch 0 Batch 110/121] avg loss 0.551491, throughput 0.0705667K wps\n",
      "[Epoch 0 Batch 120/121] avg loss 0.893116, throughput 0.303951K wps\n",
      "[Epoch 0] train avg loss 1.08498, train avg r2 -371.045,throughput 0.0291145K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 1 Batch 10/121] avg loss 0.445737, throughput 2.38095K wps\n",
      "[Epoch 1 Batch 20/121] avg loss 0.431127, throughput 2.53807K wps\n",
      "[Epoch 1 Batch 30/121] avg loss 1.99986, throughput 2.58397K wps\n",
      "[Epoch 1 Batch 40/121] avg loss 0.753377, throughput 2.50001K wps\n",
      "[Epoch 1 Batch 50/121] avg loss 0.384422, throughput 2.53807K wps\n",
      "[Epoch 1 Batch 60/121] avg loss 0.63282, throughput 2.445K wps\n",
      "[Epoch 1 Batch 70/121] avg loss 1.25096, throughput 2.48138K wps\n",
      "[Epoch 1 Batch 80/121] avg loss 0.994265, throughput 2.19298K wps\n",
      "[Epoch 1 Batch 90/121] avg loss 0.711077, throughput 2.32019K wps\n",
      "[Epoch 1 Batch 100/121] avg loss 0.543245, throughput 2.53164K wps\n",
      "[Epoch 1 Batch 110/121] avg loss 0.479555, throughput 2.42131K wps\n",
      "[Epoch 1 Batch 120/121] avg loss 0.904952, throughput 2.68817K wps\n",
      "[Epoch 1] train avg loss 0.794131, train avg r2 -0.709782,throughput 2.46737K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 2 Batch 10/121] avg loss 0.334334, throughput 2.37529K wps\n",
      "[Epoch 2 Batch 20/121] avg loss 0.338271, throughput 2.5641K wps\n",
      "[Epoch 2 Batch 30/121] avg loss 1.29968, throughput 2.60417K wps\n",
      "[Epoch 2 Batch 40/121] avg loss 0.467142, throughput 2.50627K wps\n",
      "[Epoch 2 Batch 50/121] avg loss 0.464264, throughput 2.54452K wps\n",
      "[Epoch 2 Batch 60/121] avg loss 0.675861, throughput 2.45701K wps\n",
      "[Epoch 2 Batch 70/121] avg loss 0.67928, throughput 2.48757K wps\n",
      "[Epoch 2 Batch 80/121] avg loss 0.632652, throughput 2.1978K wps\n",
      "[Epoch 2 Batch 90/121] avg loss 0.668814, throughput 2.32018K wps\n",
      "[Epoch 2 Batch 100/121] avg loss 0.397244, throughput 2.52525K wps\n",
      "[Epoch 2 Batch 110/121] avg loss 0.363474, throughput 2.42718K wps\n",
      "[Epoch 2 Batch 120/121] avg loss 0.6144, throughput 2.68817K wps\n",
      "[Epoch 2] train avg loss 0.578054, train avg r2 -0.099805,throughput 2.47191K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 3 Batch 10/121] avg loss 0.346563, throughput 2.38095K wps\n",
      "[Epoch 3 Batch 20/121] avg loss 0.285377, throughput 2.55103K wps\n",
      "[Epoch 3 Batch 30/121] avg loss 0.503292, throughput 2.59741K wps\n",
      "[Epoch 3 Batch 40/121] avg loss 0.198399, throughput 2.48138K wps\n",
      "[Epoch 3 Batch 50/121] avg loss 0.264423, throughput 2.54452K wps\n",
      "[Epoch 3 Batch 60/121] avg loss 0.518176, throughput 2.45701K wps\n",
      "[Epoch 3 Batch 70/121] avg loss 0.672406, throughput 2.48756K wps\n",
      "[Epoch 3 Batch 80/121] avg loss 0.925153, throughput 2.19298K wps\n",
      "[Epoch 3 Batch 90/121] avg loss 0.579052, throughput 2.331K wps\n",
      "[Epoch 3 Batch 100/121] avg loss 0.302126, throughput 2.5189K wps\n",
      "[Epoch 3 Batch 110/121] avg loss 0.282032, throughput 2.4213K wps\n",
      "[Epoch 3 Batch 120/121] avg loss 0.619241, throughput 2.68817K wps\n",
      "[Epoch 3] train avg loss 0.458032, train avg r2 0.29759,throughput 2.46888K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 4 Batch 10/121] avg loss 0.280717, throughput 2.36967K wps\n",
      "[Epoch 4 Batch 20/121] avg loss 0.264714, throughput 2.55102K wps\n",
      "[Epoch 4 Batch 30/121] avg loss 0.540382, throughput 2.60417K wps\n",
      "[Epoch 4 Batch 40/121] avg loss 0.213498, throughput 2.49377K wps\n",
      "[Epoch 4 Batch 50/121] avg loss 0.254761, throughput 2.53165K wps\n",
      "[Epoch 4 Batch 60/121] avg loss 0.463889, throughput 2.45098K wps\n",
      "[Epoch 4 Batch 70/121] avg loss 0.469614, throughput 2.48139K wps\n",
      "[Epoch 4 Batch 80/121] avg loss 0.858049, throughput 2.19298K wps\n",
      "[Epoch 4 Batch 90/121] avg loss 0.824735, throughput 2.30946K wps\n",
      "[Epoch 4 Batch 100/121] avg loss 0.333963, throughput 2.5189K wps\n",
      "[Epoch 4 Batch 110/121] avg loss 0.315854, throughput 2.41546K wps\n",
      "[Epoch 4 Batch 120/121] avg loss 0.468932, throughput 2.68096K wps\n",
      "[Epoch 4] train avg loss 0.440793, train avg r2 0.287872,throughput 2.46586K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 5 Batch 10/121] avg loss 0.211843, throughput 2.38663K wps\n",
      "[Epoch 5 Batch 20/121] avg loss 0.245873, throughput 2.55102K wps\n",
      "[Epoch 5 Batch 30/121] avg loss 0.505946, throughput 2.60416K wps\n",
      "[Epoch 5 Batch 40/121] avg loss 0.217395, throughput 2.48756K wps\n",
      "[Epoch 5 Batch 50/121] avg loss 0.230775, throughput 2.54454K wps\n",
      "[Epoch 5 Batch 60/121] avg loss 0.436763, throughput 2.45097K wps\n",
      "[Epoch 5 Batch 70/121] avg loss 0.404627, throughput 2.48139K wps\n",
      "[Epoch 5 Batch 80/121] avg loss 0.978911, throughput 2.19298K wps\n",
      "[Epoch 5 Batch 90/121] avg loss 1.7685, throughput 2.331K wps\n",
      "[Epoch 5 Batch 100/121] avg loss 0.619293, throughput 2.52525K wps\n",
      "[Epoch 5 Batch 110/121] avg loss 0.354048, throughput 2.42132K wps\n",
      "[Epoch 5 Batch 120/121] avg loss 0.419211, throughput 2.68817K wps\n",
      "[Epoch 5] train avg loss 0.532632, train avg r2 -0.00297064,throughput 2.4704K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 6 Batch 10/121] avg loss 0.298444, throughput 2.38662K wps\n",
      "[Epoch 6 Batch 20/121] avg loss 0.271373, throughput 2.54453K wps\n",
      "[Epoch 6 Batch 30/121] avg loss 0.488926, throughput 2.5974K wps\n",
      "[Epoch 6 Batch 40/121] avg loss 0.231706, throughput 2.39234K wps\n",
      "[Epoch 6 Batch 50/121] avg loss 0.310734, throughput 2.51889K wps\n",
      "[Epoch 6 Batch 60/121] avg loss 0.419524, throughput 2.43903K wps\n",
      "[Epoch 6 Batch 70/121] avg loss 0.407035, throughput 2.48139K wps\n",
      "[Epoch 6 Batch 80/121] avg loss 0.500032, throughput 2.18818K wps\n",
      "[Epoch 6 Batch 90/121] avg loss 0.763421, throughput 2.26757K wps\n",
      "[Epoch 6 Batch 100/121] avg loss 0.721594, throughput 2.52525K wps\n",
      "[Epoch 6 Batch 110/121] avg loss 0.489359, throughput 2.41546K wps\n",
      "[Epoch 6 Batch 120/121] avg loss 0.446845, throughput 2.68818K wps\n",
      "[Epoch 6] train avg loss 0.445761, train avg r2 0.333444,throughput 2.45138K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 7 Batch 10/121] avg loss 0.59215, throughput 2.39234K wps\n",
      "[Epoch 7 Batch 20/121] avg loss 0.413354, throughput 2.55753K wps\n",
      "[Epoch 7 Batch 30/121] avg loss 0.538742, throughput 2.60417K wps\n",
      "[Epoch 7 Batch 40/121] avg loss 0.161502, throughput 2.48139K wps\n",
      "[Epoch 7 Batch 50/121] avg loss 0.2451, throughput 2.52526K wps\n",
      "[Epoch 7 Batch 60/121] avg loss 0.438076, throughput 2.44498K wps\n",
      "[Epoch 7 Batch 70/121] avg loss 0.391354, throughput 2.48139K wps\n",
      "[Epoch 7 Batch 80/121] avg loss 0.451515, throughput 2.18341K wps\n",
      "[Epoch 7 Batch 90/121] avg loss 0.390384, throughput 2.331K wps\n",
      "[Epoch 7 Batch 100/121] avg loss 0.517985, throughput 2.5189K wps\n",
      "[Epoch 7 Batch 110/121] avg loss 0.699674, throughput 2.41545K wps\n",
      "[Epoch 7 Batch 120/121] avg loss 0.752099, throughput 2.68817K wps\n",
      "[Epoch 7] train avg loss 0.466096, train avg r2 0.195694,throughput 2.46586K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 8 Batch 10/121] avg loss 0.428188, throughput 2.39234K wps\n",
      "[Epoch 8 Batch 20/121] avg loss 0.473642, throughput 2.53165K wps\n",
      "[Epoch 8 Batch 30/121] avg loss 0.388834, throughput 2.59741K wps\n",
      "[Epoch 8 Batch 40/121] avg loss 0.142778, throughput 2.48756K wps\n",
      "[Epoch 8 Batch 50/121] avg loss 0.239944, throughput 2.53806K wps\n",
      "[Epoch 8 Batch 60/121] avg loss 0.333396, throughput 2.43903K wps\n",
      "[Epoch 8 Batch 70/121] avg loss 0.28505, throughput 2.48756K wps\n",
      "[Epoch 8 Batch 80/121] avg loss 0.448557, throughput 2.1978K wps\n",
      "[Epoch 8 Batch 90/121] avg loss 0.447434, throughput 2.34191K wps\n",
      "[Epoch 8 Batch 100/121] avg loss 0.354083, throughput 2.52525K wps\n",
      "[Epoch 8 Batch 110/121] avg loss 0.499251, throughput 2.42717K wps\n",
      "[Epoch 8 Batch 120/121] avg loss 0.412567, throughput 2.68819K wps\n",
      "[Epoch 8] train avg loss 0.370958, train avg r2 0.46372,throughput 2.46989K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 9 Batch 10/121] avg loss 0.29055, throughput 2.38663K wps\n",
      "[Epoch 9 Batch 20/121] avg loss 0.307536, throughput 2.55102K wps\n",
      "[Epoch 9 Batch 30/121] avg loss 0.590511, throughput 2.59741K wps\n",
      "[Epoch 9 Batch 40/121] avg loss 0.27995, throughput 2.50626K wps\n",
      "[Epoch 9 Batch 50/121] avg loss 0.314016, throughput 2.53165K wps\n",
      "[Epoch 9 Batch 60/121] avg loss 0.359182, throughput 2.457K wps\n",
      "[Epoch 9 Batch 70/121] avg loss 0.324761, throughput 2.48756K wps\n",
      "[Epoch 9 Batch 80/121] avg loss 0.316397, throughput 2.20264K wps\n",
      "[Epoch 9 Batch 90/121] avg loss 0.360302, throughput 2.34192K wps\n",
      "[Epoch 9 Batch 100/121] avg loss 0.315977, throughput 2.52525K wps\n",
      "[Epoch 9 Batch 110/121] avg loss 0.273078, throughput 2.4213K wps\n",
      "[Epoch 9 Batch 120/121] avg loss 0.231831, throughput 2.69542K wps\n",
      "[Epoch 9] train avg loss 0.330204, train avg r2 0.511728,throughput 2.47191K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 10 Batch 10/121] avg loss 0.228836, throughput 2.38664K wps\n",
      "[Epoch 10 Batch 20/121] avg loss 0.175022, throughput 2.53806K wps\n",
      "[Epoch 10 Batch 30/121] avg loss 0.258123, throughput 2.59741K wps\n",
      "[Epoch 10 Batch 40/121] avg loss 0.123314, throughput 2.49999K wps\n",
      "[Epoch 10 Batch 50/121] avg loss 0.16669, throughput 2.55102K wps\n",
      "[Epoch 10 Batch 60/121] avg loss 0.22351, throughput 2.457K wps\n",
      "[Epoch 10 Batch 70/121] avg loss 0.23127, throughput 2.4814K wps\n",
      "[Epoch 10 Batch 80/121] avg loss 0.343584, throughput 2.19298K wps\n",
      "[Epoch 10 Batch 90/121] avg loss 0.521618, throughput 2.33645K wps\n",
      "[Epoch 10 Batch 100/121] avg loss 0.231886, throughput 2.53164K wps\n",
      "[Epoch 10 Batch 110/121] avg loss 0.253463, throughput 2.41546K wps\n",
      "[Epoch 10 Batch 120/121] avg loss 0.317159, throughput 2.69541K wps\n",
      "[Epoch 10] train avg loss 0.256244, train avg r2 0.610495,throughput 2.4709K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 11 Batch 10/121] avg loss 0.1637, throughput 2.38094K wps\n",
      "[Epoch 11 Batch 20/121] avg loss 0.240949, throughput 2.55755K wps\n",
      "[Epoch 11 Batch 30/121] avg loss 0.300899, throughput 2.60418K wps\n",
      "[Epoch 11 Batch 40/121] avg loss 0.0998741, throughput 2.49376K wps\n",
      "[Epoch 11 Batch 50/121] avg loss 0.171419, throughput 2.52525K wps\n",
      "[Epoch 11 Batch 60/121] avg loss 0.180419, throughput 2.45098K wps\n",
      "[Epoch 11 Batch 70/121] avg loss 0.225014, throughput 2.48138K wps\n",
      "[Epoch 11 Batch 80/121] avg loss 0.258044, throughput 2.19298K wps\n",
      "[Epoch 11 Batch 90/121] avg loss 0.48406, throughput 2.32558K wps\n",
      "[Epoch 11 Batch 100/121] avg loss 0.323972, throughput 2.52526K wps\n",
      "[Epoch 11 Batch 110/121] avg loss 0.370249, throughput 2.4213K wps\n",
      "[Epoch 11 Batch 120/121] avg loss 0.280738, throughput 2.68096K wps\n",
      "[Epoch 11] train avg loss 0.258402, train avg r2 0.585651,throughput 2.46888K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 12 Batch 10/121] avg loss 0.182879, throughput 2.38095K wps\n",
      "[Epoch 12 Batch 20/121] avg loss 0.252258, throughput 2.54453K wps\n",
      "[Epoch 12 Batch 30/121] avg loss 0.269544, throughput 2.59068K wps\n",
      "[Epoch 12 Batch 40/121] avg loss 0.114313, throughput 2.5K wps\n",
      "[Epoch 12 Batch 50/121] avg loss 0.160097, throughput 2.53808K wps\n",
      "[Epoch 12 Batch 60/121] avg loss 0.194807, throughput 2.44499K wps\n",
      "[Epoch 12 Batch 70/121] avg loss 0.23842, throughput 2.48756K wps\n",
      "[Epoch 12 Batch 80/121] avg loss 0.242679, throughput 2.1978K wps\n",
      "[Epoch 12 Batch 90/121] avg loss 0.173525, throughput 2.331K wps\n",
      "[Epoch 12 Batch 100/121] avg loss 0.14541, throughput 2.52525K wps\n",
      "[Epoch 12 Batch 110/121] avg loss 0.198456, throughput 2.42131K wps\n",
      "[Epoch 12 Batch 120/121] avg loss 0.210979, throughput 2.68817K wps\n",
      "[Epoch 12] train avg loss 0.198621, train avg r2 0.716734,throughput 2.46939K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 13 Batch 10/121] avg loss 0.139693, throughput 2.38096K wps\n",
      "[Epoch 13 Batch 20/121] avg loss 0.135913, throughput 2.55754K wps\n",
      "[Epoch 13 Batch 30/121] avg loss 0.201166, throughput 2.60418K wps\n",
      "[Epoch 13 Batch 40/121] avg loss 0.077936, throughput 2.5K wps\n",
      "[Epoch 13 Batch 50/121] avg loss 0.118052, throughput 2.53808K wps\n",
      "[Epoch 13 Batch 60/121] avg loss 0.161785, throughput 2.43903K wps\n",
      "[Epoch 13 Batch 70/121] avg loss 0.195386, throughput 2.48138K wps\n",
      "[Epoch 13 Batch 80/121] avg loss 0.235969, throughput 2.19298K wps\n",
      "[Epoch 13 Batch 90/121] avg loss 0.186914, throughput 2.32019K wps\n",
      "[Epoch 13 Batch 100/121] avg loss 0.13936, throughput 2.50626K wps\n",
      "[Epoch 13 Batch 110/121] avg loss 0.250788, throughput 2.41547K wps\n",
      "[Epoch 13 Batch 120/121] avg loss 0.177263, throughput 2.68816K wps\n",
      "[Epoch 13] train avg loss 0.168426, train avg r2 0.768923,throughput 2.46788K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 14 Batch 10/121] avg loss 0.137472, throughput 2.38663K wps\n",
      "[Epoch 14 Batch 20/121] avg loss 0.163462, throughput 2.55754K wps\n",
      "[Epoch 14 Batch 30/121] avg loss 0.210523, throughput 2.58398K wps\n",
      "[Epoch 14 Batch 40/121] avg loss 0.0756834, throughput 2.5K wps\n",
      "[Epoch 14 Batch 50/121] avg loss 0.130922, throughput 2.54453K wps\n",
      "[Epoch 14 Batch 60/121] avg loss 0.136734, throughput 2.45097K wps\n",
      "[Epoch 14 Batch 70/121] avg loss 0.172998, throughput 2.48139K wps\n",
      "[Epoch 14 Batch 80/121] avg loss 0.19878, throughput 2.19298K wps\n",
      "[Epoch 14 Batch 90/121] avg loss 0.212247, throughput 2.33645K wps\n",
      "[Epoch 14 Batch 100/121] avg loss 0.138316, throughput 2.51889K wps\n",
      "[Epoch 14 Batch 110/121] avg loss 0.173411, throughput 2.42131K wps\n",
      "[Epoch 14 Batch 120/121] avg loss 0.128649, throughput 2.68097K wps\n",
      "[Epoch 14] train avg loss 0.156564, train avg r2 0.780099,throughput 2.46989K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 15 Batch 10/121] avg loss 0.107324, throughput 2.36967K wps\n",
      "[Epoch 15 Batch 20/121] avg loss 0.117962, throughput 2.55102K wps\n",
      "[Epoch 15 Batch 30/121] avg loss 0.181933, throughput 2.59741K wps\n",
      "[Epoch 15 Batch 40/121] avg loss 0.0678131, throughput 2.49376K wps\n",
      "[Epoch 15 Batch 50/121] avg loss 0.103592, throughput 2.53807K wps\n",
      "[Epoch 15 Batch 60/121] avg loss 0.122566, throughput 2.44499K wps\n",
      "[Epoch 15 Batch 70/121] avg loss 0.179706, throughput 2.47525K wps\n",
      "[Epoch 15 Batch 80/121] avg loss 0.224882, throughput 2.18818K wps\n",
      "[Epoch 15 Batch 90/121] avg loss 0.225465, throughput 2.33101K wps\n",
      "[Epoch 15 Batch 100/121] avg loss 0.12751, throughput 2.53164K wps\n",
      "[Epoch 15 Batch 110/121] avg loss 0.180128, throughput 2.42718K wps\n",
      "[Epoch 15 Batch 120/121] avg loss 0.110877, throughput 2.68816K wps\n",
      "[Epoch 15] train avg loss 0.145792, train avg r2 0.804184,throughput 2.46838K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 16 Batch 10/121] avg loss 0.0995625, throughput 2.38095K wps\n",
      "[Epoch 16 Batch 20/121] avg loss 0.115382, throughput 2.54454K wps\n",
      "[Epoch 16 Batch 30/121] avg loss 0.140205, throughput 2.60417K wps\n",
      "[Epoch 16 Batch 40/121] avg loss 0.0581984, throughput 2.48756K wps\n",
      "[Epoch 16 Batch 50/121] avg loss 0.0903538, throughput 2.53808K wps\n",
      "[Epoch 16 Batch 60/121] avg loss 0.126286, throughput 2.45097K wps\n",
      "[Epoch 16 Batch 70/121] avg loss 0.170606, throughput 2.48138K wps\n",
      "[Epoch 16 Batch 80/121] avg loss 0.166367, throughput 2.18819K wps\n",
      "[Epoch 16 Batch 90/121] avg loss 0.176785, throughput 2.331K wps\n",
      "[Epoch 16 Batch 100/121] avg loss 0.138723, throughput 2.51889K wps\n",
      "[Epoch 16 Batch 110/121] avg loss 0.155622, throughput 2.42718K wps\n",
      "[Epoch 16 Batch 120/121] avg loss 0.0963446, throughput 2.6738K wps\n",
      "[Epoch 16] train avg loss 0.127827, train avg r2 0.833763,throughput 2.46737K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 17 Batch 10/121] avg loss 0.0967463, throughput 2.36408K wps\n",
      "[Epoch 17 Batch 20/121] avg loss 0.112979, throughput 2.55102K wps\n",
      "[Epoch 17 Batch 30/121] avg loss 0.16734, throughput 2.5974K wps\n",
      "[Epoch 17 Batch 40/121] avg loss 0.062015, throughput 2.48756K wps\n",
      "[Epoch 17 Batch 50/121] avg loss 0.0904813, throughput 2.55101K wps\n",
      "[Epoch 17 Batch 60/121] avg loss 0.111087, throughput 2.45701K wps\n",
      "[Epoch 17 Batch 70/121] avg loss 0.187806, throughput 2.48756K wps\n",
      "[Epoch 17 Batch 80/121] avg loss 0.198191, throughput 2.19299K wps\n",
      "[Epoch 17 Batch 90/121] avg loss 0.148823, throughput 2.33644K wps\n",
      "[Epoch 17 Batch 100/121] avg loss 0.0945734, throughput 2.52526K wps\n",
      "[Epoch 17 Batch 110/121] avg loss 0.128179, throughput 2.42718K wps\n",
      "[Epoch 17 Batch 120/121] avg loss 0.0904121, throughput 2.68096K wps\n",
      "[Epoch 17] train avg loss 0.123993, train avg r2 0.842777,throughput 2.4704K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 18 Batch 10/121] avg loss 0.103665, throughput 2.38664K wps\n",
      "[Epoch 18 Batch 20/121] avg loss 0.107173, throughput 2.55754K wps\n",
      "[Epoch 18 Batch 30/121] avg loss 0.11516, throughput 2.60417K wps\n",
      "[Epoch 18 Batch 40/121] avg loss 0.0594941, throughput 2.49376K wps\n",
      "[Epoch 18 Batch 50/121] avg loss 0.0955023, throughput 2.54454K wps\n",
      "[Epoch 18 Batch 60/121] avg loss 0.11094, throughput 2.45097K wps\n",
      "[Epoch 18 Batch 70/121] avg loss 0.142016, throughput 2.48756K wps\n",
      "[Epoch 18 Batch 80/121] avg loss 0.165842, throughput 2.18819K wps\n",
      "[Epoch 18 Batch 90/121] avg loss 0.170048, throughput 2.33644K wps\n",
      "[Epoch 18 Batch 100/121] avg loss 0.0967246, throughput 2.53164K wps\n",
      "[Epoch 18 Batch 110/121] avg loss 0.088768, throughput 2.41546K wps\n",
      "[Epoch 18 Batch 120/121] avg loss 0.107105, throughput 2.68096K wps\n",
      "[Epoch 18] train avg loss 0.113485, train avg r2 0.860139,throughput 2.47191K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 19 Batch 10/121] avg loss 0.0905463, throughput 2.38095K wps\n",
      "[Epoch 19 Batch 20/121] avg loss 0.0970002, throughput 2.53166K wps\n",
      "[Epoch 19 Batch 30/121] avg loss 0.105308, throughput 2.61096K wps\n",
      "[Epoch 19 Batch 40/121] avg loss 0.0476482, throughput 2.50625K wps\n",
      "[Epoch 19 Batch 50/121] avg loss 0.0687076, throughput 2.55103K wps\n",
      "[Epoch 19 Batch 60/121] avg loss 0.120231, throughput 2.45098K wps\n",
      "[Epoch 19 Batch 70/121] avg loss 0.160106, throughput 2.46912K wps\n",
      "[Epoch 19 Batch 80/121] avg loss 0.189916, throughput 2.19781K wps\n",
      "[Epoch 19 Batch 90/121] avg loss 0.219706, throughput 2.33644K wps\n",
      "[Epoch 19 Batch 100/121] avg loss 0.0903748, throughput 2.53164K wps\n",
      "[Epoch 19 Batch 110/121] avg loss 0.0963119, throughput 2.42718K wps\n",
      "[Epoch 19 Batch 120/121] avg loss 0.108336, throughput 2.68817K wps\n",
      "[Epoch 19] train avg loss 0.116157, train avg r2 0.857747,throughput 2.4704K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 20 Batch 10/121] avg loss 0.103292, throughput 2.38664K wps\n",
      "[Epoch 20 Batch 20/121] avg loss 0.09394, throughput 2.55754K wps\n",
      "[Epoch 20 Batch 30/121] avg loss 0.0954392, throughput 2.60417K wps\n",
      "[Epoch 20 Batch 40/121] avg loss 0.0425338, throughput 2.49999K wps\n",
      "[Epoch 20 Batch 50/121] avg loss 0.0818709, throughput 2.53164K wps\n",
      "[Epoch 20 Batch 60/121] avg loss 0.128487, throughput 2.44499K wps\n",
      "[Epoch 20 Batch 70/121] avg loss 0.264514, throughput 2.48139K wps\n",
      "[Epoch 20 Batch 80/121] avg loss 0.350394, throughput 2.19298K wps\n",
      "[Epoch 20 Batch 90/121] avg loss 0.85507, throughput 2.331K wps\n",
      "[Epoch 20 Batch 100/121] avg loss 0.249787, throughput 2.52525K wps\n",
      "[Epoch 20 Batch 110/121] avg loss 0.183033, throughput 2.4213K wps\n",
      "[Epoch 20 Batch 120/121] avg loss 0.136006, throughput 2.68818K wps\n",
      "[Epoch 20] train avg loss 0.215265, train avg r2 0.723582,throughput 2.4704K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 21 Batch 10/121] avg loss 0.0982223, throughput 2.38664K wps\n",
      "[Epoch 21 Batch 20/121] avg loss 0.112848, throughput 2.54452K wps\n",
      "[Epoch 21 Batch 30/121] avg loss 0.24478, throughput 2.61096K wps\n",
      "[Epoch 21 Batch 40/121] avg loss 0.0686628, throughput 2.50626K wps\n",
      "[Epoch 21 Batch 50/121] avg loss 0.110539, throughput 2.55102K wps\n",
      "[Epoch 21 Batch 60/121] avg loss 0.144414, throughput 2.45098K wps\n",
      "[Epoch 21 Batch 70/121] avg loss 0.178268, throughput 2.48139K wps\n",
      "[Epoch 21 Batch 80/121] avg loss 0.261938, throughput 2.19298K wps\n",
      "[Epoch 21 Batch 90/121] avg loss 0.435611, throughput 2.33645K wps\n",
      "[Epoch 21 Batch 100/121] avg loss 0.127345, throughput 2.52525K wps\n",
      "[Epoch 21 Batch 110/121] avg loss 0.308599, throughput 2.42718K wps\n",
      "[Epoch 21 Batch 120/121] avg loss 0.171612, throughput 2.68818K wps\n",
      "[Epoch 21] train avg loss 0.1885, train avg r2 0.762291,throughput 2.47242K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 22 Batch 10/121] avg loss 0.145486, throughput 2.39808K wps\n",
      "[Epoch 22 Batch 20/121] avg loss 0.143788, throughput 2.56409K wps\n",
      "[Epoch 22 Batch 30/121] avg loss 0.162527, throughput 2.60416K wps\n",
      "[Epoch 22 Batch 40/121] avg loss 0.0711243, throughput 2.50627K wps\n",
      "[Epoch 22 Batch 50/121] avg loss 0.0708306, throughput 2.53806K wps\n",
      "[Epoch 22 Batch 60/121] avg loss 0.114305, throughput 2.457K wps\n",
      "[Epoch 22 Batch 70/121] avg loss 0.176398, throughput 2.47525K wps\n",
      "[Epoch 22 Batch 80/121] avg loss 0.350688, throughput 2.18818K wps\n",
      "[Epoch 22 Batch 90/121] avg loss 0.238618, throughput 2.32559K wps\n",
      "[Epoch 22 Batch 100/121] avg loss 0.124564, throughput 2.51889K wps\n",
      "[Epoch 22 Batch 110/121] avg loss 0.239405, throughput 2.4213K wps\n",
      "[Epoch 22 Batch 120/121] avg loss 0.146767, throughput 2.68817K wps\n",
      "[Epoch 22] train avg loss 0.1653, train avg r2 0.783251,throughput 2.47191K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 23 Batch 10/121] avg loss 0.135214, throughput 2.38095K wps\n",
      "[Epoch 23 Batch 20/121] avg loss 0.124649, throughput 2.55103K wps\n",
      "[Epoch 23 Batch 30/121] avg loss 0.132866, throughput 2.59066K wps\n",
      "[Epoch 23 Batch 40/121] avg loss 0.0471583, throughput 2.5K wps\n",
      "[Epoch 23 Batch 50/121] avg loss 0.0617919, throughput 2.53807K wps\n",
      "[Epoch 23 Batch 60/121] avg loss 0.0976741, throughput 2.45097K wps\n",
      "[Epoch 23 Batch 70/121] avg loss 0.129838, throughput 2.48756K wps\n",
      "[Epoch 23 Batch 80/121] avg loss 0.244875, throughput 2.19298K wps\n",
      "[Epoch 23 Batch 90/121] avg loss 0.271996, throughput 2.331K wps\n",
      "[Epoch 23 Batch 100/121] avg loss 0.128386, throughput 2.51256K wps\n",
      "[Epoch 23 Batch 110/121] avg loss 0.223715, throughput 2.42131K wps\n",
      "[Epoch 23 Batch 120/121] avg loss 0.126077, throughput 2.68816K wps\n",
      "[Epoch 23] train avg loss 0.143614, train avg r2 0.813372,throughput 2.46737K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 24 Batch 10/121] avg loss 0.120403, throughput 2.38104K wps\n",
      "[Epoch 24 Batch 20/121] avg loss 0.132804, throughput 2.54453K wps\n",
      "[Epoch 24 Batch 30/121] avg loss 0.134806, throughput 2.60416K wps\n",
      "[Epoch 24 Batch 40/121] avg loss 0.0546304, throughput 2.5K wps\n",
      "[Epoch 24 Batch 50/121] avg loss 0.0617625, throughput 2.52525K wps\n",
      "[Epoch 24 Batch 60/121] avg loss 0.0941111, throughput 2.45098K wps\n",
      "[Epoch 24 Batch 70/121] avg loss 0.124022, throughput 2.48756K wps\n",
      "[Epoch 24 Batch 80/121] avg loss 0.182094, throughput 2.19298K wps\n",
      "[Epoch 24 Batch 90/121] avg loss 0.179764, throughput 2.33645K wps\n",
      "[Epoch 24 Batch 100/121] avg loss 0.262378, throughput 2.53164K wps\n",
      "[Epoch 24 Batch 110/121] avg loss 0.181524, throughput 2.42718K wps\n",
      "[Epoch 24 Batch 120/121] avg loss 0.141506, throughput 2.68816K wps\n",
      "[Epoch 24] train avg loss 0.139092, train avg r2 0.808587,throughput 2.4694K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 25 Batch 10/121] avg loss 0.113968, throughput 2.38663K wps\n",
      "[Epoch 25 Batch 20/121] avg loss 0.121846, throughput 2.55103K wps\n",
      "[Epoch 25 Batch 30/121] avg loss 0.110063, throughput 2.59067K wps\n",
      "[Epoch 25 Batch 40/121] avg loss 0.0555351, throughput 2.49999K wps\n",
      "[Epoch 25 Batch 50/121] avg loss 0.0633995, throughput 2.53807K wps\n",
      "[Epoch 25 Batch 60/121] avg loss 0.0854172, throughput 2.45098K wps\n",
      "[Epoch 25 Batch 70/121] avg loss 0.121813, throughput 2.48139K wps\n",
      "[Epoch 25 Batch 80/121] avg loss 0.139061, throughput 2.19298K wps\n",
      "[Epoch 25 Batch 90/121] avg loss 0.100257, throughput 2.33644K wps\n",
      "[Epoch 25 Batch 100/121] avg loss 0.130243, throughput 2.53163K wps\n",
      "[Epoch 25 Batch 110/121] avg loss 0.0951137, throughput 2.40964K wps\n",
      "[Epoch 25 Batch 120/121] avg loss 0.114414, throughput 2.68096K wps\n",
      "[Epoch 25] train avg loss 0.104247, train avg r2 0.861221,throughput 2.46838K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 26 Batch 10/121] avg loss 0.086723, throughput 2.36968K wps\n",
      "[Epoch 26 Batch 20/121] avg loss 0.0950117, throughput 2.55102K wps\n",
      "[Epoch 26 Batch 30/121] avg loss 0.0822307, throughput 2.60416K wps\n",
      "[Epoch 26 Batch 40/121] avg loss 0.0375455, throughput 2.49999K wps\n",
      "[Epoch 26 Batch 50/121] avg loss 0.0527833, throughput 2.53808K wps\n",
      "[Epoch 26 Batch 60/121] avg loss 0.0872946, throughput 2.45096K wps\n",
      "[Epoch 26 Batch 70/121] avg loss 0.115595, throughput 2.48757K wps\n",
      "[Epoch 26 Batch 80/121] avg loss 0.129359, throughput 2.19298K wps\n",
      "[Epoch 26 Batch 90/121] avg loss 0.116513, throughput 2.33099K wps\n",
      "[Epoch 26 Batch 100/121] avg loss 0.0969332, throughput 2.52525K wps\n",
      "[Epoch 26 Batch 110/121] avg loss 0.0775041, throughput 2.42131K wps\n",
      "[Epoch 26 Batch 120/121] avg loss 0.0996944, throughput 2.68097K wps\n",
      "[Epoch 26] train avg loss 0.0897298, train avg r2 0.885823,throughput 2.46838K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 27 Batch 10/121] avg loss 0.0808467, throughput 2.38095K wps\n",
      "[Epoch 27 Batch 20/121] avg loss 0.0904776, throughput 2.55101K wps\n",
      "[Epoch 27 Batch 30/121] avg loss 0.0804554, throughput 2.58398K wps\n",
      "[Epoch 27 Batch 40/121] avg loss 0.0386761, throughput 2.50626K wps\n",
      "[Epoch 27 Batch 50/121] avg loss 0.0545632, throughput 2.53808K wps\n",
      "[Epoch 27 Batch 60/121] avg loss 0.0756902, throughput 2.45098K wps\n",
      "[Epoch 27 Batch 70/121] avg loss 0.104407, throughput 2.48139K wps\n",
      "[Epoch 27 Batch 80/121] avg loss 0.141725, throughput 2.18818K wps\n",
      "[Epoch 27 Batch 90/121] avg loss 0.116555, throughput 2.331K wps\n",
      "[Epoch 27 Batch 100/121] avg loss 0.108945, throughput 2.52525K wps\n",
      "[Epoch 27 Batch 110/121] avg loss 0.0685439, throughput 2.42131K wps\n",
      "[Epoch 27 Batch 120/121] avg loss 0.091859, throughput 2.68097K wps\n",
      "[Epoch 27] train avg loss 0.0877079, train avg r2 0.88859,throughput 2.46838K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 28 Batch 10/121] avg loss 0.0792303, throughput 2.3753K wps\n",
      "[Epoch 28 Batch 20/121] avg loss 0.0793222, throughput 2.55754K wps\n",
      "[Epoch 28 Batch 30/121] avg loss 0.0686673, throughput 2.61096K wps\n",
      "[Epoch 28 Batch 40/121] avg loss 0.0324351, throughput 2.5K wps\n",
      "[Epoch 28 Batch 50/121] avg loss 0.0575886, throughput 2.55102K wps\n",
      "[Epoch 28 Batch 60/121] avg loss 0.0781506, throughput 2.45699K wps\n",
      "[Epoch 28 Batch 70/121] avg loss 0.0968104, throughput 2.48756K wps\n",
      "[Epoch 28 Batch 80/121] avg loss 0.131985, throughput 2.20264K wps\n",
      "[Epoch 28 Batch 90/121] avg loss 0.0784906, throughput 2.33645K wps\n",
      "[Epoch 28 Batch 100/121] avg loss 0.101681, throughput 2.53164K wps\n",
      "[Epoch 28 Batch 110/121] avg loss 0.0626643, throughput 2.42718K wps\n",
      "[Epoch 28 Batch 120/121] avg loss 0.0890675, throughput 2.68818K wps\n",
      "[Epoch 28] train avg loss 0.0796438, train avg r2 0.8988,throughput 2.47393K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 29 Batch 10/121] avg loss 0.0660927, throughput 2.38663K wps\n",
      "[Epoch 29 Batch 20/121] avg loss 0.0726341, throughput 2.54453K wps\n",
      "[Epoch 29 Batch 30/121] avg loss 0.0669321, throughput 2.60417K wps\n",
      "[Epoch 29 Batch 40/121] avg loss 0.0320473, throughput 2.48756K wps\n",
      "[Epoch 29 Batch 50/121] avg loss 0.0541576, throughput 2.54453K wps\n",
      "[Epoch 29 Batch 60/121] avg loss 0.0696208, throughput 2.457K wps\n",
      "[Epoch 29 Batch 70/121] avg loss 0.0919639, throughput 2.47523K wps\n",
      "[Epoch 29 Batch 80/121] avg loss 0.10394, throughput 2.19298K wps\n",
      "[Epoch 29 Batch 90/121] avg loss 0.0810921, throughput 2.33645K wps\n",
      "[Epoch 29 Batch 100/121] avg loss 0.082132, throughput 2.52524K wps\n",
      "[Epoch 29 Batch 110/121] avg loss 0.0639797, throughput 2.42131K wps\n",
      "[Epoch 29 Batch 120/121] avg loss 0.0775766, throughput 2.68095K wps\n",
      "[Epoch 29] train avg loss 0.0718191, train avg r2 0.911107,throughput 2.46888K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 30 Batch 10/121] avg loss 0.0724139, throughput 2.37529K wps\n",
      "[Epoch 30 Batch 20/121] avg loss 0.0576429, throughput 2.55101K wps\n",
      "[Epoch 30 Batch 30/121] avg loss 0.0698263, throughput 2.59739K wps\n",
      "[Epoch 30 Batch 40/121] avg loss 0.034037, throughput 2.5K wps\n",
      "[Epoch 30 Batch 50/121] avg loss 0.0815825, throughput 2.54453K wps\n",
      "[Epoch 30 Batch 60/121] avg loss 0.0728433, throughput 2.39808K wps\n",
      "[Epoch 30 Batch 70/121] avg loss 0.0900745, throughput 2.36406K wps\n",
      "[Epoch 30 Batch 80/121] avg loss 0.0827715, throughput 2.18818K wps\n",
      "[Epoch 30 Batch 90/121] avg loss 0.0975545, throughput 2.29885K wps\n",
      "[Epoch 30 Batch 100/121] avg loss 0.0937216, throughput 2.49377K wps\n",
      "[Epoch 30 Batch 110/121] avg loss 0.143181, throughput 2.41546K wps\n",
      "[Epoch 30 Batch 120/121] avg loss 0.0766221, throughput 2.61098K wps\n",
      "[Epoch 30] train avg loss 0.08102, train avg r2 0.903182,throughput 2.44247K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 31 Batch 10/121] avg loss 0.0965956, throughput 2.38663K wps\n",
      "[Epoch 31 Batch 20/121] avg loss 0.0692578, throughput 2.55755K wps\n",
      "[Epoch 31 Batch 30/121] avg loss 0.0767669, throughput 2.60416K wps\n",
      "[Epoch 31 Batch 40/121] avg loss 0.0350185, throughput 2.48139K wps\n",
      "[Epoch 31 Batch 50/121] avg loss 0.0809023, throughput 2.54454K wps\n",
      "[Epoch 31 Batch 60/121] avg loss 0.0786474, throughput 2.44499K wps\n",
      "[Epoch 31 Batch 70/121] avg loss 0.0779322, throughput 2.48139K wps\n",
      "[Epoch 31 Batch 80/121] avg loss 0.0993172, throughput 2.18818K wps\n",
      "[Epoch 31 Batch 90/121] avg loss 0.101948, throughput 2.331K wps\n",
      "[Epoch 31 Batch 100/121] avg loss 0.0635044, throughput 2.52524K wps\n",
      "[Epoch 31 Batch 110/121] avg loss 0.125651, throughput 2.40964K wps\n",
      "[Epoch 31 Batch 120/121] avg loss 0.0813173, throughput 2.68816K wps\n",
      "[Epoch 31] train avg loss 0.0822056, train avg r2 0.902138,throughput 2.46838K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 32 Batch 10/121] avg loss 0.0771657, throughput 2.37529K wps\n",
      "[Epoch 32 Batch 20/121] avg loss 0.0662635, throughput 2.54452K wps\n",
      "[Epoch 32 Batch 30/121] avg loss 0.0812804, throughput 2.60416K wps\n",
      "[Epoch 32 Batch 40/121] avg loss 0.0513582, throughput 2.5K wps\n",
      "[Epoch 32 Batch 50/121] avg loss 0.0737961, throughput 2.54453K wps\n",
      "[Epoch 32 Batch 60/121] avg loss 0.0697945, throughput 2.44499K wps\n",
      "[Epoch 32 Batch 70/121] avg loss 0.0816001, throughput 2.47525K wps\n",
      "[Epoch 32 Batch 80/121] avg loss 0.0817122, throughput 2.19297K wps\n",
      "[Epoch 32 Batch 90/121] avg loss 0.060994, throughput 2.33645K wps\n",
      "[Epoch 32 Batch 100/121] avg loss 0.0597883, throughput 2.52525K wps\n",
      "[Epoch 32 Batch 110/121] avg loss 0.0709295, throughput 2.41546K wps\n",
      "[Epoch 32 Batch 120/121] avg loss 0.0776103, throughput 2.68817K wps\n",
      "[Epoch 32] train avg loss 0.0710043, train avg r2 0.916867,throughput 2.46888K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 33 Batch 10/121] avg loss 0.0572545, throughput 2.38663K wps\n",
      "[Epoch 33 Batch 20/121] avg loss 0.0648114, throughput 2.54453K wps\n",
      "[Epoch 33 Batch 30/121] avg loss 0.0627713, throughput 2.60417K wps\n",
      "[Epoch 33 Batch 40/121] avg loss 0.0352661, throughput 2.5K wps\n",
      "[Epoch 33 Batch 50/121] avg loss 0.0555728, throughput 2.53165K wps\n",
      "[Epoch 33 Batch 60/121] avg loss 0.0693745, throughput 2.45097K wps\n",
      "[Epoch 33 Batch 70/121] avg loss 0.0750343, throughput 2.48139K wps\n",
      "[Epoch 33 Batch 80/121] avg loss 0.0782378, throughput 2.18818K wps\n",
      "[Epoch 33 Batch 90/121] avg loss 0.0533106, throughput 2.331K wps\n",
      "[Epoch 33 Batch 100/121] avg loss 0.0525294, throughput 2.51888K wps\n",
      "[Epoch 33 Batch 110/121] avg loss 0.0552846, throughput 2.41546K wps\n",
      "[Epoch 33 Batch 120/121] avg loss 0.0654826, throughput 2.68095K wps\n",
      "[Epoch 33] train avg loss 0.0604112, train avg r2 0.929152,throughput 2.46687K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 34 Batch 10/121] avg loss 0.0465003, throughput 2.39234K wps\n",
      "[Epoch 34 Batch 20/121] avg loss 0.0532176, throughput 2.53808K wps\n",
      "[Epoch 34 Batch 30/121] avg loss 0.0571461, throughput 2.59739K wps\n",
      "[Epoch 34 Batch 40/121] avg loss 0.0289276, throughput 2.5K wps\n",
      "[Epoch 34 Batch 50/121] avg loss 0.0455786, throughput 2.53806K wps\n",
      "[Epoch 34 Batch 60/121] avg loss 0.0573722, throughput 2.445K wps\n",
      "[Epoch 34 Batch 70/121] avg loss 0.0820638, throughput 2.47524K wps\n",
      "[Epoch 34 Batch 80/121] avg loss 0.0754478, throughput 2.19299K wps\n",
      "[Epoch 34 Batch 90/121] avg loss 0.0517263, throughput 2.331K wps\n",
      "[Epoch 34 Batch 100/121] avg loss 0.0493651, throughput 2.51888K wps\n",
      "[Epoch 34 Batch 110/121] avg loss 0.0478534, throughput 2.42131K wps\n",
      "[Epoch 34 Batch 120/121] avg loss 0.0579094, throughput 2.67379K wps\n",
      "[Epoch 34] train avg loss 0.0544224, train avg r2 0.936826,throughput 2.46788K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 35 Batch 10/121] avg loss 0.0451386, throughput 2.38095K wps\n",
      "[Epoch 35 Batch 20/121] avg loss 0.0495295, throughput 2.53807K wps\n",
      "[Epoch 35 Batch 30/121] avg loss 0.0499926, throughput 2.61096K wps\n",
      "[Epoch 35 Batch 40/121] avg loss 0.0238342, throughput 2.50626K wps\n",
      "[Epoch 35 Batch 50/121] avg loss 0.0461506, throughput 2.53165K wps\n",
      "[Epoch 35 Batch 60/121] avg loss 0.0557835, throughput 2.45098K wps\n",
      "[Epoch 35 Batch 70/121] avg loss 0.074137, throughput 2.48139K wps\n",
      "[Epoch 35 Batch 80/121] avg loss 0.0770743, throughput 2.19298K wps\n",
      "[Epoch 35 Batch 90/121] avg loss 0.052368, throughput 2.33645K wps\n",
      "[Epoch 35 Batch 100/121] avg loss 0.0488565, throughput 2.53164K wps\n",
      "[Epoch 35 Batch 110/121] avg loss 0.0464077, throughput 2.42131K wps\n",
      "[Epoch 35 Batch 120/121] avg loss 0.0487956, throughput 2.68817K wps\n",
      "[Epoch 35] train avg loss 0.0514983, train avg r2 0.94041,throughput 2.4704K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 36 Batch 10/121] avg loss 0.0383703, throughput 2.38095K wps\n",
      "[Epoch 36 Batch 20/121] avg loss 0.0476328, throughput 2.55755K wps\n",
      "[Epoch 36 Batch 30/121] avg loss 0.0424159, throughput 2.57731K wps\n",
      "[Epoch 36 Batch 40/121] avg loss 0.0225869, throughput 2.50001K wps\n",
      "[Epoch 36 Batch 50/121] avg loss 0.0419854, throughput 2.53808K wps\n",
      "[Epoch 36 Batch 60/121] avg loss 0.0514616, throughput 2.45097K wps\n",
      "[Epoch 36 Batch 70/121] avg loss 0.0719777, throughput 2.48138K wps\n",
      "[Epoch 36 Batch 80/121] avg loss 0.0787899, throughput 2.19298K wps\n",
      "[Epoch 36 Batch 90/121] avg loss 0.0494648, throughput 2.32558K wps\n",
      "[Epoch 36 Batch 100/121] avg loss 0.0474941, throughput 2.51889K wps\n",
      "[Epoch 36 Batch 110/121] avg loss 0.0404516, throughput 2.41545K wps\n",
      "[Epoch 36 Batch 120/121] avg loss 0.0457155, throughput 2.68818K wps\n",
      "[Epoch 36] train avg loss 0.0481797, train avg r2 0.945242,throughput 2.46687K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 37 Batch 10/121] avg loss 0.0378339, throughput 2.39234K wps\n",
      "[Epoch 37 Batch 20/121] avg loss 0.0438041, throughput 2.55755K wps\n",
      "[Epoch 37 Batch 30/121] avg loss 0.0444725, throughput 2.60417K wps\n",
      "[Epoch 37 Batch 40/121] avg loss 0.0219968, throughput 2.50625K wps\n",
      "[Epoch 37 Batch 50/121] avg loss 0.0394242, throughput 2.52525K wps\n",
      "[Epoch 37 Batch 60/121] avg loss 0.0545427, throughput 2.457K wps\n",
      "[Epoch 37 Batch 70/121] avg loss 0.065813, throughput 2.47525K wps\n",
      "[Epoch 37 Batch 80/121] avg loss 0.0781243, throughput 2.19298K wps\n",
      "[Epoch 37 Batch 90/121] avg loss 0.0524053, throughput 2.33101K wps\n",
      "[Epoch 37 Batch 100/121] avg loss 0.0440356, throughput 2.52525K wps\n",
      "[Epoch 37 Batch 110/121] avg loss 0.0451642, throughput 2.42131K wps\n",
      "[Epoch 37 Batch 120/121] avg loss 0.040411, throughput 2.68097K wps\n",
      "[Epoch 37] train avg loss 0.0473201, train avg r2 0.946915,throughput 2.4709K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 38 Batch 10/121] avg loss 0.0342615, throughput 2.36407K wps\n",
      "[Epoch 38 Batch 20/121] avg loss 0.0447074, throughput 2.54453K wps\n",
      "[Epoch 38 Batch 30/121] avg loss 0.0476651, throughput 2.59068K wps\n",
      "[Epoch 38 Batch 40/121] avg loss 0.0220344, throughput 2.50625K wps\n",
      "[Epoch 38 Batch 50/121] avg loss 0.0386183, throughput 2.55103K wps\n",
      "[Epoch 38 Batch 60/121] avg loss 0.0499542, throughput 2.44498K wps\n",
      "[Epoch 38 Batch 70/121] avg loss 0.0651475, throughput 2.47526K wps\n",
      "[Epoch 38 Batch 80/121] avg loss 0.0679192, throughput 2.19298K wps\n",
      "[Epoch 38 Batch 90/121] avg loss 0.0490842, throughput 2.32558K wps\n",
      "[Epoch 38 Batch 100/121] avg loss 0.0447092, throughput 2.52525K wps\n",
      "[Epoch 38 Batch 110/121] avg loss 0.0486661, throughput 2.42718K wps\n",
      "[Epoch 38 Batch 120/121] avg loss 0.0432847, throughput 2.68096K wps\n",
      "[Epoch 38] train avg loss 0.0463281, train avg r2 0.946785,throughput 2.46637K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 39 Batch 10/121] avg loss 0.0366478, throughput 2.37529K wps\n",
      "[Epoch 39 Batch 20/121] avg loss 0.0410321, throughput 2.55102K wps\n",
      "[Epoch 39 Batch 30/121] avg loss 0.0493794, throughput 2.60417K wps\n",
      "[Epoch 39 Batch 40/121] avg loss 0.0250657, throughput 2.50627K wps\n",
      "[Epoch 39 Batch 50/121] avg loss 0.0395915, throughput 2.53806K wps\n",
      "[Epoch 39 Batch 60/121] avg loss 0.0482958, throughput 2.45701K wps\n",
      "[Epoch 39 Batch 70/121] avg loss 0.0607577, throughput 2.48755K wps\n",
      "[Epoch 39 Batch 80/121] avg loss 0.0718518, throughput 2.19298K wps\n",
      "[Epoch 39 Batch 90/121] avg loss 0.0487818, throughput 2.33645K wps\n",
      "[Epoch 39 Batch 100/121] avg loss 0.0442162, throughput 2.51889K wps\n",
      "[Epoch 39 Batch 110/121] avg loss 0.0453396, throughput 2.42131K wps\n",
      "[Epoch 39 Batch 120/121] avg loss 0.0408616, throughput 2.68818K wps\n",
      "[Epoch 39] train avg loss 0.0459668, train avg r2 0.948064,throughput 2.4709K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 40 Batch 10/121] avg loss 0.0346566, throughput 2.39234K wps\n",
      "[Epoch 40 Batch 20/121] avg loss 0.0430009, throughput 2.55752K wps\n",
      "[Epoch 40 Batch 30/121] avg loss 0.0477823, throughput 2.58399K wps\n",
      "[Epoch 40 Batch 40/121] avg loss 0.0224532, throughput 2.50626K wps\n",
      "[Epoch 40 Batch 50/121] avg loss 0.039347, throughput 2.53164K wps\n",
      "[Epoch 40 Batch 60/121] avg loss 0.0464699, throughput 2.45098K wps\n",
      "[Epoch 40 Batch 70/121] avg loss 0.059727, throughput 2.47524K wps\n",
      "[Epoch 40 Batch 80/121] avg loss 0.0688138, throughput 2.18818K wps\n",
      "[Epoch 40 Batch 90/121] avg loss 0.0548041, throughput 2.33645K wps\n",
      "[Epoch 40 Batch 100/121] avg loss 0.0558938, throughput 2.51887K wps\n",
      "[Epoch 40 Batch 110/121] avg loss 0.0466698, throughput 2.42132K wps\n",
      "[Epoch 40 Batch 120/121] avg loss 0.0664993, throughput 2.68097K wps\n",
      "[Epoch 40] train avg loss 0.0488517, train avg r2 0.941871,throughput 2.46737K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 41 Batch 10/121] avg loss 0.0505968, throughput 2.36967K wps\n",
      "[Epoch 41 Batch 20/121] avg loss 0.0399148, throughput 2.55101K wps\n",
      "[Epoch 41 Batch 30/121] avg loss 0.0448929, throughput 2.5974K wps\n",
      "[Epoch 41 Batch 40/121] avg loss 0.0209621, throughput 2.5K wps\n",
      "[Epoch 41 Batch 50/121] avg loss 0.0401043, throughput 2.54453K wps\n",
      "[Epoch 41 Batch 60/121] avg loss 0.0484417, throughput 2.45099K wps\n",
      "[Epoch 41 Batch 70/121] avg loss 0.0590673, throughput 2.48138K wps\n",
      "[Epoch 41 Batch 80/121] avg loss 0.0661682, throughput 2.19298K wps\n",
      "[Epoch 41 Batch 90/121] avg loss 0.0577072, throughput 2.33645K wps\n",
      "[Epoch 41 Batch 100/121] avg loss 0.0563798, throughput 2.52525K wps\n",
      "[Epoch 41 Batch 110/121] avg loss 0.0388296, throughput 2.42131K wps\n",
      "[Epoch 41 Batch 120/121] avg loss 0.0598322, throughput 2.68096K wps\n",
      "[Epoch 41] train avg loss 0.0485586, train avg r2 0.941998,throughput 2.46939K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 42 Batch 10/121] avg loss 0.050237, throughput 2.38663K wps\n",
      "[Epoch 42 Batch 20/121] avg loss 0.0422705, throughput 2.55102K wps\n",
      "[Epoch 42 Batch 30/121] avg loss 0.0429212, throughput 2.5974K wps\n",
      "[Epoch 42 Batch 40/121] avg loss 0.0222611, throughput 2.48137K wps\n",
      "[Epoch 42 Batch 50/121] avg loss 0.0368339, throughput 2.53809K wps\n",
      "[Epoch 42 Batch 60/121] avg loss 0.0464031, throughput 2.445K wps\n",
      "[Epoch 42 Batch 70/121] avg loss 0.0590545, throughput 2.48138K wps\n",
      "[Epoch 42 Batch 80/121] avg loss 0.0628245, throughput 2.19298K wps\n",
      "[Epoch 42 Batch 90/121] avg loss 0.049516, throughput 2.33645K wps\n",
      "[Epoch 42 Batch 100/121] avg loss 0.0576457, throughput 2.51254K wps\n",
      "[Epoch 42 Batch 110/121] avg loss 0.0394144, throughput 2.41549K wps\n",
      "[Epoch 42 Batch 120/121] avg loss 0.0551293, throughput 2.68096K wps\n",
      "[Epoch 42] train avg loss 0.0470304, train avg r2 0.94327,throughput 2.46536K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 43 Batch 10/121] avg loss 0.0506121, throughput 2.36967K wps\n",
      "[Epoch 43 Batch 20/121] avg loss 0.0449649, throughput 2.55102K wps\n",
      "[Epoch 43 Batch 30/121] avg loss 0.0426094, throughput 2.61096K wps\n",
      "[Epoch 43 Batch 40/121] avg loss 0.0213788, throughput 2.5K wps\n",
      "[Epoch 43 Batch 50/121] avg loss 0.0401564, throughput 2.53806K wps\n",
      "[Epoch 43 Batch 60/121] avg loss 0.045534, throughput 2.45098K wps\n",
      "[Epoch 43 Batch 70/121] avg loss 0.0595033, throughput 2.4814K wps\n",
      "[Epoch 43 Batch 80/121] avg loss 0.0606125, throughput 2.19298K wps\n",
      "[Epoch 43 Batch 90/121] avg loss 0.0460801, throughput 2.33099K wps\n",
      "[Epoch 43 Batch 100/121] avg loss 0.050113, throughput 2.5189K wps\n",
      "[Epoch 43 Batch 110/121] avg loss 0.0342802, throughput 2.42131K wps\n",
      "[Epoch 43 Batch 120/121] avg loss 0.0477027, throughput 2.68096K wps\n",
      "[Epoch 43] train avg loss 0.0452865, train avg r2 0.94567,throughput 2.46788K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 44 Batch 10/121] avg loss 0.0503869, throughput 2.38663K wps\n",
      "[Epoch 44 Batch 20/121] avg loss 0.0452947, throughput 2.55103K wps\n",
      "[Epoch 44 Batch 30/121] avg loss 0.0420864, throughput 2.60416K wps\n",
      "[Epoch 44 Batch 40/121] avg loss 0.0224233, throughput 2.48757K wps\n",
      "[Epoch 44 Batch 50/121] avg loss 0.0380935, throughput 2.54453K wps\n",
      "[Epoch 44 Batch 60/121] avg loss 0.0453226, throughput 2.44499K wps\n",
      "[Epoch 44 Batch 70/121] avg loss 0.0527, throughput 2.48139K wps\n",
      "[Epoch 44 Batch 80/121] avg loss 0.0636087, throughput 2.19298K wps\n",
      "[Epoch 44 Batch 90/121] avg loss 0.0430609, throughput 2.33644K wps\n",
      "[Epoch 44 Batch 100/121] avg loss 0.0452294, throughput 2.52525K wps\n",
      "[Epoch 44 Batch 110/121] avg loss 0.0346319, throughput 2.42131K wps\n",
      "[Epoch 44 Batch 120/121] avg loss 0.0424149, throughput 2.68817K wps\n",
      "[Epoch 44] train avg loss 0.0437547, train avg r2 0.947925,throughput 2.4709K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 45 Batch 10/121] avg loss 0.045435, throughput 2.38095K wps\n",
      "[Epoch 45 Batch 20/121] avg loss 0.0462045, throughput 2.53165K wps\n",
      "[Epoch 45 Batch 30/121] avg loss 0.0402932, throughput 2.60416K wps\n",
      "[Epoch 45 Batch 40/121] avg loss 0.0219991, throughput 2.49999K wps\n",
      "[Epoch 45 Batch 50/121] avg loss 0.0366201, throughput 2.53808K wps\n",
      "[Epoch 45 Batch 60/121] avg loss 0.0442436, throughput 2.44499K wps\n",
      "[Epoch 45 Batch 70/121] avg loss 0.0545172, throughput 2.48139K wps\n",
      "[Epoch 45 Batch 80/121] avg loss 0.0628105, throughput 2.19297K wps\n",
      "[Epoch 45 Batch 90/121] avg loss 0.0438061, throughput 2.33645K wps\n",
      "[Epoch 45 Batch 100/121] avg loss 0.0460601, throughput 2.52525K wps\n",
      "[Epoch 45 Batch 110/121] avg loss 0.0329386, throughput 2.41546K wps\n",
      "[Epoch 45 Batch 120/121] avg loss 0.0359128, throughput 2.68097K wps\n",
      "[Epoch 45] train avg loss 0.042561, train avg r2 0.949565,throughput 2.46737K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 46 Batch 10/121] avg loss 0.0459499, throughput 2.39234K wps\n",
      "[Epoch 46 Batch 20/121] avg loss 0.0486718, throughput 2.55755K wps\n",
      "[Epoch 46 Batch 30/121] avg loss 0.0382284, throughput 2.60417K wps\n",
      "[Epoch 46 Batch 40/121] avg loss 0.0211359, throughput 2.49999K wps\n",
      "[Epoch 46 Batch 50/121] avg loss 0.0383969, throughput 2.53165K wps\n",
      "[Epoch 46 Batch 60/121] avg loss 0.0447847, throughput 2.45098K wps\n",
      "[Epoch 46 Batch 70/121] avg loss 0.05307, throughput 2.48139K wps\n",
      "[Epoch 46 Batch 80/121] avg loss 0.0576529, throughput 2.19298K wps\n",
      "[Epoch 46 Batch 90/121] avg loss 0.0460104, throughput 2.34194K wps\n",
      "[Epoch 46 Batch 100/121] avg loss 0.0463873, throughput 2.5189K wps\n",
      "[Epoch 46 Batch 110/121] avg loss 0.0318219, throughput 2.42717K wps\n",
      "[Epoch 46 Batch 120/121] avg loss 0.0353846, throughput 2.68096K wps\n",
      "[Epoch 46] train avg loss 0.0422732, train avg r2 0.949795,throughput 2.4714K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 47 Batch 10/121] avg loss 0.0439126, throughput 2.38095K wps\n",
      "[Epoch 47 Batch 20/121] avg loss 0.0450484, throughput 2.53806K wps\n",
      "[Epoch 47 Batch 30/121] avg loss 0.0369212, throughput 2.60416K wps\n",
      "[Epoch 47 Batch 40/121] avg loss 0.0216257, throughput 2.5K wps\n",
      "[Epoch 47 Batch 50/121] avg loss 0.0399262, throughput 2.53807K wps\n",
      "[Epoch 47 Batch 60/121] avg loss 0.043275, throughput 2.44499K wps\n",
      "[Epoch 47 Batch 70/121] avg loss 0.0522673, throughput 2.47525K wps\n",
      "[Epoch 47 Batch 80/121] avg loss 0.0559258, throughput 2.18818K wps\n",
      "[Epoch 47 Batch 90/121] avg loss 0.0409969, throughput 2.33645K wps\n",
      "[Epoch 47 Batch 100/121] avg loss 0.0452576, throughput 2.52525K wps\n",
      "[Epoch 47 Batch 110/121] avg loss 0.0313293, throughput 2.42718K wps\n",
      "[Epoch 47 Batch 120/121] avg loss 0.0342462, throughput 2.68818K wps\n",
      "[Epoch 47] train avg loss 0.0408804, train avg r2 0.951229,throughput 2.46888K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 48 Batch 10/121] avg loss 0.0437769, throughput 2.39235K wps\n",
      "[Epoch 48 Batch 20/121] avg loss 0.0463122, throughput 2.55101K wps\n",
      "[Epoch 48 Batch 30/121] avg loss 0.0368618, throughput 2.61096K wps\n",
      "[Epoch 48 Batch 40/121] avg loss 0.0199678, throughput 2.51255K wps\n",
      "[Epoch 48 Batch 50/121] avg loss 0.0393891, throughput 2.53165K wps\n",
      "[Epoch 48 Batch 60/121] avg loss 0.0430058, throughput 2.45098K wps\n",
      "[Epoch 48 Batch 70/121] avg loss 0.0521476, throughput 2.47526K wps\n",
      "[Epoch 48 Batch 80/121] avg loss 0.0562456, throughput 2.19298K wps\n",
      "[Epoch 48 Batch 90/121] avg loss 0.0412822, throughput 2.33644K wps\n",
      "[Epoch 48 Batch 100/121] avg loss 0.0397501, throughput 2.51889K wps\n",
      "[Epoch 48 Batch 110/121] avg loss 0.0303131, throughput 2.42718K wps\n",
      "[Epoch 48 Batch 120/121] avg loss 0.03079, throughput 2.68096K wps\n",
      "[Epoch 48] train avg loss 0.0399738, train avg r2 0.953292,throughput 2.47292K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 49 Batch 10/121] avg loss 0.0405339, throughput 2.38664K wps\n",
      "[Epoch 49 Batch 20/121] avg loss 0.0419389, throughput 2.55754K wps\n",
      "[Epoch 49 Batch 30/121] avg loss 0.0328011, throughput 2.58398K wps\n",
      "[Epoch 49 Batch 40/121] avg loss 0.0209924, throughput 2.5K wps\n",
      "[Epoch 49 Batch 50/121] avg loss 0.0388982, throughput 2.55101K wps\n",
      "[Epoch 49 Batch 60/121] avg loss 0.0419168, throughput 2.45097K wps\n",
      "[Epoch 49 Batch 70/121] avg loss 0.0505984, throughput 2.48755K wps\n",
      "[Epoch 49 Batch 80/121] avg loss 0.0556128, throughput 2.19299K wps\n",
      "[Epoch 49 Batch 90/121] avg loss 0.039982, throughput 2.33645K wps\n",
      "[Epoch 49 Batch 100/121] avg loss 0.0392782, throughput 2.51255K wps\n",
      "[Epoch 49 Batch 110/121] avg loss 0.0322276, throughput 2.42132K wps\n",
      "[Epoch 49 Batch 120/121] avg loss 0.0297283, throughput 2.68816K wps\n",
      "[Epoch 49] train avg loss 0.0386959, train avg r2 0.954601,throughput 2.46939K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 50 Batch 10/121] avg loss 0.0390206, throughput 2.37529K wps\n",
      "[Epoch 50 Batch 20/121] avg loss 0.0509266, throughput 2.49999K wps\n",
      "[Epoch 50 Batch 30/121] avg loss 0.0410028, throughput 2.59741K wps\n",
      "[Epoch 50 Batch 40/121] avg loss 0.021415, throughput 2.5K wps\n",
      "[Epoch 50 Batch 50/121] avg loss 0.0433411, throughput 2.52525K wps\n",
      "[Epoch 50 Batch 60/121] avg loss 0.0457732, throughput 2.43903K wps\n",
      "[Epoch 50 Batch 70/121] avg loss 0.0532416, throughput 2.48139K wps\n",
      "[Epoch 50 Batch 80/121] avg loss 0.0510214, throughput 2.18819K wps\n",
      "[Epoch 50 Batch 90/121] avg loss 0.0367607, throughput 2.33644K wps\n",
      "[Epoch 50 Batch 100/121] avg loss 0.043795, throughput 2.5189K wps\n",
      "[Epoch 50 Batch 110/121] avg loss 0.0324639, throughput 2.42131K wps\n",
      "[Epoch 50 Batch 120/121] avg loss 0.0279921, throughput 2.66667K wps\n",
      "[Epoch 50] train avg loss 0.0405489, train avg r2 0.951815,throughput 2.46135K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 51 Batch 10/121] avg loss 0.0366493, throughput 2.38096K wps\n",
      "[Epoch 51 Batch 20/121] avg loss 0.0464746, throughput 2.55102K wps\n",
      "[Epoch 51 Batch 30/121] avg loss 0.0410258, throughput 2.59066K wps\n",
      "[Epoch 51 Batch 40/121] avg loss 0.0241424, throughput 2.50628K wps\n",
      "[Epoch 51 Batch 50/121] avg loss 0.0419, throughput 2.55101K wps\n",
      "[Epoch 51 Batch 60/121] avg loss 0.043335, throughput 2.45098K wps\n",
      "[Epoch 51 Batch 70/121] avg loss 0.0485435, throughput 2.47525K wps\n",
      "[Epoch 51 Batch 80/121] avg loss 0.0568338, throughput 2.18818K wps\n",
      "[Epoch 51 Batch 90/121] avg loss 0.0399408, throughput 2.33644K wps\n",
      "[Epoch 51 Batch 100/121] avg loss 0.0391154, throughput 2.52525K wps\n",
      "[Epoch 51 Batch 110/121] avg loss 0.0329732, throughput 2.4213K wps\n",
      "[Epoch 51 Batch 120/121] avg loss 0.0302647, throughput 2.68097K wps\n",
      "[Epoch 51] train avg loss 0.0400824, train avg r2 0.95275,throughput 2.46939K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 52 Batch 10/121] avg loss 0.035552, throughput 2.37528K wps\n",
      "[Epoch 52 Batch 20/121] avg loss 0.0448746, throughput 2.54454K wps\n",
      "[Epoch 52 Batch 30/121] avg loss 0.0451191, throughput 2.59741K wps\n",
      "[Epoch 52 Batch 40/121] avg loss 0.0255248, throughput 2.49999K wps\n",
      "[Epoch 52 Batch 50/121] avg loss 0.0397002, throughput 2.54453K wps\n",
      "[Epoch 52 Batch 60/121] avg loss 0.0425782, throughput 2.445K wps\n",
      "[Epoch 52 Batch 70/121] avg loss 0.0498599, throughput 2.47524K wps\n",
      "[Epoch 52 Batch 80/121] avg loss 0.0520418, throughput 2.18818K wps\n",
      "[Epoch 52 Batch 90/121] avg loss 0.0379682, throughput 2.33102K wps\n",
      "[Epoch 52 Batch 100/121] avg loss 0.039759, throughput 2.51889K wps\n",
      "[Epoch 52 Batch 110/121] avg loss 0.0289422, throughput 2.41546K wps\n",
      "[Epoch 52 Batch 120/121] avg loss 0.0305584, throughput 2.6738K wps\n",
      "[Epoch 52] train avg loss 0.0393689, train avg r2 0.954022,throughput 2.46637K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 53 Batch 10/121] avg loss 0.0341385, throughput 2.38095K wps\n",
      "[Epoch 53 Batch 20/121] avg loss 0.039695, throughput 2.54453K wps\n",
      "[Epoch 53 Batch 30/121] avg loss 0.0421333, throughput 2.60417K wps\n",
      "[Epoch 53 Batch 40/121] avg loss 0.0257246, throughput 2.49376K wps\n",
      "[Epoch 53 Batch 50/121] avg loss 0.0351791, throughput 2.55102K wps\n",
      "[Epoch 53 Batch 60/121] avg loss 0.0446144, throughput 2.445K wps\n",
      "[Epoch 53 Batch 70/121] avg loss 0.0478216, throughput 2.48139K wps\n",
      "[Epoch 53 Batch 80/121] avg loss 0.0508695, throughput 2.19298K wps\n",
      "[Epoch 53 Batch 90/121] avg loss 0.0357822, throughput 2.331K wps\n",
      "[Epoch 53 Batch 100/121] avg loss 0.0395465, throughput 2.52524K wps\n",
      "[Epoch 53 Batch 110/121] avg loss 0.0278637, throughput 2.42131K wps\n",
      "[Epoch 53 Batch 120/121] avg loss 0.0299174, throughput 2.68096K wps\n",
      "[Epoch 53] train avg loss 0.037759, train avg r2 0.956016,throughput 2.46838K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 54 Batch 10/121] avg loss 0.0295689, throughput 2.36406K wps\n",
      "[Epoch 54 Batch 20/121] avg loss 0.0397922, throughput 2.55103K wps\n",
      "[Epoch 54 Batch 30/121] avg loss 0.042106, throughput 2.5974K wps\n",
      "[Epoch 54 Batch 40/121] avg loss 0.0237031, throughput 2.5K wps\n",
      "[Epoch 54 Batch 50/121] avg loss 0.0392213, throughput 2.54453K wps\n",
      "[Epoch 54 Batch 60/121] avg loss 0.0403233, throughput 2.43903K wps\n",
      "[Epoch 54 Batch 70/121] avg loss 0.0486251, throughput 2.48138K wps\n",
      "[Epoch 54 Batch 80/121] avg loss 0.0531449, throughput 2.19298K wps\n",
      "[Epoch 54 Batch 90/121] avg loss 0.0362572, throughput 2.331K wps\n",
      "[Epoch 54 Batch 100/121] avg loss 0.0405522, throughput 2.45701K wps\n",
      "[Epoch 54 Batch 110/121] avg loss 0.0298951, throughput 2.41545K wps\n",
      "[Epoch 54 Batch 120/121] avg loss 0.0292514, throughput 2.61096K wps\n",
      "[Epoch 54] train avg loss 0.0376911, train avg r2 0.956155,throughput 2.45486K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 55 Batch 10/121] avg loss 0.0318917, throughput 2.37529K wps\n",
      "[Epoch 55 Batch 20/121] avg loss 0.0358375, throughput 2.51888K wps\n",
      "[Epoch 55 Batch 30/121] avg loss 0.0405068, throughput 2.57731K wps\n",
      "[Epoch 55 Batch 40/121] avg loss 0.0228916, throughput 2.47525K wps\n",
      "[Epoch 55 Batch 50/121] avg loss 0.0376745, throughput 2.55102K wps\n",
      "[Epoch 55 Batch 60/121] avg loss 0.0383976, throughput 2.457K wps\n",
      "[Epoch 55 Batch 70/121] avg loss 0.046292, throughput 2.4814K wps\n",
      "[Epoch 55 Batch 80/121] avg loss 0.0528607, throughput 2.19298K wps\n",
      "[Epoch 55 Batch 90/121] avg loss 0.0362865, throughput 2.34192K wps\n",
      "[Epoch 55 Batch 100/121] avg loss 0.041222, throughput 2.53164K wps\n",
      "[Epoch 55 Batch 110/121] avg loss 0.0280806, throughput 2.41545K wps\n",
      "[Epoch 55 Batch 120/121] avg loss 0.0285644, throughput 2.68817K wps\n",
      "[Epoch 55] train avg loss 0.0366985, train avg r2 0.956962,throughput 2.46386K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 56 Batch 10/121] avg loss 0.0287171, throughput 2.36966K wps\n",
      "[Epoch 56 Batch 20/121] avg loss 0.0369041, throughput 2.55102K wps\n",
      "[Epoch 56 Batch 30/121] avg loss 0.0377594, throughput 2.59741K wps\n",
      "[Epoch 56 Batch 40/121] avg loss 0.0197591, throughput 2.49999K wps\n",
      "[Epoch 56 Batch 50/121] avg loss 0.0332062, throughput 2.53165K wps\n",
      "[Epoch 56 Batch 60/121] avg loss 0.0386514, throughput 2.45098K wps\n",
      "[Epoch 56 Batch 70/121] avg loss 0.0464054, throughput 2.48138K wps\n",
      "[Epoch 56 Batch 80/121] avg loss 0.0494916, throughput 2.18819K wps\n",
      "[Epoch 56 Batch 90/121] avg loss 0.0339863, throughput 2.33644K wps\n",
      "[Epoch 56 Batch 100/121] avg loss 0.0398767, throughput 2.51257K wps\n",
      "[Epoch 56 Batch 110/121] avg loss 0.026183, throughput 2.4213K wps\n",
      "[Epoch 56 Batch 120/121] avg loss 0.0276148, throughput 2.6738K wps\n",
      "[Epoch 56] train avg loss 0.0348706, train avg r2 0.958781,throughput 2.46737K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 57 Batch 10/121] avg loss 0.0287737, throughput 2.39234K wps\n",
      "[Epoch 57 Batch 20/121] avg loss 0.0333629, throughput 2.55754K wps\n",
      "[Epoch 57 Batch 30/121] avg loss 0.0356119, throughput 2.61097K wps\n",
      "[Epoch 57 Batch 40/121] avg loss 0.0211136, throughput 2.48139K wps\n",
      "[Epoch 57 Batch 50/121] avg loss 0.0323638, throughput 2.54453K wps\n",
      "[Epoch 57 Batch 60/121] avg loss 0.037455, throughput 2.45097K wps\n",
      "[Epoch 57 Batch 70/121] avg loss 0.0486054, throughput 2.48139K wps\n",
      "[Epoch 57 Batch 80/121] avg loss 0.0525033, throughput 2.19298K wps\n",
      "[Epoch 57 Batch 90/121] avg loss 0.0358561, throughput 2.331K wps\n",
      "[Epoch 57 Batch 100/121] avg loss 0.0396989, throughput 2.52526K wps\n",
      "[Epoch 57 Batch 110/121] avg loss 0.0257893, throughput 2.42718K wps\n",
      "[Epoch 57 Batch 120/121] avg loss 0.0265846, throughput 2.68096K wps\n",
      "[Epoch 57] train avg loss 0.0348038, train avg r2 0.959044,throughput 2.4709K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 58 Batch 10/121] avg loss 0.0309695, throughput 2.38663K wps\n",
      "[Epoch 58 Batch 20/121] avg loss 0.0357649, throughput 2.52526K wps\n",
      "[Epoch 58 Batch 30/121] avg loss 0.0354653, throughput 2.60416K wps\n",
      "[Epoch 58 Batch 40/121] avg loss 0.0230959, throughput 2.5K wps\n",
      "[Epoch 58 Batch 50/121] avg loss 0.0351759, throughput 2.54452K wps\n",
      "[Epoch 58 Batch 60/121] avg loss 0.0354484, throughput 2.445K wps\n",
      "[Epoch 58 Batch 70/121] avg loss 0.0432929, throughput 2.47524K wps\n",
      "[Epoch 58 Batch 80/121] avg loss 0.0515786, throughput 2.19299K wps\n",
      "[Epoch 58 Batch 90/121] avg loss 0.0329302, throughput 2.331K wps\n",
      "[Epoch 58 Batch 100/121] avg loss 0.0403295, throughput 2.51889K wps\n",
      "[Epoch 58 Batch 110/121] avg loss 0.027562, throughput 2.42131K wps\n",
      "[Epoch 58 Batch 120/121] avg loss 0.0275318, throughput 2.68097K wps\n",
      "[Epoch 58] train avg loss 0.034918, train avg r2 0.958468,throughput 2.46788K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 59 Batch 10/121] avg loss 0.0263177, throughput 2.38663K wps\n",
      "[Epoch 59 Batch 20/121] avg loss 0.034169, throughput 2.55754K wps\n",
      "[Epoch 59 Batch 30/121] avg loss 0.0333788, throughput 2.60416K wps\n",
      "[Epoch 59 Batch 40/121] avg loss 0.0235201, throughput 2.5K wps\n",
      "[Epoch 59 Batch 50/121] avg loss 0.0315847, throughput 2.52524K wps\n",
      "[Epoch 59 Batch 60/121] avg loss 0.0355151, throughput 2.45701K wps\n",
      "[Epoch 59 Batch 70/121] avg loss 0.0443216, throughput 2.48139K wps\n",
      "[Epoch 59 Batch 80/121] avg loss 0.0516278, throughput 2.19779K wps\n",
      "[Epoch 59 Batch 90/121] avg loss 0.0365225, throughput 2.32558K wps\n",
      "[Epoch 59 Batch 100/121] avg loss 0.0362544, throughput 2.52525K wps\n",
      "[Epoch 59 Batch 110/121] avg loss 0.0254586, throughput 2.41546K wps\n",
      "[Epoch 59 Batch 120/121] avg loss 0.0281435, throughput 2.68096K wps\n",
      "[Epoch 59] train avg loss 0.0338907, train avg r2 0.960481,throughput 2.46888K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 60 Batch 10/121] avg loss 0.0269605, throughput 2.38664K wps\n",
      "[Epoch 60 Batch 20/121] avg loss 0.0351913, throughput 2.53165K wps\n",
      "[Epoch 60 Batch 30/121] avg loss 0.0344092, throughput 2.5974K wps\n",
      "[Epoch 60 Batch 40/121] avg loss 0.0208182, throughput 2.49377K wps\n",
      "[Epoch 60 Batch 50/121] avg loss 0.0340788, throughput 2.54452K wps\n",
      "[Epoch 60 Batch 60/121] avg loss 0.036828, throughput 2.45098K wps\n",
      "[Epoch 60 Batch 70/121] avg loss 0.0463305, throughput 2.48138K wps\n",
      "[Epoch 60 Batch 80/121] avg loss 0.0465975, throughput 2.18819K wps\n",
      "[Epoch 60 Batch 90/121] avg loss 0.0357372, throughput 2.33644K wps\n",
      "[Epoch 60 Batch 100/121] avg loss 0.0442418, throughput 2.52525K wps\n",
      "[Epoch 60 Batch 110/121] avg loss 0.0280524, throughput 2.4213K wps\n",
      "[Epoch 60 Batch 120/121] avg loss 0.0286054, throughput 2.68096K wps\n",
      "[Epoch 60] train avg loss 0.0348162, train avg r2 0.958015,throughput 2.46788K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 61 Batch 10/121] avg loss 0.0266361, throughput 2.38664K wps\n",
      "[Epoch 61 Batch 20/121] avg loss 0.0369233, throughput 2.55754K wps\n",
      "[Epoch 61 Batch 30/121] avg loss 0.0333132, throughput 2.60417K wps\n",
      "[Epoch 61 Batch 40/121] avg loss 0.0221826, throughput 2.49377K wps\n",
      "[Epoch 61 Batch 50/121] avg loss 0.0340871, throughput 2.53165K wps\n",
      "[Epoch 61 Batch 60/121] avg loss 0.0392626, throughput 2.45699K wps\n",
      "[Epoch 61 Batch 70/121] avg loss 0.0468681, throughput 2.48138K wps\n",
      "[Epoch 61 Batch 80/121] avg loss 0.0488008, throughput 2.19297K wps\n",
      "[Epoch 61 Batch 90/121] avg loss 0.0350992, throughput 2.33101K wps\n",
      "[Epoch 61 Batch 100/121] avg loss 0.0424326, throughput 2.53164K wps\n",
      "[Epoch 61 Batch 110/121] avg loss 0.0280945, throughput 2.42718K wps\n",
      "[Epoch 61 Batch 120/121] avg loss 0.0289943, throughput 2.68096K wps\n",
      "[Epoch 61] train avg loss 0.0352194, train avg r2 0.957428,throughput 2.46989K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 62 Batch 10/121] avg loss 0.0266222, throughput 2.38095K wps\n",
      "[Epoch 62 Batch 20/121] avg loss 0.0333796, throughput 2.54453K wps\n",
      "[Epoch 62 Batch 30/121] avg loss 0.0329142, throughput 2.57069K wps\n",
      "[Epoch 62 Batch 40/121] avg loss 0.0188984, throughput 2.49377K wps\n",
      "[Epoch 62 Batch 50/121] avg loss 0.034279, throughput 2.54452K wps\n",
      "[Epoch 62 Batch 60/121] avg loss 0.0382887, throughput 2.44499K wps\n",
      "[Epoch 62 Batch 70/121] avg loss 0.0466399, throughput 2.47526K wps\n",
      "[Epoch 62 Batch 80/121] avg loss 0.0474053, throughput 2.18818K wps\n",
      "[Epoch 62 Batch 90/121] avg loss 0.0349013, throughput 2.331K wps\n",
      "[Epoch 62 Batch 100/121] avg loss 0.0400676, throughput 2.52524K wps\n",
      "[Epoch 62 Batch 110/121] avg loss 0.0290465, throughput 2.4213K wps\n",
      "[Epoch 62 Batch 120/121] avg loss 0.0301879, throughput 2.6738K wps\n",
      "[Epoch 62] train avg loss 0.0343766, train avg r2 0.958697,throughput 2.46486K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 63 Batch 10/121] avg loss 0.0284764, throughput 2.38095K wps\n",
      "[Epoch 63 Batch 20/121] avg loss 0.0345899, throughput 2.55103K wps\n",
      "[Epoch 63 Batch 30/121] avg loss 0.0312312, throughput 2.60415K wps\n",
      "[Epoch 63 Batch 40/121] avg loss 0.0198033, throughput 2.49378K wps\n",
      "[Epoch 63 Batch 50/121] avg loss 0.0360868, throughput 2.51889K wps\n",
      "[Epoch 63 Batch 60/121] avg loss 0.0381294, throughput 2.457K wps\n",
      "[Epoch 63 Batch 70/121] avg loss 0.0433373, throughput 2.48755K wps\n",
      "[Epoch 63 Batch 80/121] avg loss 0.0500833, throughput 2.1978K wps\n",
      "[Epoch 63 Batch 90/121] avg loss 0.0332561, throughput 2.34191K wps\n",
      "[Epoch 63 Batch 100/121] avg loss 0.0391146, throughput 2.52524K wps\n",
      "[Epoch 63 Batch 110/121] avg loss 0.0282026, throughput 2.42718K wps\n",
      "[Epoch 63 Batch 120/121] avg loss 0.0308515, throughput 2.68098K wps\n",
      "[Epoch 63] train avg loss 0.0344215, train avg r2 0.958451,throughput 2.46989K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 64 Batch 10/121] avg loss 0.0294338, throughput 2.39234K wps\n",
      "[Epoch 64 Batch 20/121] avg loss 0.0348857, throughput 2.55754K wps\n",
      "[Epoch 64 Batch 30/121] avg loss 0.0334064, throughput 2.59067K wps\n",
      "[Epoch 64 Batch 40/121] avg loss 0.0184843, throughput 2.50001K wps\n",
      "[Epoch 64 Batch 50/121] avg loss 0.0351493, throughput 2.54452K wps\n",
      "[Epoch 64 Batch 60/121] avg loss 0.0379294, throughput 2.45098K wps\n",
      "[Epoch 64 Batch 70/121] avg loss 0.0456819, throughput 2.48756K wps\n",
      "[Epoch 64 Batch 80/121] avg loss 0.0521928, throughput 2.19299K wps\n",
      "[Epoch 64 Batch 90/121] avg loss 0.0344757, throughput 2.331K wps\n",
      "[Epoch 64 Batch 100/121] avg loss 0.0349719, throughput 2.52525K wps\n",
      "[Epoch 64 Batch 110/121] avg loss 0.0262823, throughput 2.42131K wps\n",
      "[Epoch 64 Batch 120/121] avg loss 0.03016, throughput 2.68097K wps\n",
      "[Epoch 64] train avg loss 0.0344085, train avg r2 0.959414,throughput 2.47241K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 65 Batch 10/121] avg loss 0.026914, throughput 2.36967K wps\n",
      "[Epoch 65 Batch 20/121] avg loss 0.034057, throughput 2.55102K wps\n",
      "[Epoch 65 Batch 30/121] avg loss 0.0317798, throughput 2.5974K wps\n",
      "[Epoch 65 Batch 40/121] avg loss 0.0190102, throughput 2.49999K wps\n",
      "[Epoch 65 Batch 50/121] avg loss 0.0349184, throughput 2.54453K wps\n",
      "[Epoch 65 Batch 60/121] avg loss 0.0384141, throughput 2.45098K wps\n",
      "[Epoch 65 Batch 70/121] avg loss 0.0464498, throughput 2.47524K wps\n",
      "[Epoch 65 Batch 80/121] avg loss 0.0484563, throughput 2.19298K wps\n",
      "[Epoch 65 Batch 90/121] avg loss 0.0331366, throughput 2.33101K wps\n",
      "[Epoch 65 Batch 100/121] avg loss 0.0369998, throughput 2.52526K wps\n",
      "[Epoch 65 Batch 110/121] avg loss 0.025613, throughput 2.42718K wps\n",
      "[Epoch 65 Batch 120/121] avg loss 0.0293787, throughput 2.68097K wps\n",
      "[Epoch 65] train avg loss 0.0337517, train avg r2 0.959711,throughput 2.46838K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 66 Batch 10/121] avg loss 0.0296757, throughput 2.38663K wps\n",
      "[Epoch 66 Batch 20/121] avg loss 0.0347997, throughput 2.55102K wps\n",
      "[Epoch 66 Batch 30/121] avg loss 0.0305307, throughput 2.59068K wps\n",
      "[Epoch 66 Batch 40/121] avg loss 0.0167235, throughput 2.49376K wps\n",
      "[Epoch 66 Batch 50/121] avg loss 0.0312194, throughput 2.55754K wps\n",
      "[Epoch 66 Batch 60/121] avg loss 0.0386579, throughput 2.44498K wps\n",
      "[Epoch 66 Batch 70/121] avg loss 0.0438185, throughput 2.48139K wps\n",
      "[Epoch 66 Batch 80/121] avg loss 0.04803, throughput 2.19297K wps\n",
      "[Epoch 66 Batch 90/121] avg loss 0.0337098, throughput 2.33645K wps\n",
      "[Epoch 66 Batch 100/121] avg loss 0.0330239, throughput 2.52525K wps\n",
      "[Epoch 66 Batch 110/121] avg loss 0.026073, throughput 2.4213K wps\n",
      "[Epoch 66 Batch 120/121] avg loss 0.0294041, throughput 2.68097K wps\n",
      "[Epoch 66] train avg loss 0.0329585, train avg r2 0.96069,throughput 2.46989K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 67 Batch 10/121] avg loss 0.0289165, throughput 2.36967K wps\n",
      "[Epoch 67 Batch 20/121] avg loss 0.0338784, throughput 2.55754K wps\n",
      "[Epoch 67 Batch 30/121] avg loss 0.0281803, throughput 2.5974K wps\n",
      "[Epoch 67 Batch 40/121] avg loss 0.0176155, throughput 2.50001K wps\n",
      "[Epoch 67 Batch 50/121] avg loss 0.0297885, throughput 2.53807K wps\n",
      "[Epoch 67 Batch 60/121] avg loss 0.0369007, throughput 2.45096K wps\n",
      "[Epoch 67 Batch 70/121] avg loss 0.0441695, throughput 2.47526K wps\n",
      "[Epoch 67 Batch 80/121] avg loss 0.0465009, throughput 2.1978K wps\n",
      "[Epoch 67 Batch 90/121] avg loss 0.0326506, throughput 2.34193K wps\n",
      "[Epoch 67 Batch 100/121] avg loss 0.0346149, throughput 2.52525K wps\n",
      "[Epoch 67 Batch 110/121] avg loss 0.0274896, throughput 2.42718K wps\n",
      "[Epoch 67 Batch 120/121] avg loss 0.0292698, throughput 2.69541K wps\n",
      "[Epoch 67] train avg loss 0.0324871, train avg r2 0.961036,throughput 2.46939K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 68 Batch 10/121] avg loss 0.029523, throughput 2.38095K wps\n",
      "[Epoch 68 Batch 20/121] avg loss 0.0345225, throughput 2.55102K wps\n",
      "[Epoch 68 Batch 30/121] avg loss 0.0305246, throughput 2.59741K wps\n",
      "[Epoch 68 Batch 40/121] avg loss 0.0180536, throughput 2.49375K wps\n",
      "[Epoch 68 Batch 50/121] avg loss 0.032548, throughput 2.54453K wps\n",
      "[Epoch 68 Batch 60/121] avg loss 0.0376024, throughput 2.45098K wps\n",
      "[Epoch 68 Batch 70/121] avg loss 0.042265, throughput 2.48138K wps\n",
      "[Epoch 68 Batch 80/121] avg loss 0.0456161, throughput 2.18818K wps\n",
      "[Epoch 68 Batch 90/121] avg loss 0.033714, throughput 2.33645K wps\n",
      "[Epoch 68 Batch 100/121] avg loss 0.0336363, throughput 2.52525K wps\n",
      "[Epoch 68 Batch 110/121] avg loss 0.0266617, throughput 2.41546K wps\n",
      "[Epoch 68 Batch 120/121] avg loss 0.0266678, throughput 2.6738K wps\n",
      "[Epoch 68] train avg loss 0.0326022, train avg r2 0.960839,throughput 2.46737K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 69 Batch 10/121] avg loss 0.0279138, throughput 2.38095K wps\n",
      "[Epoch 69 Batch 20/121] avg loss 0.0323557, throughput 2.54452K wps\n",
      "[Epoch 69 Batch 30/121] avg loss 0.0304165, throughput 2.59741K wps\n",
      "[Epoch 69 Batch 40/121] avg loss 0.0182292, throughput 2.50626K wps\n",
      "[Epoch 69 Batch 50/121] avg loss 0.0311739, throughput 2.53807K wps\n",
      "[Epoch 69 Batch 60/121] avg loss 0.0354099, throughput 2.43903K wps\n",
      "[Epoch 69 Batch 70/121] avg loss 0.0452733, throughput 2.48139K wps\n",
      "[Epoch 69 Batch 80/121] avg loss 0.0487345, throughput 2.18818K wps\n",
      "[Epoch 69 Batch 90/121] avg loss 0.030424, throughput 2.32558K wps\n",
      "[Epoch 69 Batch 100/121] avg loss 0.0332622, throughput 2.51257K wps\n",
      "[Epoch 69 Batch 110/121] avg loss 0.0253529, throughput 2.4213K wps\n",
      "[Epoch 69 Batch 120/121] avg loss 0.0298296, throughput 2.68097K wps\n",
      "[Epoch 69] train avg loss 0.032351, train avg r2 0.961544,throughput 2.46637K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 70 Batch 10/121] avg loss 0.0298089, throughput 2.38095K wps\n",
      "[Epoch 70 Batch 20/121] avg loss 0.0328728, throughput 2.54453K wps\n",
      "[Epoch 70 Batch 30/121] avg loss 0.0294508, throughput 2.60417K wps\n",
      "[Epoch 70 Batch 40/121] avg loss 0.0148305, throughput 2.48755K wps\n",
      "[Epoch 70 Batch 50/121] avg loss 0.0318027, throughput 2.55101K wps\n",
      "[Epoch 70 Batch 60/121] avg loss 0.0368095, throughput 2.44498K wps\n",
      "[Epoch 70 Batch 70/121] avg loss 0.0428572, throughput 2.48139K wps\n",
      "[Epoch 70 Batch 80/121] avg loss 0.0462731, throughput 2.1978K wps\n",
      "[Epoch 70 Batch 90/121] avg loss 0.032248, throughput 2.33101K wps\n",
      "[Epoch 70 Batch 100/121] avg loss 0.0326043, throughput 2.51256K wps\n",
      "[Epoch 70 Batch 110/121] avg loss 0.0235497, throughput 2.42131K wps\n",
      "[Epoch 70 Batch 120/121] avg loss 0.0252963, throughput 2.68096K wps\n",
      "[Epoch 70] train avg loss 0.0315187, train avg r2 0.962781,throughput 2.46788K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 71 Batch 10/121] avg loss 0.0277231, throughput 2.38096K wps\n",
      "[Epoch 71 Batch 20/121] avg loss 0.0363124, throughput 2.54452K wps\n",
      "[Epoch 71 Batch 30/121] avg loss 0.0288426, throughput 2.61096K wps\n",
      "[Epoch 71 Batch 40/121] avg loss 0.017057, throughput 2.5K wps\n",
      "[Epoch 71 Batch 50/121] avg loss 0.0343241, throughput 2.54453K wps\n",
      "[Epoch 71 Batch 60/121] avg loss 0.0337196, throughput 2.45098K wps\n",
      "[Epoch 71 Batch 70/121] avg loss 0.042193, throughput 2.48139K wps\n",
      "[Epoch 71 Batch 80/121] avg loss 0.0457822, throughput 2.19298K wps\n",
      "[Epoch 71 Batch 90/121] avg loss 0.0308735, throughput 2.331K wps\n",
      "[Epoch 71 Batch 100/121] avg loss 0.033555, throughput 2.51257K wps\n",
      "[Epoch 71 Batch 110/121] avg loss 0.0263032, throughput 2.42718K wps\n",
      "[Epoch 71 Batch 120/121] avg loss 0.024652, throughput 2.68098K wps\n",
      "[Epoch 71] train avg loss 0.0317721, train avg r2 0.961499,throughput 2.46989K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 72 Batch 10/121] avg loss 0.0287065, throughput 2.38095K wps\n",
      "[Epoch 72 Batch 20/121] avg loss 0.0330827, throughput 2.55102K wps\n",
      "[Epoch 72 Batch 30/121] avg loss 0.0282614, throughput 2.60415K wps\n",
      "[Epoch 72 Batch 40/121] avg loss 0.016765, throughput 2.5K wps\n",
      "[Epoch 72 Batch 50/121] avg loss 0.0333099, throughput 2.51889K wps\n",
      "[Epoch 72 Batch 60/121] avg loss 0.0317072, throughput 2.45099K wps\n",
      "[Epoch 72 Batch 70/121] avg loss 0.0410798, throughput 2.48138K wps\n",
      "[Epoch 72 Batch 80/121] avg loss 0.0459243, throughput 2.19298K wps\n",
      "[Epoch 72 Batch 90/121] avg loss 0.0307, throughput 2.33645K wps\n",
      "[Epoch 72 Batch 100/121] avg loss 0.0329281, throughput 2.51889K wps\n",
      "[Epoch 72 Batch 110/121] avg loss 0.0261046, throughput 2.41546K wps\n",
      "[Epoch 72 Batch 120/121] avg loss 0.0243052, throughput 2.68097K wps\n",
      "[Epoch 72] train avg loss 0.0310693, train avg r2 0.962706,throughput 2.46838K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 73 Batch 10/121] avg loss 0.0283495, throughput 2.38663K wps\n",
      "[Epoch 73 Batch 20/121] avg loss 0.0329137, throughput 2.54453K wps\n",
      "[Epoch 73 Batch 30/121] avg loss 0.0305662, throughput 2.60417K wps\n",
      "[Epoch 73 Batch 40/121] avg loss 0.0163423, throughput 2.49999K wps\n",
      "[Epoch 73 Batch 50/121] avg loss 0.0301627, throughput 2.54451K wps\n",
      "[Epoch 73 Batch 60/121] avg loss 0.0349892, throughput 2.45099K wps\n",
      "[Epoch 73 Batch 70/121] avg loss 0.0444245, throughput 2.47525K wps\n",
      "[Epoch 73 Batch 80/121] avg loss 0.044455, throughput 2.19298K wps\n",
      "[Epoch 73 Batch 90/121] avg loss 0.0306075, throughput 2.33644K wps\n",
      "[Epoch 73 Batch 100/121] avg loss 0.0331115, throughput 2.52525K wps\n",
      "[Epoch 73 Batch 110/121] avg loss 0.0250622, throughput 2.42131K wps\n",
      "[Epoch 73 Batch 120/121] avg loss 0.0245446, throughput 2.68096K wps\n",
      "[Epoch 73] train avg loss 0.0312895, train avg r2 0.963045,throughput 2.46888K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 74 Batch 10/121] avg loss 0.0249605, throughput 2.38663K wps\n",
      "[Epoch 74 Batch 20/121] avg loss 0.0316067, throughput 2.55754K wps\n",
      "[Epoch 74 Batch 30/121] avg loss 0.0285692, throughput 2.60416K wps\n",
      "[Epoch 74 Batch 40/121] avg loss 0.0158977, throughput 2.50001K wps\n",
      "[Epoch 74 Batch 50/121] avg loss 0.0305786, throughput 2.51257K wps\n",
      "[Epoch 74 Batch 60/121] avg loss 0.034475, throughput 2.43902K wps\n",
      "[Epoch 74 Batch 70/121] avg loss 0.0419942, throughput 2.48137K wps\n",
      "[Epoch 74 Batch 80/121] avg loss 0.0433878, throughput 2.19299K wps\n",
      "[Epoch 74 Batch 90/121] avg loss 0.0280739, throughput 2.34192K wps\n",
      "[Epoch 74 Batch 100/121] avg loss 0.0334693, throughput 2.52523K wps\n",
      "[Epoch 74 Batch 110/121] avg loss 0.0255101, throughput 2.42719K wps\n",
      "[Epoch 74 Batch 120/121] avg loss 0.0234625, throughput 2.69541K wps\n",
      "[Epoch 74] train avg loss 0.0301517, train avg r2 0.964253,throughput 2.46939K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 75 Batch 10/121] avg loss 0.0264514, throughput 2.39234K wps\n",
      "[Epoch 75 Batch 20/121] avg loss 0.0309624, throughput 2.56409K wps\n",
      "[Epoch 75 Batch 30/121] avg loss 0.0270879, throughput 2.59066K wps\n",
      "[Epoch 75 Batch 40/121] avg loss 0.0169525, throughput 2.50627K wps\n",
      "[Epoch 75 Batch 50/121] avg loss 0.0305968, throughput 2.54453K wps\n",
      "[Epoch 75 Batch 60/121] avg loss 0.0340102, throughput 2.45099K wps\n",
      "[Epoch 75 Batch 70/121] avg loss 0.0405347, throughput 2.48139K wps\n",
      "[Epoch 75 Batch 80/121] avg loss 0.0450197, throughput 2.18818K wps\n",
      "[Epoch 75 Batch 90/121] avg loss 0.0314373, throughput 2.331K wps\n",
      "[Epoch 75 Batch 100/121] avg loss 0.0316419, throughput 2.51889K wps\n",
      "[Epoch 75 Batch 110/121] avg loss 0.0235738, throughput 2.42131K wps\n",
      "[Epoch 75 Batch 120/121] avg loss 0.023229, throughput 2.66667K wps\n",
      "[Epoch 75] train avg loss 0.0301153, train avg r2 0.964495,throughput 2.46939K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 76 Batch 10/121] avg loss 0.0257614, throughput 2.38095K wps\n",
      "[Epoch 76 Batch 20/121] avg loss 0.0306831, throughput 2.55101K wps\n",
      "[Epoch 76 Batch 30/121] avg loss 0.0266659, throughput 2.60416K wps\n",
      "[Epoch 76 Batch 40/121] avg loss 0.01726, throughput 2.49377K wps\n",
      "[Epoch 76 Batch 50/121] avg loss 0.028617, throughput 2.52524K wps\n",
      "[Epoch 76 Batch 60/121] avg loss 0.0329443, throughput 2.44499K wps\n",
      "[Epoch 76 Batch 70/121] avg loss 0.0396162, throughput 2.48755K wps\n",
      "[Epoch 76 Batch 80/121] avg loss 0.0430017, throughput 2.18819K wps\n",
      "[Epoch 76 Batch 90/121] avg loss 0.0317276, throughput 2.33644K wps\n",
      "[Epoch 76 Batch 100/121] avg loss 0.0302062, throughput 2.51889K wps\n",
      "[Epoch 76 Batch 110/121] avg loss 0.0251024, throughput 2.42131K wps\n",
      "[Epoch 76 Batch 120/121] avg loss 0.021841, throughput 2.68097K wps\n",
      "[Epoch 76] train avg loss 0.0294432, train avg r2 0.965575,throughput 2.46788K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 77 Batch 10/121] avg loss 0.0251575, throughput 2.38663K wps\n",
      "[Epoch 77 Batch 20/121] avg loss 0.0308437, throughput 2.54452K wps\n",
      "[Epoch 77 Batch 30/121] avg loss 0.0276449, throughput 2.59742K wps\n",
      "[Epoch 77 Batch 40/121] avg loss 0.0155999, throughput 2.5K wps\n",
      "[Epoch 77 Batch 50/121] avg loss 0.0286463, throughput 2.53806K wps\n",
      "[Epoch 77 Batch 60/121] avg loss 0.0334898, throughput 2.44499K wps\n",
      "[Epoch 77 Batch 70/121] avg loss 0.0398873, throughput 2.48139K wps\n",
      "[Epoch 77 Batch 80/121] avg loss 0.0431685, throughput 2.19298K wps\n",
      "[Epoch 77 Batch 90/121] avg loss 0.0314596, throughput 2.331K wps\n",
      "[Epoch 77 Batch 100/121] avg loss 0.0314974, throughput 2.52525K wps\n",
      "[Epoch 77 Batch 110/121] avg loss 0.0242567, throughput 2.4213K wps\n",
      "[Epoch 77 Batch 120/121] avg loss 0.022346, throughput 2.6738K wps\n",
      "[Epoch 77] train avg loss 0.0294905, train avg r2 0.965286,throughput 2.46737K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 78 Batch 10/121] avg loss 0.0229362, throughput 2.37529K wps\n",
      "[Epoch 78 Batch 20/121] avg loss 0.0293228, throughput 2.54453K wps\n",
      "[Epoch 78 Batch 30/121] avg loss 0.0269213, throughput 2.60416K wps\n",
      "[Epoch 78 Batch 40/121] avg loss 0.017129, throughput 2.5K wps\n",
      "[Epoch 78 Batch 50/121] avg loss 0.0294997, throughput 2.53808K wps\n",
      "[Epoch 78 Batch 60/121] avg loss 0.0323357, throughput 2.45097K wps\n",
      "[Epoch 78 Batch 70/121] avg loss 0.0399794, throughput 2.46914K wps\n",
      "[Epoch 78 Batch 80/121] avg loss 0.0458893, throughput 2.19298K wps\n",
      "[Epoch 78 Batch 90/121] avg loss 0.0293784, throughput 2.331K wps\n",
      "[Epoch 78 Batch 100/121] avg loss 0.0304631, throughput 2.52525K wps\n",
      "[Epoch 78 Batch 110/121] avg loss 0.0230353, throughput 2.42719K wps\n",
      "[Epoch 78 Batch 120/121] avg loss 0.0222502, throughput 2.66666K wps\n",
      "[Epoch 78] train avg loss 0.0290839, train avg r2 0.966252,throughput 2.46486K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 79 Batch 10/121] avg loss 0.0240051, throughput 2.36407K wps\n",
      "[Epoch 79 Batch 20/121] avg loss 0.0305173, throughput 2.54453K wps\n",
      "[Epoch 79 Batch 30/121] avg loss 0.027402, throughput 2.50626K wps\n",
      "[Epoch 79 Batch 40/121] avg loss 0.0161825, throughput 2.48139K wps\n",
      "[Epoch 79 Batch 50/121] avg loss 0.0290226, throughput 2.46305K wps\n",
      "[Epoch 79 Batch 60/121] avg loss 0.0311556, throughput 2.43903K wps\n",
      "[Epoch 79 Batch 70/121] avg loss 0.0381298, throughput 2.48139K wps\n",
      "[Epoch 79 Batch 80/121] avg loss 0.0436774, throughput 2.18818K wps\n",
      "[Epoch 79 Batch 90/121] avg loss 0.0301275, throughput 2.32558K wps\n",
      "[Epoch 79 Batch 100/121] avg loss 0.0300735, throughput 2.51889K wps\n",
      "[Epoch 79 Batch 110/121] avg loss 0.0252543, throughput 2.4213K wps\n",
      "[Epoch 79 Batch 120/121] avg loss 0.0223371, throughput 2.68817K wps\n",
      "[Epoch 79] train avg loss 0.0289799, train avg r2 0.965984,throughput 2.45088K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 80 Batch 10/121] avg loss 0.0227079, throughput 2.36967K wps\n",
      "[Epoch 80 Batch 20/121] avg loss 0.0295112, throughput 2.54453K wps\n",
      "[Epoch 80 Batch 30/121] avg loss 0.027053, throughput 2.59739K wps\n",
      "[Epoch 80 Batch 40/121] avg loss 0.0148746, throughput 2.49376K wps\n",
      "[Epoch 80 Batch 50/121] avg loss 0.0291213, throughput 2.54452K wps\n",
      "[Epoch 80 Batch 60/121] avg loss 0.029446, throughput 2.44499K wps\n",
      "[Epoch 80 Batch 70/121] avg loss 0.0398824, throughput 2.48138K wps\n",
      "[Epoch 80 Batch 80/121] avg loss 0.0471527, throughput 2.18819K wps\n",
      "[Epoch 80 Batch 90/121] avg loss 0.0306271, throughput 2.331K wps\n",
      "[Epoch 80 Batch 100/121] avg loss 0.0294416, throughput 2.52524K wps\n",
      "[Epoch 80 Batch 110/121] avg loss 0.0239218, throughput 2.41547K wps\n",
      "[Epoch 80 Batch 120/121] avg loss 0.0206497, throughput 2.66666K wps\n",
      "[Epoch 80] train avg loss 0.0286915, train avg r2 0.966705,throughput 2.46436K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 81 Batch 10/121] avg loss 0.0217331, throughput 2.38663K wps\n",
      "[Epoch 81 Batch 20/121] avg loss 0.0276607, throughput 2.55102K wps\n",
      "[Epoch 81 Batch 30/121] avg loss 0.025223, throughput 2.60417K wps\n",
      "[Epoch 81 Batch 40/121] avg loss 0.0169037, throughput 2.49376K wps\n",
      "[Epoch 81 Batch 50/121] avg loss 0.0294078, throughput 2.54454K wps\n",
      "[Epoch 81 Batch 60/121] avg loss 0.032935, throughput 2.45097K wps\n",
      "[Epoch 81 Batch 70/121] avg loss 0.037998, throughput 2.48139K wps\n",
      "[Epoch 81 Batch 80/121] avg loss 0.0433544, throughput 2.1978K wps\n",
      "[Epoch 81 Batch 90/121] avg loss 0.0307815, throughput 2.34192K wps\n",
      "[Epoch 81 Batch 100/121] avg loss 0.0314348, throughput 2.52525K wps\n",
      "[Epoch 81 Batch 110/121] avg loss 0.023442, throughput 2.42719K wps\n",
      "[Epoch 81 Batch 120/121] avg loss 0.0211613, throughput 2.68816K wps\n",
      "[Epoch 81] train avg loss 0.0284919, train avg r2 0.966468,throughput 2.47242K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 82 Batch 10/121] avg loss 0.0215398, throughput 2.38095K wps\n",
      "[Epoch 82 Batch 20/121] avg loss 0.0305404, throughput 2.55754K wps\n",
      "[Epoch 82 Batch 30/121] avg loss 0.0256803, throughput 2.60416K wps\n",
      "[Epoch 82 Batch 40/121] avg loss 0.0149433, throughput 2.50626K wps\n",
      "[Epoch 82 Batch 50/121] avg loss 0.0274098, throughput 2.54452K wps\n",
      "[Epoch 82 Batch 60/121] avg loss 0.0348593, throughput 2.45098K wps\n",
      "[Epoch 82 Batch 70/121] avg loss 0.0389492, throughput 2.4814K wps\n",
      "[Epoch 82 Batch 80/121] avg loss 0.0424417, throughput 2.18819K wps\n",
      "[Epoch 82 Batch 90/121] avg loss 0.0293602, throughput 2.331K wps\n",
      "[Epoch 82 Batch 100/121] avg loss 0.0288682, throughput 2.5189K wps\n",
      "[Epoch 82 Batch 110/121] avg loss 0.0252726, throughput 2.4213K wps\n",
      "[Epoch 82 Batch 120/121] avg loss 0.0211763, throughput 2.68818K wps\n",
      "[Epoch 82] train avg loss 0.0284101, train avg r2 0.966672,throughput 2.46989K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 83 Batch 10/121] avg loss 0.0249902, throughput 2.39234K wps\n",
      "[Epoch 83 Batch 20/121] avg loss 0.0288924, throughput 2.55102K wps\n",
      "[Epoch 83 Batch 30/121] avg loss 0.0241001, throughput 2.61097K wps\n",
      "[Epoch 83 Batch 40/121] avg loss 0.0158436, throughput 2.49376K wps\n",
      "[Epoch 83 Batch 50/121] avg loss 0.0298981, throughput 2.55101K wps\n",
      "[Epoch 83 Batch 60/121] avg loss 0.0327658, throughput 2.45098K wps\n",
      "[Epoch 83 Batch 70/121] avg loss 0.0385607, throughput 2.48139K wps\n",
      "[Epoch 83 Batch 80/121] avg loss 0.0448747, throughput 2.19298K wps\n",
      "[Epoch 83 Batch 90/121] avg loss 0.0319337, throughput 2.33645K wps\n",
      "[Epoch 83 Batch 100/121] avg loss 0.0296406, throughput 2.53164K wps\n",
      "[Epoch 83 Batch 110/121] avg loss 0.0224231, throughput 2.42718K wps\n",
      "[Epoch 83 Batch 120/121] avg loss 0.0217492, throughput 2.68816K wps\n",
      "[Epoch 83] train avg loss 0.0287978, train avg r2 0.9665,throughput 2.47191K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 84 Batch 10/121] avg loss 0.0222466, throughput 2.39234K wps\n",
      "[Epoch 84 Batch 20/121] avg loss 0.0284563, throughput 2.54453K wps\n",
      "[Epoch 84 Batch 30/121] avg loss 0.0249718, throughput 2.60417K wps\n",
      "[Epoch 84 Batch 40/121] avg loss 0.0166702, throughput 2.50626K wps\n",
      "[Epoch 84 Batch 50/121] avg loss 0.0277985, throughput 2.53806K wps\n",
      "[Epoch 84 Batch 60/121] avg loss 0.0313588, throughput 2.44499K wps\n",
      "[Epoch 84 Batch 70/121] avg loss 0.0362408, throughput 2.48138K wps\n",
      "[Epoch 84 Batch 80/121] avg loss 0.0440147, throughput 2.18819K wps\n",
      "[Epoch 84 Batch 90/121] avg loss 0.0303353, throughput 2.32558K wps\n",
      "[Epoch 84 Batch 100/121] avg loss 0.0287058, throughput 2.52525K wps\n",
      "[Epoch 84 Batch 110/121] avg loss 0.0230039, throughput 2.4213K wps\n",
      "[Epoch 84 Batch 120/121] avg loss 0.0219615, throughput 2.68097K wps\n",
      "[Epoch 84] train avg loss 0.027968, train avg r2 0.967351,throughput 2.46888K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 85 Batch 10/121] avg loss 0.0244387, throughput 2.38663K wps\n",
      "[Epoch 85 Batch 20/121] avg loss 0.0298495, throughput 2.55754K wps\n",
      "[Epoch 85 Batch 30/121] avg loss 0.0259214, throughput 2.60416K wps\n",
      "[Epoch 85 Batch 40/121] avg loss 0.0163525, throughput 2.49377K wps\n",
      "[Epoch 85 Batch 50/121] avg loss 0.0266108, throughput 2.52524K wps\n",
      "[Epoch 85 Batch 60/121] avg loss 0.0305592, throughput 2.44499K wps\n",
      "[Epoch 85 Batch 70/121] avg loss 0.0382297, throughput 2.48756K wps\n",
      "[Epoch 85 Batch 80/121] avg loss 0.0427283, throughput 2.18818K wps\n",
      "[Epoch 85 Batch 90/121] avg loss 0.0289377, throughput 2.33101K wps\n",
      "[Epoch 85 Batch 100/121] avg loss 0.0285787, throughput 2.52525K wps\n",
      "[Epoch 85 Batch 110/121] avg loss 0.0233581, throughput 2.42717K wps\n",
      "[Epoch 85 Batch 120/121] avg loss 0.0221439, throughput 2.68818K wps\n",
      "[Epoch 85] train avg loss 0.0281338, train avg r2 0.966986,throughput 2.46888K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 86 Batch 10/121] avg loss 0.022689, throughput 2.38095K wps\n",
      "[Epoch 86 Batch 20/121] avg loss 0.0293577, throughput 2.54452K wps\n",
      "[Epoch 86 Batch 30/121] avg loss 0.0240643, throughput 2.60417K wps\n",
      "[Epoch 86 Batch 40/121] avg loss 0.0159125, throughput 2.49376K wps\n",
      "[Epoch 86 Batch 50/121] avg loss 0.0283128, throughput 2.54453K wps\n",
      "[Epoch 86 Batch 60/121] avg loss 0.03244, throughput 2.43903K wps\n",
      "[Epoch 86 Batch 70/121] avg loss 0.0405004, throughput 2.47524K wps\n",
      "[Epoch 86 Batch 80/121] avg loss 0.0427404, throughput 2.19299K wps\n",
      "[Epoch 86 Batch 90/121] avg loss 0.027383, throughput 2.33644K wps\n",
      "[Epoch 86 Batch 100/121] avg loss 0.0299595, throughput 2.52525K wps\n",
      "[Epoch 86 Batch 110/121] avg loss 0.0221248, throughput 2.42131K wps\n",
      "[Epoch 86 Batch 120/121] avg loss 0.0220126, throughput 2.68096K wps\n",
      "[Epoch 86] train avg loss 0.0281131, train avg r2 0.967102,throughput 2.46737K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 87 Batch 10/121] avg loss 0.0223619, throughput 2.38095K wps\n",
      "[Epoch 87 Batch 20/121] avg loss 0.0300379, throughput 2.55754K wps\n",
      "[Epoch 87 Batch 30/121] avg loss 0.0261433, throughput 2.60417K wps\n",
      "[Epoch 87 Batch 40/121] avg loss 0.0163597, throughput 2.5K wps\n",
      "[Epoch 87 Batch 50/121] avg loss 0.0277306, throughput 2.52525K wps\n",
      "[Epoch 87 Batch 60/121] avg loss 0.0310275, throughput 2.45098K wps\n",
      "[Epoch 87 Batch 70/121] avg loss 0.0406372, throughput 2.48139K wps\n",
      "[Epoch 87 Batch 80/121] avg loss 0.0434226, throughput 2.19298K wps\n",
      "[Epoch 87 Batch 90/121] avg loss 0.0272529, throughput 2.331K wps\n",
      "[Epoch 87 Batch 100/121] avg loss 0.0299108, throughput 2.51889K wps\n",
      "[Epoch 87 Batch 110/121] avg loss 0.0222158, throughput 2.42131K wps\n",
      "[Epoch 87 Batch 120/121] avg loss 0.021187, throughput 2.68097K wps\n",
      "[Epoch 87] train avg loss 0.02818, train avg r2 0.967027,throughput 2.46888K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 88 Batch 10/121] avg loss 0.0231741, throughput 2.38664K wps\n",
      "[Epoch 88 Batch 20/121] avg loss 0.0292535, throughput 2.55102K wps\n",
      "[Epoch 88 Batch 30/121] avg loss 0.0259628, throughput 2.58398K wps\n",
      "[Epoch 88 Batch 40/121] avg loss 0.0167499, throughput 2.49376K wps\n",
      "[Epoch 88 Batch 50/121] avg loss 0.0288936, throughput 2.53807K wps\n",
      "[Epoch 88 Batch 60/121] avg loss 0.0307039, throughput 2.45098K wps\n",
      "[Epoch 88 Batch 70/121] avg loss 0.0401135, throughput 2.47525K wps\n",
      "[Epoch 88 Batch 80/121] avg loss 0.0465484, throughput 2.18818K wps\n",
      "[Epoch 88 Batch 90/121] avg loss 0.0303218, throughput 2.32558K wps\n",
      "[Epoch 88 Batch 100/121] avg loss 0.0292179, throughput 2.51257K wps\n",
      "[Epoch 88 Batch 110/121] avg loss 0.0221807, throughput 2.42131K wps\n",
      "[Epoch 88 Batch 120/121] avg loss 0.0221953, throughput 2.6738K wps\n",
      "[Epoch 88] train avg loss 0.0287668, train avg r2 0.96704,throughput 2.46536K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 89 Batch 10/121] avg loss 0.0215886, throughput 2.38664K wps\n",
      "[Epoch 89 Batch 20/121] avg loss 0.029165, throughput 2.55102K wps\n",
      "[Epoch 89 Batch 30/121] avg loss 0.0239681, throughput 2.5974K wps\n",
      "[Epoch 89 Batch 40/121] avg loss 0.0161061, throughput 2.49376K wps\n",
      "[Epoch 89 Batch 50/121] avg loss 0.0285517, throughput 2.53164K wps\n",
      "[Epoch 89 Batch 60/121] avg loss 0.0307797, throughput 2.43903K wps\n",
      "[Epoch 89 Batch 70/121] avg loss 0.0370579, throughput 2.48139K wps\n",
      "[Epoch 89 Batch 80/121] avg loss 0.0434128, throughput 2.19298K wps\n",
      "[Epoch 89 Batch 90/121] avg loss 0.031053, throughput 2.32558K wps\n",
      "[Epoch 89 Batch 100/121] avg loss 0.0282832, throughput 2.52526K wps\n",
      "[Epoch 89 Batch 110/121] avg loss 0.022659, throughput 2.41546K wps\n",
      "[Epoch 89 Batch 120/121] avg loss 0.0211741, throughput 2.68097K wps\n",
      "[Epoch 89] train avg loss 0.0278072, train avg r2 0.967509,throughput 2.46687K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 90 Batch 10/121] avg loss 0.0212602, throughput 2.38096K wps\n",
      "[Epoch 90 Batch 20/121] avg loss 0.0288238, throughput 2.55754K wps\n",
      "[Epoch 90 Batch 30/121] avg loss 0.0257196, throughput 2.59067K wps\n",
      "[Epoch 90 Batch 40/121] avg loss 0.0148708, throughput 2.49376K wps\n",
      "[Epoch 90 Batch 50/121] avg loss 0.0292573, throughput 2.53165K wps\n",
      "[Epoch 90 Batch 60/121] avg loss 0.0314994, throughput 2.40963K wps\n",
      "[Epoch 90 Batch 70/121] avg loss 0.0376408, throughput 2.48139K wps\n",
      "[Epoch 90 Batch 80/121] avg loss 0.0441785, throughput 2.19298K wps\n",
      "[Epoch 90 Batch 90/121] avg loss 0.0301622, throughput 2.33645K wps\n",
      "[Epoch 90 Batch 100/121] avg loss 0.029721, throughput 2.52525K wps\n",
      "[Epoch 90 Batch 110/121] avg loss 0.0224297, throughput 2.42131K wps\n",
      "[Epoch 90 Batch 120/121] avg loss 0.0211434, throughput 2.68096K wps\n",
      "[Epoch 90] train avg loss 0.0280567, train avg r2 0.967051,throughput 2.46436K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 91 Batch 10/121] avg loss 0.0218577, throughput 2.36967K wps\n",
      "[Epoch 91 Batch 20/121] avg loss 0.0293478, throughput 2.55754K wps\n",
      "[Epoch 91 Batch 30/121] avg loss 0.0253188, throughput 2.5974K wps\n",
      "[Epoch 91 Batch 40/121] avg loss 0.0148564, throughput 2.49377K wps\n",
      "[Epoch 91 Batch 50/121] avg loss 0.0268276, throughput 2.54453K wps\n",
      "[Epoch 91 Batch 60/121] avg loss 0.0323329, throughput 2.44499K wps\n",
      "[Epoch 91 Batch 70/121] avg loss 0.0398933, throughput 2.48138K wps\n",
      "[Epoch 91 Batch 80/121] avg loss 0.0449573, throughput 2.18819K wps\n",
      "[Epoch 91 Batch 90/121] avg loss 0.0300014, throughput 2.33099K wps\n",
      "[Epoch 91 Batch 100/121] avg loss 0.029572, throughput 2.52525K wps\n",
      "[Epoch 91 Batch 110/121] avg loss 0.0240633, throughput 2.42131K wps\n",
      "[Epoch 91 Batch 120/121] avg loss 0.021709, throughput 2.68096K wps\n",
      "[Epoch 91] train avg loss 0.0283898, train avg r2 0.966737,throughput 2.46788K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 92 Batch 10/121] avg loss 0.0211266, throughput 2.38663K wps\n",
      "[Epoch 92 Batch 20/121] avg loss 0.0268467, throughput 2.55754K wps\n",
      "[Epoch 92 Batch 30/121] avg loss 0.0269758, throughput 2.58399K wps\n",
      "[Epoch 92 Batch 40/121] avg loss 0.0145038, throughput 2.48138K wps\n",
      "[Epoch 92 Batch 50/121] avg loss 0.0270453, throughput 2.54453K wps\n",
      "[Epoch 92 Batch 60/121] avg loss 0.0329152, throughput 2.44499K wps\n",
      "[Epoch 92 Batch 70/121] avg loss 0.0369389, throughput 2.48138K wps\n",
      "[Epoch 92 Batch 80/121] avg loss 0.0408413, throughput 2.18818K wps\n",
      "[Epoch 92 Batch 90/121] avg loss 0.0294998, throughput 2.331K wps\n",
      "[Epoch 92 Batch 100/121] avg loss 0.0286814, throughput 2.51889K wps\n",
      "[Epoch 92 Batch 110/121] avg loss 0.0224206, throughput 2.4213K wps\n",
      "[Epoch 92 Batch 120/121] avg loss 0.0213769, throughput 2.68097K wps\n",
      "[Epoch 92] train avg loss 0.0274203, train avg r2 0.967989,throughput 2.46687K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 93 Batch 10/121] avg loss 0.0236086, throughput 2.36966K wps\n",
      "[Epoch 93 Batch 20/121] avg loss 0.0302601, throughput 2.55101K wps\n",
      "[Epoch 93 Batch 30/121] avg loss 0.0263886, throughput 2.60417K wps\n",
      "[Epoch 93 Batch 40/121] avg loss 0.0154225, throughput 2.50626K wps\n",
      "[Epoch 93 Batch 50/121] avg loss 0.0274752, throughput 2.53806K wps\n",
      "[Epoch 93 Batch 60/121] avg loss 0.0301902, throughput 2.46305K wps\n",
      "[Epoch 93 Batch 70/121] avg loss 0.0346434, throughput 2.48139K wps\n",
      "[Epoch 93 Batch 80/121] avg loss 0.044246, throughput 2.1978K wps\n",
      "[Epoch 93 Batch 90/121] avg loss 0.0287033, throughput 2.33099K wps\n",
      "[Epoch 93 Batch 100/121] avg loss 0.0283123, throughput 2.53165K wps\n",
      "[Epoch 93 Batch 110/121] avg loss 0.0230871, throughput 2.41545K wps\n",
      "[Epoch 93 Batch 120/121] avg loss 0.0208858, throughput 2.68818K wps\n",
      "[Epoch 93] train avg loss 0.0277608, train avg r2 0.967515,throughput 2.46939K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 94 Batch 10/121] avg loss 0.0219181, throughput 2.38663K wps\n",
      "[Epoch 94 Batch 20/121] avg loss 0.0297634, throughput 2.55754K wps\n",
      "[Epoch 94 Batch 30/121] avg loss 0.0252462, throughput 2.61096K wps\n",
      "[Epoch 94 Batch 40/121] avg loss 0.0163171, throughput 2.48756K wps\n",
      "[Epoch 94 Batch 50/121] avg loss 0.028471, throughput 2.53807K wps\n",
      "[Epoch 94 Batch 60/121] avg loss 0.0324334, throughput 2.45098K wps\n",
      "[Epoch 94 Batch 70/121] avg loss 0.0342381, throughput 2.48139K wps\n",
      "[Epoch 94 Batch 80/121] avg loss 0.0427623, throughput 2.19298K wps\n",
      "[Epoch 94 Batch 90/121] avg loss 0.0303901, throughput 2.331K wps\n",
      "[Epoch 94 Batch 100/121] avg loss 0.027553, throughput 2.53806K wps\n",
      "[Epoch 94 Batch 110/121] avg loss 0.0226773, throughput 2.42718K wps\n",
      "[Epoch 94 Batch 120/121] avg loss 0.0191909, throughput 2.68818K wps\n",
      "[Epoch 94] train avg loss 0.0275737, train avg r2 0.967872,throughput 2.47292K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 95 Batch 10/121] avg loss 0.0216059, throughput 2.09644K wps\n",
      "[Epoch 95 Batch 20/121] avg loss 0.0267154, throughput 2.33101K wps\n",
      "[Epoch 95 Batch 30/121] avg loss 0.0264018, throughput 2.44499K wps\n",
      "[Epoch 95 Batch 40/121] avg loss 0.0147746, throughput 2.46914K wps\n",
      "[Epoch 95 Batch 50/121] avg loss 0.0282803, throughput 2.41546K wps\n",
      "[Epoch 95 Batch 60/121] avg loss 0.0310861, throughput 2.38663K wps\n",
      "[Epoch 95 Batch 70/121] avg loss 0.0354598, throughput 2.46914K wps\n",
      "[Epoch 95 Batch 80/121] avg loss 0.0407566, throughput 2.17865K wps\n",
      "[Epoch 95 Batch 90/121] avg loss 0.029094, throughput 2.32558K wps\n",
      "[Epoch 95 Batch 100/121] avg loss 0.0285885, throughput 2.50627K wps\n",
      "[Epoch 95 Batch 110/121] avg loss 0.0197493, throughput 2.40964K wps\n",
      "[Epoch 95 Batch 120/121] avg loss 0.0212923, throughput 2.65957K wps\n",
      "[Epoch 95] train avg loss 0.0269738, train avg r2 0.968431,throughput 2.38847K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 96 Batch 10/121] avg loss 0.0201564, throughput 2.36966K wps\n",
      "[Epoch 96 Batch 20/121] avg loss 0.0265619, throughput 2.52526K wps\n",
      "[Epoch 96 Batch 30/121] avg loss 0.0263924, throughput 2.58398K wps\n",
      "[Epoch 96 Batch 40/121] avg loss 0.016351, throughput 2.47524K wps\n",
      "[Epoch 96 Batch 50/121] avg loss 0.0295283, throughput 2.53807K wps\n",
      "[Epoch 96 Batch 60/121] avg loss 0.0298223, throughput 2.44498K wps\n",
      "[Epoch 96 Batch 70/121] avg loss 0.0359784, throughput 2.47526K wps\n",
      "[Epoch 96 Batch 80/121] avg loss 0.0429791, throughput 2.19298K wps\n",
      "[Epoch 96 Batch 90/121] avg loss 0.0299981, throughput 2.32558K wps\n",
      "[Epoch 96 Batch 100/121] avg loss 0.0305351, throughput 2.52525K wps\n",
      "[Epoch 96 Batch 110/121] avg loss 0.0225539, throughput 2.4213K wps\n",
      "[Epoch 96 Batch 120/121] avg loss 0.0205776, throughput 2.6738K wps\n",
      "[Epoch 96] train avg loss 0.0276084, train avg r2 0.968052,throughput 2.46135K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 97 Batch 10/121] avg loss 0.0224187, throughput 2.36967K wps\n",
      "[Epoch 97 Batch 20/121] avg loss 0.0304257, throughput 2.53165K wps\n",
      "[Epoch 97 Batch 30/121] avg loss 0.02406, throughput 2.60416K wps\n",
      "[Epoch 97 Batch 40/121] avg loss 0.0159027, throughput 2.49378K wps\n",
      "[Epoch 97 Batch 50/121] avg loss 0.0291637, throughput 2.53807K wps\n",
      "[Epoch 97 Batch 60/121] avg loss 0.0339053, throughput 2.44498K wps\n",
      "[Epoch 97 Batch 70/121] avg loss 0.0342113, throughput 2.48138K wps\n",
      "[Epoch 97 Batch 80/121] avg loss 0.0455889, throughput 2.19298K wps\n",
      "[Epoch 97 Batch 90/121] avg loss 0.0306517, throughput 2.33644K wps\n",
      "[Epoch 97 Batch 100/121] avg loss 0.0287967, throughput 2.52526K wps\n",
      "[Epoch 97 Batch 110/121] avg loss 0.0228228, throughput 2.41546K wps\n",
      "[Epoch 97 Batch 120/121] avg loss 0.0196233, throughput 2.68096K wps\n",
      "[Epoch 97] train avg loss 0.0281191, train avg r2 0.967325,throughput 2.46587K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 98 Batch 10/121] avg loss 0.0239872, throughput 2.39234K wps\n",
      "[Epoch 98 Batch 20/121] avg loss 0.0273858, throughput 2.55101K wps\n",
      "[Epoch 98 Batch 30/121] avg loss 0.0248671, throughput 2.60416K wps\n",
      "[Epoch 98 Batch 40/121] avg loss 0.0153851, throughput 2.47525K wps\n",
      "[Epoch 98 Batch 50/121] avg loss 0.0269129, throughput 2.53808K wps\n",
      "[Epoch 98 Batch 60/121] avg loss 0.0326239, throughput 2.44498K wps\n",
      "[Epoch 98 Batch 70/121] avg loss 0.0354444, throughput 2.46306K wps\n",
      "[Epoch 98 Batch 80/121] avg loss 0.0426667, throughput 2.1834K wps\n",
      "[Epoch 98 Batch 90/121] avg loss 0.0301221, throughput 2.33645K wps\n",
      "[Epoch 98 Batch 100/121] avg loss 0.0292656, throughput 2.52524K wps\n",
      "[Epoch 98 Batch 110/121] avg loss 0.0209655, throughput 2.42131K wps\n",
      "[Epoch 98 Batch 120/121] avg loss 0.0202272, throughput 2.68816K wps\n",
      "[Epoch 98] train avg loss 0.0274819, train avg r2 0.967854,throughput 2.46536K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 99 Batch 10/121] avg loss 0.0222575, throughput 2.38095K wps\n",
      "[Epoch 99 Batch 20/121] avg loss 0.0275287, throughput 2.53164K wps\n",
      "[Epoch 99 Batch 30/121] avg loss 0.024674, throughput 2.61096K wps\n",
      "[Epoch 99 Batch 40/121] avg loss 0.0142442, throughput 2.49999K wps\n",
      "[Epoch 99 Batch 50/121] avg loss 0.0281236, throughput 2.53165K wps\n",
      "[Epoch 99 Batch 60/121] avg loss 0.029757, throughput 2.44499K wps\n",
      "[Epoch 99 Batch 70/121] avg loss 0.0369997, throughput 2.47525K wps\n",
      "[Epoch 99 Batch 80/121] avg loss 0.0428055, throughput 2.19297K wps\n",
      "[Epoch 99 Batch 90/121] avg loss 0.0306883, throughput 2.331K wps\n",
      "[Epoch 99 Batch 100/121] avg loss 0.0289735, throughput 2.52525K wps\n",
      "[Epoch 99 Batch 110/121] avg loss 0.022163, throughput 2.4213K wps\n",
      "[Epoch 99 Batch 120/121] avg loss 0.0193392, throughput 2.68096K wps\n",
      "[Epoch 99] train avg loss 0.0272895, train avg r2 0.968302,throughput 2.46637K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 100 Batch 10/121] avg loss 0.0219476, throughput 2.38095K wps\n",
      "[Epoch 100 Batch 20/121] avg loss 0.0289227, throughput 2.54453K wps\n",
      "[Epoch 100 Batch 30/121] avg loss 0.0268368, throughput 2.59741K wps\n",
      "[Epoch 100 Batch 40/121] avg loss 0.0156275, throughput 2.49999K wps\n",
      "[Epoch 100 Batch 50/121] avg loss 0.0266289, throughput 2.52525K wps\n",
      "[Epoch 100 Batch 60/121] avg loss 0.0309608, throughput 2.45098K wps\n",
      "[Epoch 100 Batch 70/121] avg loss 0.0386137, throughput 2.47524K wps\n",
      "[Epoch 100 Batch 80/121] avg loss 0.0419824, throughput 2.19299K wps\n",
      "[Epoch 100 Batch 90/121] avg loss 0.0301127, throughput 2.33099K wps\n",
      "[Epoch 100 Batch 100/121] avg loss 0.0287787, throughput 2.5189K wps\n",
      "[Epoch 100 Batch 110/121] avg loss 0.0217092, throughput 2.42717K wps\n",
      "[Epoch 100 Batch 120/121] avg loss 0.0213611, throughput 2.68817K wps\n",
      "[Epoch 100] train avg loss 0.0277778, train avg r2 0.967837,throughput 2.46687K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 101 Batch 10/121] avg loss 0.0210318, throughput 2.38663K wps\n",
      "[Epoch 101 Batch 20/121] avg loss 0.0277104, throughput 2.53165K wps\n",
      "[Epoch 101 Batch 30/121] avg loss 0.0248881, throughput 2.61096K wps\n",
      "[Epoch 101 Batch 40/121] avg loss 0.0163581, throughput 2.49376K wps\n",
      "[Epoch 101 Batch 50/121] avg loss 0.0270835, throughput 2.54453K wps\n",
      "[Epoch 101 Batch 60/121] avg loss 0.0301925, throughput 2.44499K wps\n",
      "[Epoch 101 Batch 70/121] avg loss 0.0398562, throughput 2.48756K wps\n",
      "[Epoch 101 Batch 80/121] avg loss 0.0421961, throughput 2.19297K wps\n",
      "[Epoch 101 Batch 90/121] avg loss 0.028269, throughput 2.33101K wps\n",
      "[Epoch 101 Batch 100/121] avg loss 0.0287765, throughput 2.52525K wps\n",
      "[Epoch 101 Batch 110/121] avg loss 0.0218047, throughput 2.41546K wps\n",
      "[Epoch 101 Batch 120/121] avg loss 0.0212787, throughput 2.68817K wps\n",
      "[Epoch 101] train avg loss 0.027443, train avg r2 0.968209,throughput 2.46888K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 102 Batch 10/121] avg loss 0.0209168, throughput 2.38664K wps\n",
      "[Epoch 102 Batch 20/121] avg loss 0.0267174, throughput 2.55101K wps\n",
      "[Epoch 102 Batch 30/121] avg loss 0.0245068, throughput 2.5974K wps\n",
      "[Epoch 102 Batch 40/121] avg loss 0.0158222, throughput 2.49377K wps\n",
      "[Epoch 102 Batch 50/121] avg loss 0.0285947, throughput 2.52525K wps\n",
      "[Epoch 102 Batch 60/121] avg loss 0.0315304, throughput 2.44499K wps\n",
      "[Epoch 102 Batch 70/121] avg loss 0.0365478, throughput 2.48756K wps\n",
      "[Epoch 102 Batch 80/121] avg loss 0.0423933, throughput 2.19298K wps\n",
      "[Epoch 102 Batch 90/121] avg loss 0.0311327, throughput 2.331K wps\n",
      "[Epoch 102 Batch 100/121] avg loss 0.0280269, throughput 2.52525K wps\n",
      "[Epoch 102 Batch 110/121] avg loss 0.022217, throughput 2.4213K wps\n",
      "[Epoch 102 Batch 120/121] avg loss 0.0206104, throughput 2.68817K wps\n",
      "[Epoch 102] train avg loss 0.0274086, train avg r2 0.968352,throughput 2.46888K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 103 Batch 10/121] avg loss 0.0223903, throughput 2.38662K wps\n",
      "[Epoch 103 Batch 20/121] avg loss 0.0291587, throughput 2.54453K wps\n",
      "[Epoch 103 Batch 30/121] avg loss 0.0263866, throughput 2.44499K wps\n",
      "[Epoch 103 Batch 40/121] avg loss 0.0155506, throughput 2.49377K wps\n",
      "[Epoch 103 Batch 50/121] avg loss 0.0272199, throughput 2.55102K wps\n",
      "[Epoch 103 Batch 60/121] avg loss 0.0310129, throughput 2.45099K wps\n",
      "[Epoch 103 Batch 70/121] avg loss 0.0368482, throughput 2.46306K wps\n",
      "[Epoch 103 Batch 80/121] avg loss 0.0449907, throughput 2.13675K wps\n",
      "[Epoch 103 Batch 90/121] avg loss 0.0282609, throughput 2.331K wps\n",
      "[Epoch 103 Batch 100/121] avg loss 0.0311196, throughput 2.52524K wps\n",
      "[Epoch 103 Batch 110/121] avg loss 0.0227268, throughput 2.42132K wps\n",
      "[Epoch 103 Batch 120/121] avg loss 0.0218386, throughput 2.68095K wps\n",
      "[Epoch 103] train avg loss 0.0281128, train avg r2 0.966997,throughput 2.44939K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 104 Batch 10/121] avg loss 0.0220497, throughput 2.38096K wps\n",
      "[Epoch 104 Batch 20/121] avg loss 0.0279904, throughput 2.55754K wps\n",
      "[Epoch 104 Batch 30/121] avg loss 0.0241785, throughput 2.59741K wps\n",
      "[Epoch 104 Batch 40/121] avg loss 0.0144459, throughput 2.50625K wps\n",
      "[Epoch 104 Batch 50/121] avg loss 0.0294133, throughput 2.5189K wps\n",
      "[Epoch 104 Batch 60/121] avg loss 0.0305511, throughput 2.45097K wps\n",
      "[Epoch 104 Batch 70/121] avg loss 0.0356334, throughput 2.47525K wps\n",
      "[Epoch 104 Batch 80/121] avg loss 0.0409419, throughput 2.19779K wps\n",
      "[Epoch 104 Batch 90/121] avg loss 0.0295596, throughput 2.331K wps\n",
      "[Epoch 104 Batch 100/121] avg loss 0.0293873, throughput 2.52525K wps\n",
      "[Epoch 104 Batch 110/121] avg loss 0.0218399, throughput 2.42718K wps\n",
      "[Epoch 104 Batch 120/121] avg loss 0.0197339, throughput 2.6738K wps\n",
      "[Epoch 104] train avg loss 0.0271347, train avg r2 0.968404,throughput 2.46687K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 105 Batch 10/121] avg loss 0.0211608, throughput 2.38095K wps\n",
      "[Epoch 105 Batch 20/121] avg loss 0.0290978, throughput 2.55102K wps\n",
      "[Epoch 105 Batch 30/121] avg loss 0.0271421, throughput 2.58398K wps\n",
      "[Epoch 105 Batch 40/121] avg loss 0.0150268, throughput 2.49377K wps\n",
      "[Epoch 105 Batch 50/121] avg loss 0.0277307, throughput 2.53807K wps\n",
      "[Epoch 105 Batch 60/121] avg loss 0.0318401, throughput 2.44499K wps\n",
      "[Epoch 105 Batch 70/121] avg loss 0.0374279, throughput 2.48756K wps\n",
      "[Epoch 105 Batch 80/121] avg loss 0.0416621, throughput 2.19297K wps\n",
      "[Epoch 105 Batch 90/121] avg loss 0.0275375, throughput 2.331K wps\n",
      "[Epoch 105 Batch 100/121] avg loss 0.0305263, throughput 2.51889K wps\n",
      "[Epoch 105 Batch 110/121] avg loss 0.0231562, throughput 2.42718K wps\n",
      "[Epoch 105 Batch 120/121] avg loss 0.0205244, throughput 2.68817K wps\n",
      "[Epoch 105] train avg loss 0.0277276, train avg r2 0.967685,throughput 2.46788K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 106 Batch 10/121] avg loss 0.0209738, throughput 2.3753K wps\n",
      "[Epoch 106 Batch 20/121] avg loss 0.0274008, throughput 2.55102K wps\n",
      "[Epoch 106 Batch 30/121] avg loss 0.0262359, throughput 2.60416K wps\n",
      "[Epoch 106 Batch 40/121] avg loss 0.0147784, throughput 2.51257K wps\n",
      "[Epoch 106 Batch 50/121] avg loss 0.0253083, throughput 2.53807K wps\n",
      "[Epoch 106 Batch 60/121] avg loss 0.0319085, throughput 2.44499K wps\n",
      "[Epoch 106 Batch 70/121] avg loss 0.0366748, throughput 2.46914K wps\n",
      "[Epoch 106 Batch 80/121] avg loss 0.0422829, throughput 2.19298K wps\n",
      "[Epoch 106 Batch 90/121] avg loss 0.0297187, throughput 2.32558K wps\n",
      "[Epoch 106 Batch 100/121] avg loss 0.0277423, throughput 2.51889K wps\n",
      "[Epoch 106 Batch 110/121] avg loss 0.0218639, throughput 2.4213K wps\n",
      "[Epoch 106 Batch 120/121] avg loss 0.0200698, throughput 2.68096K wps\n",
      "[Epoch 106] train avg loss 0.0270745, train avg r2 0.968425,throughput 2.46687K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 107 Batch 10/121] avg loss 0.0213892, throughput 2.38664K wps\n",
      "[Epoch 107 Batch 20/121] avg loss 0.0261422, throughput 2.54453K wps\n",
      "[Epoch 107 Batch 30/121] avg loss 0.026226, throughput 2.59067K wps\n",
      "[Epoch 107 Batch 40/121] avg loss 0.0143279, throughput 2.5K wps\n",
      "[Epoch 107 Batch 50/121] avg loss 0.0289368, throughput 2.53807K wps\n",
      "[Epoch 107 Batch 60/121] avg loss 0.0323014, throughput 2.43901K wps\n",
      "[Epoch 107 Batch 70/121] avg loss 0.0372086, throughput 2.47526K wps\n",
      "[Epoch 107 Batch 80/121] avg loss 0.0421822, throughput 2.19298K wps\n",
      "[Epoch 107 Batch 90/121] avg loss 0.0306049, throughput 2.33101K wps\n",
      "[Epoch 107 Batch 100/121] avg loss 0.0287928, throughput 2.51889K wps\n",
      "[Epoch 107 Batch 110/121] avg loss 0.0210686, throughput 2.42131K wps\n",
      "[Epoch 107 Batch 120/121] avg loss 0.0201345, throughput 2.68096K wps\n",
      "[Epoch 107] train avg loss 0.0274408, train avg r2 0.968143,throughput 2.46788K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 108 Batch 10/121] avg loss 0.0205259, throughput 2.36967K wps\n",
      "[Epoch 108 Batch 20/121] avg loss 0.0263953, throughput 2.55101K wps\n",
      "[Epoch 108 Batch 30/121] avg loss 0.0248095, throughput 2.60416K wps\n",
      "[Epoch 108 Batch 40/121] avg loss 0.0145849, throughput 2.49376K wps\n",
      "[Epoch 108 Batch 50/121] avg loss 0.0287842, throughput 2.53808K wps\n",
      "[Epoch 108 Batch 60/121] avg loss 0.0309184, throughput 2.44499K wps\n",
      "[Epoch 108 Batch 70/121] avg loss 0.0359155, throughput 2.47525K wps\n",
      "[Epoch 108 Batch 80/121] avg loss 0.0418471, throughput 2.19299K wps\n",
      "[Epoch 108 Batch 90/121] avg loss 0.0295451, throughput 2.33644K wps\n",
      "[Epoch 108 Batch 100/121] avg loss 0.0293963, throughput 2.52525K wps\n",
      "[Epoch 108 Batch 110/121] avg loss 0.0215327, throughput 2.41546K wps\n",
      "[Epoch 108 Batch 120/121] avg loss 0.0193019, throughput 2.66667K wps\n",
      "[Epoch 108] train avg loss 0.0269612, train avg r2 0.96823,throughput 2.46586K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 109 Batch 10/121] avg loss 0.0201466, throughput 2.38095K wps\n",
      "[Epoch 109 Batch 20/121] avg loss 0.025906, throughput 2.54452K wps\n",
      "[Epoch 109 Batch 30/121] avg loss 0.0247519, throughput 2.59741K wps\n",
      "[Epoch 109 Batch 40/121] avg loss 0.0156209, throughput 2.48139K wps\n",
      "[Epoch 109 Batch 50/121] avg loss 0.0295104, throughput 2.53164K wps\n",
      "[Epoch 109 Batch 60/121] avg loss 0.0292448, throughput 2.45097K wps\n",
      "[Epoch 109 Batch 70/121] avg loss 0.0363832, throughput 2.48757K wps\n",
      "[Epoch 109 Batch 80/121] avg loss 0.0437395, throughput 2.19297K wps\n",
      "[Epoch 109 Batch 90/121] avg loss 0.0286581, throughput 2.33645K wps\n",
      "[Epoch 109 Batch 100/121] avg loss 0.0273876, throughput 2.51889K wps\n",
      "[Epoch 109 Batch 110/121] avg loss 0.0221653, throughput 2.4213K wps\n",
      "[Epoch 109 Batch 120/121] avg loss 0.020632, throughput 2.68097K wps\n",
      "[Epoch 109] train avg loss 0.0270031, train avg r2 0.968827,throughput 2.46737K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 110 Batch 10/121] avg loss 0.0225313, throughput 2.36962K wps\n",
      "[Epoch 110 Batch 20/121] avg loss 0.0263928, throughput 2.55754K wps\n",
      "[Epoch 110 Batch 30/121] avg loss 0.0244871, throughput 2.60417K wps\n",
      "[Epoch 110 Batch 40/121] avg loss 0.0154159, throughput 2.50626K wps\n",
      "[Epoch 110 Batch 50/121] avg loss 0.0271562, throughput 2.53807K wps\n",
      "[Epoch 110 Batch 60/121] avg loss 0.0295248, throughput 2.457K wps\n",
      "[Epoch 110 Batch 70/121] avg loss 0.0333364, throughput 2.48756K wps\n",
      "[Epoch 110 Batch 80/121] avg loss 0.0426565, throughput 2.18341K wps\n",
      "[Epoch 110 Batch 90/121] avg loss 0.0304607, throughput 2.33099K wps\n",
      "[Epoch 110 Batch 100/121] avg loss 0.0282952, throughput 2.53164K wps\n",
      "[Epoch 110 Batch 110/121] avg loss 0.0234152, throughput 2.4213K wps\n",
      "[Epoch 110 Batch 120/121] avg loss 0.0200714, throughput 2.6738K wps\n",
      "[Epoch 110] train avg loss 0.0269736, train avg r2 0.968207,throughput 2.46938K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 111 Batch 10/121] avg loss 0.0222552, throughput 2.38095K wps\n",
      "[Epoch 111 Batch 20/121] avg loss 0.0299891, throughput 2.55101K wps\n",
      "[Epoch 111 Batch 30/121] avg loss 0.0258984, throughput 2.60417K wps\n",
      "[Epoch 111 Batch 40/121] avg loss 0.0147513, throughput 2.49377K wps\n",
      "[Epoch 111 Batch 50/121] avg loss 0.0272884, throughput 2.53807K wps\n",
      "[Epoch 111 Batch 60/121] avg loss 0.0308641, throughput 2.45097K wps\n",
      "[Epoch 111 Batch 70/121] avg loss 0.0332459, throughput 2.4814K wps\n",
      "[Epoch 111 Batch 80/121] avg loss 0.0434604, throughput 2.18819K wps\n",
      "[Epoch 111 Batch 90/121] avg loss 0.0288945, throughput 2.33645K wps\n",
      "[Epoch 111 Batch 100/121] avg loss 0.0286834, throughput 2.51889K wps\n",
      "[Epoch 111 Batch 110/121] avg loss 0.0232465, throughput 2.42718K wps\n",
      "[Epoch 111 Batch 120/121] avg loss 0.0196614, throughput 2.68097K wps\n",
      "[Epoch 111] train avg loss 0.0273435, train avg r2 0.967819,throughput 2.46939K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 112 Batch 10/121] avg loss 0.0232964, throughput 2.38664K wps\n",
      "[Epoch 112 Batch 20/121] avg loss 0.0271148, throughput 2.53807K wps\n",
      "[Epoch 112 Batch 30/121] avg loss 0.026562, throughput 2.59739K wps\n",
      "[Epoch 112 Batch 40/121] avg loss 0.0144094, throughput 2.50001K wps\n",
      "[Epoch 112 Batch 50/121] avg loss 0.0286097, throughput 2.55102K wps\n",
      "[Epoch 112 Batch 60/121] avg loss 0.0312013, throughput 2.457K wps\n",
      "[Epoch 112 Batch 70/121] avg loss 0.0374252, throughput 2.48139K wps\n",
      "[Epoch 112 Batch 80/121] avg loss 0.0420671, throughput 2.19298K wps\n",
      "[Epoch 112 Batch 90/121] avg loss 0.0313963, throughput 2.331K wps\n",
      "[Epoch 112 Batch 100/121] avg loss 0.0299666, throughput 2.5189K wps\n",
      "[Epoch 112 Batch 110/121] avg loss 0.0213188, throughput 2.41546K wps\n",
      "[Epoch 112 Batch 120/121] avg loss 0.0200681, throughput 2.68816K wps\n",
      "[Epoch 112] train avg loss 0.0277776, train avg r2 0.967802,throughput 2.46888K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 113 Batch 10/121] avg loss 0.0229627, throughput 2.38096K wps\n",
      "[Epoch 113 Batch 20/121] avg loss 0.0261215, throughput 2.53808K wps\n",
      "[Epoch 113 Batch 30/121] avg loss 0.0260187, throughput 2.5974K wps\n",
      "[Epoch 113 Batch 40/121] avg loss 0.0144087, throughput 2.5K wps\n",
      "[Epoch 113 Batch 50/121] avg loss 0.0276344, throughput 2.52525K wps\n",
      "[Epoch 113 Batch 60/121] avg loss 0.0303867, throughput 2.45098K wps\n",
      "[Epoch 113 Batch 70/121] avg loss 0.0373881, throughput 2.48139K wps\n",
      "[Epoch 113 Batch 80/121] avg loss 0.0411018, throughput 2.19298K wps\n",
      "[Epoch 113 Batch 90/121] avg loss 0.0264957, throughput 2.32558K wps\n",
      "[Epoch 113 Batch 100/121] avg loss 0.0285213, throughput 2.51889K wps\n",
      "[Epoch 113 Batch 110/121] avg loss 0.0219937, throughput 2.42718K wps\n",
      "[Epoch 113 Batch 120/121] avg loss 0.0207714, throughput 2.6954K wps\n",
      "[Epoch 113] train avg loss 0.026977, train avg r2 0.968111,throughput 2.46788K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 114 Batch 10/121] avg loss 0.021778, throughput 2.38664K wps\n",
      "[Epoch 114 Batch 20/121] avg loss 0.0277508, throughput 2.53807K wps\n",
      "[Epoch 114 Batch 30/121] avg loss 0.0247358, throughput 2.59741K wps\n",
      "[Epoch 114 Batch 40/121] avg loss 0.0147911, throughput 2.5K wps\n",
      "[Epoch 114 Batch 50/121] avg loss 0.0284282, throughput 2.54453K wps\n",
      "[Epoch 114 Batch 60/121] avg loss 0.0304282, throughput 2.45098K wps\n",
      "[Epoch 114 Batch 70/121] avg loss 0.0342681, throughput 2.47524K wps\n",
      "[Epoch 114 Batch 80/121] avg loss 0.0418756, throughput 2.19299K wps\n",
      "[Epoch 114 Batch 90/121] avg loss 0.0295874, throughput 2.331K wps\n",
      "[Epoch 114 Batch 100/121] avg loss 0.0286956, throughput 2.53162K wps\n",
      "[Epoch 114 Batch 110/121] avg loss 0.0208363, throughput 2.41547K wps\n",
      "[Epoch 114 Batch 120/121] avg loss 0.0212221, throughput 2.6738K wps\n",
      "[Epoch 114] train avg loss 0.0270237, train avg r2 0.968236,throughput 2.46838K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 115 Batch 10/121] avg loss 0.0215905, throughput 2.38095K wps\n",
      "[Epoch 115 Batch 20/121] avg loss 0.0278367, throughput 2.55101K wps\n",
      "[Epoch 115 Batch 30/121] avg loss 0.026338, throughput 2.5974K wps\n",
      "[Epoch 115 Batch 40/121] avg loss 0.0163093, throughput 2.50626K wps\n",
      "[Epoch 115 Batch 50/121] avg loss 0.0250799, throughput 2.52526K wps\n",
      "[Epoch 115 Batch 60/121] avg loss 0.0302636, throughput 2.44498K wps\n",
      "[Epoch 115 Batch 70/121] avg loss 0.0351505, throughput 2.47525K wps\n",
      "[Epoch 115 Batch 80/121] avg loss 0.0387601, throughput 2.19298K wps\n",
      "[Epoch 115 Batch 90/121] avg loss 0.0295331, throughput 2.331K wps\n",
      "[Epoch 115 Batch 100/121] avg loss 0.0290628, throughput 2.5189K wps\n",
      "[Epoch 115 Batch 110/121] avg loss 0.0217017, throughput 2.4213K wps\n",
      "[Epoch 115 Batch 120/121] avg loss 0.0211778, throughput 2.68097K wps\n",
      "[Epoch 115] train avg loss 0.0268891, train avg r2 0.968518,throughput 2.46737K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 116 Batch 10/121] avg loss 0.0210848, throughput 2.38663K wps\n",
      "[Epoch 116 Batch 20/121] avg loss 0.0273064, throughput 2.55102K wps\n",
      "[Epoch 116 Batch 30/121] avg loss 0.0268313, throughput 2.57732K wps\n",
      "[Epoch 116 Batch 40/121] avg loss 0.015673, throughput 2.49376K wps\n",
      "[Epoch 116 Batch 50/121] avg loss 0.0291056, throughput 2.54453K wps\n",
      "[Epoch 116 Batch 60/121] avg loss 0.0323636, throughput 2.45099K wps\n",
      "[Epoch 116 Batch 70/121] avg loss 0.036822, throughput 2.48138K wps\n",
      "[Epoch 116 Batch 80/121] avg loss 0.0418526, throughput 2.19298K wps\n",
      "[Epoch 116 Batch 90/121] avg loss 0.0305913, throughput 2.33645K wps\n",
      "[Epoch 116 Batch 100/121] avg loss 0.0296476, throughput 2.52525K wps\n",
      "[Epoch 116 Batch 110/121] avg loss 0.0228677, throughput 2.41546K wps\n",
      "[Epoch 116 Batch 120/121] avg loss 0.0195665, throughput 2.68097K wps\n",
      "[Epoch 116] train avg loss 0.0277995, train avg r2 0.967565,throughput 2.46838K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 117 Batch 10/121] avg loss 0.0226413, throughput 2.38663K wps\n",
      "[Epoch 117 Batch 20/121] avg loss 0.0273401, throughput 2.55103K wps\n",
      "[Epoch 117 Batch 30/121] avg loss 0.0239256, throughput 2.60416K wps\n",
      "[Epoch 117 Batch 40/121] avg loss 0.0135764, throughput 2.5K wps\n",
      "[Epoch 117 Batch 50/121] avg loss 0.0275162, throughput 2.5189K wps\n",
      "[Epoch 117 Batch 60/121] avg loss 0.0320409, throughput 2.43903K wps\n",
      "[Epoch 117 Batch 70/121] avg loss 0.036128, throughput 2.47525K wps\n",
      "[Epoch 117 Batch 80/121] avg loss 0.0428405, throughput 2.19297K wps\n",
      "[Epoch 117 Batch 90/121] avg loss 0.0302403, throughput 2.34192K wps\n",
      "[Epoch 117 Batch 100/121] avg loss 0.0286962, throughput 2.51889K wps\n",
      "[Epoch 117 Batch 110/121] avg loss 0.0216761, throughput 2.42718K wps\n",
      "[Epoch 117 Batch 120/121] avg loss 0.0192463, throughput 2.67381K wps\n",
      "[Epoch 117] train avg loss 0.0271439, train avg r2 0.968647,throughput 2.46788K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 118 Batch 10/121] avg loss 0.0213608, throughput 2.38664K wps\n",
      "[Epoch 118 Batch 20/121] avg loss 0.0268417, throughput 2.55101K wps\n",
      "[Epoch 118 Batch 30/121] avg loss 0.0245359, throughput 2.59067K wps\n",
      "[Epoch 118 Batch 40/121] avg loss 0.0141263, throughput 2.50001K wps\n",
      "[Epoch 118 Batch 50/121] avg loss 0.027801, throughput 2.53806K wps\n",
      "[Epoch 118 Batch 60/121] avg loss 0.0332615, throughput 2.45098K wps\n",
      "[Epoch 118 Batch 70/121] avg loss 0.0379487, throughput 2.47525K wps\n",
      "[Epoch 118 Batch 80/121] avg loss 0.0430978, throughput 2.19297K wps\n",
      "[Epoch 118 Batch 90/121] avg loss 0.0272569, throughput 2.33645K wps\n",
      "[Epoch 118 Batch 100/121] avg loss 0.0269178, throughput 2.5189K wps\n",
      "[Epoch 118 Batch 110/121] avg loss 0.0216786, throughput 2.4213K wps\n",
      "[Epoch 118 Batch 120/121] avg loss 0.0201711, throughput 2.69542K wps\n",
      "[Epoch 118] train avg loss 0.027075, train avg r2 0.968742,throughput 2.46939K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 119 Batch 10/121] avg loss 0.022628, throughput 2.3753K wps\n",
      "[Epoch 119 Batch 20/121] avg loss 0.0292011, throughput 2.55102K wps\n",
      "[Epoch 119 Batch 30/121] avg loss 0.0254125, throughput 2.60416K wps\n",
      "[Epoch 119 Batch 40/121] avg loss 0.0163657, throughput 2.49377K wps\n",
      "[Epoch 119 Batch 50/121] avg loss 0.0286074, throughput 2.54451K wps\n",
      "[Epoch 119 Batch 60/121] avg loss 0.031943, throughput 2.45098K wps\n",
      "[Epoch 119 Batch 70/121] avg loss 0.0380384, throughput 2.47526K wps\n",
      "[Epoch 119 Batch 80/121] avg loss 0.0405587, throughput 2.18818K wps\n",
      "[Epoch 119 Batch 90/121] avg loss 0.0296831, throughput 2.32558K wps\n",
      "[Epoch 119 Batch 100/121] avg loss 0.0288308, throughput 2.51257K wps\n",
      "[Epoch 119 Batch 110/121] avg loss 0.0211987, throughput 2.4213K wps\n",
      "[Epoch 119 Batch 120/121] avg loss 0.021006, throughput 2.68817K wps\n",
      "[Epoch 119] train avg loss 0.0277795, train avg r2 0.967695,throughput 2.46788K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 120 Batch 10/121] avg loss 0.0217467, throughput 2.38094K wps\n",
      "[Epoch 120 Batch 20/121] avg loss 0.0282133, throughput 2.55103K wps\n",
      "[Epoch 120 Batch 30/121] avg loss 0.0255297, throughput 2.59067K wps\n",
      "[Epoch 120 Batch 40/121] avg loss 0.0154955, throughput 2.49377K wps\n",
      "[Epoch 120 Batch 50/121] avg loss 0.0251178, throughput 2.53806K wps\n",
      "[Epoch 120 Batch 60/121] avg loss 0.0303814, throughput 2.45098K wps\n",
      "[Epoch 120 Batch 70/121] avg loss 0.0388422, throughput 2.48139K wps\n",
      "[Epoch 120 Batch 80/121] avg loss 0.0407547, throughput 2.19298K wps\n",
      "[Epoch 120 Batch 90/121] avg loss 0.0301505, throughput 2.33645K wps\n",
      "[Epoch 120 Batch 100/121] avg loss 0.0281943, throughput 2.52526K wps\n",
      "[Epoch 120 Batch 110/121] avg loss 0.020572, throughput 2.4213K wps\n",
      "[Epoch 120 Batch 120/121] avg loss 0.0206944, throughput 2.68097K wps\n",
      "[Epoch 120] train avg loss 0.0271377, train avg r2 0.968288,throughput 2.46788K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 121 Batch 10/121] avg loss 0.0220473, throughput 2.36967K wps\n",
      "[Epoch 121 Batch 20/121] avg loss 0.0260256, throughput 2.54453K wps\n",
      "[Epoch 121 Batch 30/121] avg loss 0.0235653, throughput 2.60416K wps\n",
      "[Epoch 121 Batch 40/121] avg loss 0.0148643, throughput 2.50001K wps\n",
      "[Epoch 121 Batch 50/121] avg loss 0.0275579, throughput 2.54453K wps\n",
      "[Epoch 121 Batch 60/121] avg loss 0.0327856, throughput 2.44498K wps\n",
      "[Epoch 121 Batch 70/121] avg loss 0.0338742, throughput 2.48139K wps\n",
      "[Epoch 121 Batch 80/121] avg loss 0.0412509, throughput 2.19298K wps\n",
      "[Epoch 121 Batch 90/121] avg loss 0.0277618, throughput 2.32558K wps\n",
      "[Epoch 121 Batch 100/121] avg loss 0.0274893, throughput 2.51889K wps\n",
      "[Epoch 121 Batch 110/121] avg loss 0.0222535, throughput 2.41545K wps\n",
      "[Epoch 121 Batch 120/121] avg loss 0.0199717, throughput 2.68097K wps\n",
      "[Epoch 121] train avg loss 0.0266113, train avg r2 0.968611,throughput 2.46788K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 122 Batch 10/121] avg loss 0.0210603, throughput 2.38662K wps\n",
      "[Epoch 122 Batch 20/121] avg loss 0.028133, throughput 2.55103K wps\n",
      "[Epoch 122 Batch 30/121] avg loss 0.0262007, throughput 2.59067K wps\n",
      "[Epoch 122 Batch 40/121] avg loss 0.0153973, throughput 2.4814K wps\n",
      "[Epoch 122 Batch 50/121] avg loss 0.0256862, throughput 2.54452K wps\n",
      "[Epoch 122 Batch 60/121] avg loss 0.0306369, throughput 2.44498K wps\n",
      "[Epoch 122 Batch 70/121] avg loss 0.0357265, throughput 2.48141K wps\n",
      "[Epoch 122 Batch 80/121] avg loss 0.044483, throughput 2.19298K wps\n",
      "[Epoch 122 Batch 90/121] avg loss 0.0248128, throughput 2.33099K wps\n",
      "[Epoch 122 Batch 100/121] avg loss 0.0280651, throughput 2.51257K wps\n",
      "[Epoch 122 Batch 110/121] avg loss 0.0236222, throughput 2.4213K wps\n",
      "[Epoch 122 Batch 120/121] avg loss 0.0197911, throughput 2.68097K wps\n",
      "[Epoch 122] train avg loss 0.0269588, train avg r2 0.968581,throughput 2.46737K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 123 Batch 10/121] avg loss 0.0217743, throughput 2.38663K wps\n",
      "[Epoch 123 Batch 20/121] avg loss 0.0266072, throughput 2.53164K wps\n",
      "[Epoch 123 Batch 30/121] avg loss 0.0256516, throughput 2.61096K wps\n",
      "[Epoch 123 Batch 40/121] avg loss 0.0158317, throughput 2.49376K wps\n",
      "[Epoch 123 Batch 50/121] avg loss 0.0258152, throughput 2.54453K wps\n",
      "[Epoch 123 Batch 60/121] avg loss 0.0300309, throughput 2.44498K wps\n",
      "[Epoch 123 Batch 70/121] avg loss 0.0357843, throughput 2.48139K wps\n",
      "[Epoch 123 Batch 80/121] avg loss 0.0454099, throughput 2.1978K wps\n",
      "[Epoch 123 Batch 90/121] avg loss 0.0288905, throughput 2.33644K wps\n",
      "[Epoch 123 Batch 100/121] avg loss 0.027339, throughput 2.51888K wps\n",
      "[Epoch 123 Batch 110/121] avg loss 0.0209685, throughput 2.41546K wps\n",
      "[Epoch 123 Batch 120/121] avg loss 0.0204903, throughput 2.68096K wps\n",
      "[Epoch 123] train avg loss 0.0270423, train avg r2 0.968453,throughput 2.46687K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 124 Batch 10/121] avg loss 0.0205482, throughput 2.38095K wps\n",
      "[Epoch 124 Batch 20/121] avg loss 0.0257318, throughput 2.53807K wps\n",
      "[Epoch 124 Batch 30/121] avg loss 0.0239562, throughput 2.59741K wps\n",
      "[Epoch 124 Batch 40/121] avg loss 0.015423, throughput 2.46914K wps\n",
      "[Epoch 124 Batch 50/121] avg loss 0.0274277, throughput 2.53807K wps\n",
      "[Epoch 124 Batch 60/121] avg loss 0.0314692, throughput 2.43902K wps\n",
      "[Epoch 124 Batch 70/121] avg loss 0.0363853, throughput 2.47525K wps\n",
      "[Epoch 124 Batch 80/121] avg loss 0.042397, throughput 2.19298K wps\n",
      "[Epoch 124 Batch 90/121] avg loss 0.0302096, throughput 2.33644K wps\n",
      "[Epoch 124 Batch 100/121] avg loss 0.0281454, throughput 2.52524K wps\n",
      "[Epoch 124 Batch 110/121] avg loss 0.0234026, throughput 2.4213K wps\n",
      "[Epoch 124 Batch 120/121] avg loss 0.0203512, throughput 2.68097K wps\n",
      "[Epoch 124] train avg loss 0.0271119, train avg r2 0.968388,throughput 2.46436K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 125 Batch 10/121] avg loss 0.0230802, throughput 2.38095K wps\n",
      "[Epoch 125 Batch 20/121] avg loss 0.027843, throughput 2.53165K wps\n",
      "[Epoch 125 Batch 30/121] avg loss 0.0256971, throughput 2.60417K wps\n",
      "[Epoch 125 Batch 40/121] avg loss 0.0155485, throughput 2.48756K wps\n",
      "[Epoch 125 Batch 50/121] avg loss 0.0277676, throughput 2.54453K wps\n",
      "[Epoch 125 Batch 60/121] avg loss 0.0302019, throughput 2.44498K wps\n",
      "[Epoch 125 Batch 70/121] avg loss 0.0378599, throughput 2.4814K wps\n",
      "[Epoch 125 Batch 80/121] avg loss 0.0435773, throughput 2.18818K wps\n",
      "[Epoch 125 Batch 90/121] avg loss 0.0299649, throughput 2.331K wps\n",
      "[Epoch 125 Batch 100/121] avg loss 0.0273431, throughput 2.5189K wps\n",
      "[Epoch 125 Batch 110/121] avg loss 0.022099, throughput 2.4213K wps\n",
      "[Epoch 125 Batch 120/121] avg loss 0.0206859, throughput 2.68096K wps\n",
      "[Epoch 125] train avg loss 0.0276285, train avg r2 0.968383,throughput 2.46687K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 126 Batch 10/121] avg loss 0.0213994, throughput 2.38664K wps\n",
      "[Epoch 126 Batch 20/121] avg loss 0.027752, throughput 2.54453K wps\n",
      "[Epoch 126 Batch 30/121] avg loss 0.0231559, throughput 2.60416K wps\n",
      "[Epoch 126 Batch 40/121] avg loss 0.0151365, throughput 2.49377K wps\n",
      "[Epoch 126 Batch 50/121] avg loss 0.0263454, throughput 2.51255K wps\n",
      "[Epoch 126 Batch 60/121] avg loss 0.0338883, throughput 2.45097K wps\n",
      "[Epoch 126 Batch 70/121] avg loss 0.0348397, throughput 2.47525K wps\n",
      "[Epoch 126 Batch 80/121] avg loss 0.0430878, throughput 2.18341K wps\n",
      "[Epoch 126 Batch 90/121] avg loss 0.028937, throughput 2.33645K wps\n",
      "[Epoch 126 Batch 100/121] avg loss 0.0275929, throughput 2.51889K wps\n",
      "[Epoch 126 Batch 110/121] avg loss 0.0235308, throughput 2.42131K wps\n",
      "[Epoch 126 Batch 120/121] avg loss 0.0189809, throughput 2.68096K wps\n",
      "[Epoch 126] train avg loss 0.027047, train avg r2 0.968523,throughput 2.46536K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 127 Batch 10/121] avg loss 0.0191928, throughput 2.38663K wps\n",
      "[Epoch 127 Batch 20/121] avg loss 0.0283203, throughput 2.53165K wps\n",
      "[Epoch 127 Batch 30/121] avg loss 0.0234919, throughput 2.60416K wps\n",
      "[Epoch 127 Batch 40/121] avg loss 0.0158536, throughput 2.5K wps\n",
      "[Epoch 127 Batch 50/121] avg loss 0.0297489, throughput 2.5K wps\n",
      "[Epoch 127 Batch 60/121] avg loss 0.0310013, throughput 2.40964K wps\n",
      "[Epoch 127 Batch 70/121] avg loss 0.0349955, throughput 2.47525K wps\n",
      "[Epoch 127 Batch 80/121] avg loss 0.0443284, throughput 2.15054K wps\n",
      "[Epoch 127 Batch 90/121] avg loss 0.0287007, throughput 2.32019K wps\n",
      "[Epoch 127 Batch 100/121] avg loss 0.0292264, throughput 2.45099K wps\n",
      "[Epoch 127 Batch 110/121] avg loss 0.0233256, throughput 2.4213K wps\n",
      "[Epoch 127 Batch 120/121] avg loss 0.0206685, throughput 2.68098K wps\n",
      "[Epoch 127] train avg loss 0.0273983, train avg r2 0.967566,throughput 2.44989K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 128 Batch 10/121] avg loss 0.0212602, throughput 2.38096K wps\n",
      "[Epoch 128 Batch 20/121] avg loss 0.0279648, throughput 2.55101K wps\n",
      "[Epoch 128 Batch 30/121] avg loss 0.0263658, throughput 2.59741K wps\n",
      "[Epoch 128 Batch 40/121] avg loss 0.0158005, throughput 2.49999K wps\n",
      "[Epoch 128 Batch 50/121] avg loss 0.0268972, throughput 2.51257K wps\n",
      "[Epoch 128 Batch 60/121] avg loss 0.0322043, throughput 2.44498K wps\n",
      "[Epoch 128 Batch 70/121] avg loss 0.0368353, throughput 2.47526K wps\n",
      "[Epoch 128 Batch 80/121] avg loss 0.040142, throughput 2.17865K wps\n",
      "[Epoch 128 Batch 90/121] avg loss 0.0279098, throughput 2.32558K wps\n",
      "[Epoch 128 Batch 100/121] avg loss 0.027994, throughput 2.51889K wps\n",
      "[Epoch 128 Batch 110/121] avg loss 0.0229316, throughput 2.42131K wps\n",
      "[Epoch 128 Batch 120/121] avg loss 0.0193527, throughput 2.68817K wps\n",
      "[Epoch 128] train avg loss 0.027125, train avg r2 0.968797,throughput 2.46486K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 129 Batch 10/121] avg loss 0.0214729, throughput 2.38663K wps\n",
      "[Epoch 129 Batch 20/121] avg loss 0.0276702, throughput 2.55101K wps\n",
      "[Epoch 129 Batch 30/121] avg loss 0.0267286, throughput 2.58399K wps\n",
      "[Epoch 129 Batch 40/121] avg loss 0.0146981, throughput 2.50626K wps\n",
      "[Epoch 129 Batch 50/121] avg loss 0.0277542, throughput 2.53163K wps\n",
      "[Epoch 129 Batch 60/121] avg loss 0.032274, throughput 2.45098K wps\n",
      "[Epoch 129 Batch 70/121] avg loss 0.0331165, throughput 2.48138K wps\n",
      "[Epoch 129 Batch 80/121] avg loss 0.0435741, throughput 2.20265K wps\n",
      "[Epoch 129 Batch 90/121] avg loss 0.0293847, throughput 2.33645K wps\n",
      "[Epoch 129 Batch 100/121] avg loss 0.0294914, throughput 2.52525K wps\n",
      "[Epoch 129 Batch 110/121] avg loss 0.0223504, throughput 2.42718K wps\n",
      "[Epoch 129 Batch 120/121] avg loss 0.0227029, throughput 2.68817K wps\n",
      "[Epoch 129] train avg loss 0.0275902, train avg r2 0.967872,throughput 2.4704K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 130 Batch 10/121] avg loss 0.0217355, throughput 2.38663K wps\n",
      "[Epoch 130 Batch 20/121] avg loss 0.0273174, throughput 2.55103K wps\n",
      "[Epoch 130 Batch 30/121] avg loss 0.0236321, throughput 2.60416K wps\n",
      "[Epoch 130 Batch 40/121] avg loss 0.015121, throughput 2.50627K wps\n",
      "[Epoch 130 Batch 50/121] avg loss 0.0275216, throughput 2.53806K wps\n",
      "[Epoch 130 Batch 60/121] avg loss 0.0321514, throughput 2.45097K wps\n",
      "[Epoch 130 Batch 70/121] avg loss 0.0342269, throughput 2.47525K wps\n",
      "[Epoch 130 Batch 80/121] avg loss 0.0420461, throughput 2.17865K wps\n",
      "[Epoch 130 Batch 90/121] avg loss 0.0288417, throughput 2.331K wps\n",
      "[Epoch 130 Batch 100/121] avg loss 0.0290586, throughput 2.52525K wps\n",
      "[Epoch 130 Batch 110/121] avg loss 0.0211661, throughput 2.4213K wps\n",
      "[Epoch 130 Batch 120/121] avg loss 0.0195798, throughput 2.6738K wps\n",
      "[Epoch 130] train avg loss 0.026857, train avg r2 0.968601,throughput 2.46838K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 131 Batch 10/121] avg loss 0.0203894, throughput 2.38663K wps\n",
      "[Epoch 131 Batch 20/121] avg loss 0.0267742, throughput 2.55103K wps\n",
      "[Epoch 131 Batch 30/121] avg loss 0.025073, throughput 2.58397K wps\n",
      "[Epoch 131 Batch 40/121] avg loss 0.0146631, throughput 2.50626K wps\n",
      "[Epoch 131 Batch 50/121] avg loss 0.0273579, throughput 2.52526K wps\n",
      "[Epoch 131 Batch 60/121] avg loss 0.0314113, throughput 2.44497K wps\n",
      "[Epoch 131 Batch 70/121] avg loss 0.0378166, throughput 2.47526K wps\n",
      "[Epoch 131 Batch 80/121] avg loss 0.0418713, throughput 2.19298K wps\n",
      "[Epoch 131 Batch 90/121] avg loss 0.0287708, throughput 2.33645K wps\n",
      "[Epoch 131 Batch 100/121] avg loss 0.0305003, throughput 2.52525K wps\n",
      "[Epoch 131 Batch 110/121] avg loss 0.0216874, throughput 2.41547K wps\n",
      "[Epoch 131 Batch 120/121] avg loss 0.0202362, throughput 2.68095K wps\n",
      "[Epoch 131] train avg loss 0.0272029, train avg r2 0.968179,throughput 2.46737K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 132 Batch 10/121] avg loss 0.0219058, throughput 2.36966K wps\n",
      "[Epoch 132 Batch 20/121] avg loss 0.0295106, throughput 2.54453K wps\n",
      "[Epoch 132 Batch 30/121] avg loss 0.0258151, throughput 2.60416K wps\n",
      "[Epoch 132 Batch 40/121] avg loss 0.0157631, throughput 2.49999K wps\n",
      "[Epoch 132 Batch 50/121] avg loss 0.0266276, throughput 2.53806K wps\n",
      "[Epoch 132 Batch 60/121] avg loss 0.0305175, throughput 2.45098K wps\n",
      "[Epoch 132 Batch 70/121] avg loss 0.0347315, throughput 2.48139K wps\n",
      "[Epoch 132 Batch 80/121] avg loss 0.0401948, throughput 2.18819K wps\n",
      "[Epoch 132 Batch 90/121] avg loss 0.0280225, throughput 2.33644K wps\n",
      "[Epoch 132 Batch 100/121] avg loss 0.027742, throughput 2.5189K wps\n",
      "[Epoch 132 Batch 110/121] avg loss 0.0244055, throughput 2.42718K wps\n",
      "[Epoch 132 Batch 120/121] avg loss 0.0206092, throughput 2.68096K wps\n",
      "[Epoch 132] train avg loss 0.0271475, train avg r2 0.968141,throughput 2.46637K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 133 Batch 10/121] avg loss 0.0198593, throughput 2.39234K wps\n",
      "[Epoch 133 Batch 20/121] avg loss 0.0251696, throughput 2.55754K wps\n",
      "[Epoch 133 Batch 30/121] avg loss 0.0237631, throughput 2.59067K wps\n",
      "[Epoch 133 Batch 40/121] avg loss 0.014687, throughput 2.49999K wps\n",
      "[Epoch 133 Batch 50/121] avg loss 0.0304961, throughput 2.54453K wps\n",
      "[Epoch 133 Batch 60/121] avg loss 0.03105, throughput 2.45098K wps\n",
      "[Epoch 133 Batch 70/121] avg loss 0.0367966, throughput 2.48138K wps\n",
      "[Epoch 133 Batch 80/121] avg loss 0.0435053, throughput 2.19299K wps\n",
      "[Epoch 133 Batch 90/121] avg loss 0.0293598, throughput 2.331K wps\n",
      "[Epoch 133 Batch 100/121] avg loss 0.0285876, throughput 2.51888K wps\n",
      "[Epoch 133 Batch 110/121] avg loss 0.020904, throughput 2.42132K wps\n",
      "[Epoch 133 Batch 120/121] avg loss 0.0213357, throughput 2.68096K wps\n",
      "[Epoch 133] train avg loss 0.0271169, train avg r2 0.968546,throughput 2.46989K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 134 Batch 10/121] avg loss 0.0226431, throughput 2.36968K wps\n",
      "[Epoch 134 Batch 20/121] avg loss 0.02941, throughput 2.54452K wps\n",
      "[Epoch 134 Batch 30/121] avg loss 0.0247947, throughput 2.5974K wps\n",
      "[Epoch 134 Batch 40/121] avg loss 0.0168863, throughput 2.49377K wps\n",
      "[Epoch 134 Batch 50/121] avg loss 0.0276776, throughput 2.53807K wps\n",
      "[Epoch 134 Batch 60/121] avg loss 0.02956, throughput 2.45098K wps\n",
      "[Epoch 134 Batch 70/121] avg loss 0.0372827, throughput 2.47525K wps\n",
      "[Epoch 134 Batch 80/121] avg loss 0.0445656, throughput 2.19298K wps\n",
      "[Epoch 134 Batch 90/121] avg loss 0.0288515, throughput 2.331K wps\n",
      "[Epoch 134 Batch 100/121] avg loss 0.0281425, throughput 2.51889K wps\n",
      "[Epoch 134 Batch 110/121] avg loss 0.0212408, throughput 2.40963K wps\n",
      "[Epoch 134 Batch 120/121] avg loss 0.0201417, throughput 2.68098K wps\n",
      "[Epoch 134] train avg loss 0.0276026, train avg r2 0.96759,throughput 2.46486K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 135 Batch 10/121] avg loss 0.0227185, throughput 2.37529K wps\n",
      "[Epoch 135 Batch 20/121] avg loss 0.0276334, throughput 2.54453K wps\n",
      "[Epoch 135 Batch 30/121] avg loss 0.0264788, throughput 2.5974K wps\n",
      "[Epoch 135 Batch 40/121] avg loss 0.0158207, throughput 2.48139K wps\n",
      "[Epoch 135 Batch 50/121] avg loss 0.0272781, throughput 2.53807K wps\n",
      "[Epoch 135 Batch 60/121] avg loss 0.030675, throughput 2.44499K wps\n",
      "[Epoch 135 Batch 70/121] avg loss 0.0358803, throughput 2.48756K wps\n",
      "[Epoch 135 Batch 80/121] avg loss 0.0414903, throughput 2.19299K wps\n",
      "[Epoch 135 Batch 90/121] avg loss 0.0280607, throughput 2.32558K wps\n",
      "[Epoch 135 Batch 100/121] avg loss 0.0275072, throughput 2.52526K wps\n",
      "[Epoch 135 Batch 110/121] avg loss 0.0216064, throughput 2.42718K wps\n",
      "[Epoch 135 Batch 120/121] avg loss 0.0190123, throughput 2.69542K wps\n",
      "[Epoch 135] train avg loss 0.027003, train avg r2 0.968867,throughput 2.46737K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 136 Batch 10/121] avg loss 0.0215151, throughput 2.3753K wps\n",
      "[Epoch 136 Batch 20/121] avg loss 0.0284769, throughput 2.55755K wps\n",
      "[Epoch 136 Batch 30/121] avg loss 0.0247192, throughput 2.60415K wps\n",
      "[Epoch 136 Batch 40/121] avg loss 0.0156574, throughput 2.50001K wps\n",
      "[Epoch 136 Batch 50/121] avg loss 0.0292121, throughput 2.54453K wps\n",
      "[Epoch 136 Batch 60/121] avg loss 0.0297074, throughput 2.45098K wps\n",
      "[Epoch 136 Batch 70/121] avg loss 0.0348835, throughput 2.48756K wps\n",
      "[Epoch 136 Batch 80/121] avg loss 0.0407058, throughput 2.19298K wps\n",
      "[Epoch 136 Batch 90/121] avg loss 0.029179, throughput 2.33645K wps\n",
      "[Epoch 136 Batch 100/121] avg loss 0.028979, throughput 2.51889K wps\n",
      "[Epoch 136 Batch 110/121] avg loss 0.0226107, throughput 2.4213K wps\n",
      "[Epoch 136 Batch 120/121] avg loss 0.020171, throughput 2.68818K wps\n",
      "[Epoch 136] train avg loss 0.0271436, train avg r2 0.968083,throughput 2.4704K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 137 Batch 10/121] avg loss 0.0206119, throughput 2.38663K wps\n",
      "[Epoch 137 Batch 20/121] avg loss 0.0276449, throughput 2.55754K wps\n",
      "[Epoch 137 Batch 30/121] avg loss 0.0261948, throughput 2.60416K wps\n",
      "[Epoch 137 Batch 40/121] avg loss 0.0151637, throughput 2.49376K wps\n",
      "[Epoch 137 Batch 50/121] avg loss 0.0257831, throughput 2.53805K wps\n",
      "[Epoch 137 Batch 60/121] avg loss 0.0314394, throughput 2.45099K wps\n",
      "[Epoch 137 Batch 70/121] avg loss 0.0369666, throughput 2.48756K wps\n",
      "[Epoch 137 Batch 80/121] avg loss 0.0421908, throughput 2.19298K wps\n",
      "[Epoch 137 Batch 90/121] avg loss 0.0286832, throughput 2.33645K wps\n",
      "[Epoch 137 Batch 100/121] avg loss 0.0284946, throughput 2.52526K wps\n",
      "[Epoch 137 Batch 110/121] avg loss 0.0207562, throughput 2.42717K wps\n",
      "[Epoch 137 Batch 120/121] avg loss 0.0199208, throughput 2.68098K wps\n",
      "[Epoch 137] train avg loss 0.0269819, train avg r2 0.968385,throughput 2.46989K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 138 Batch 10/121] avg loss 0.021764, throughput 2.38663K wps\n",
      "[Epoch 138 Batch 20/121] avg loss 0.0275296, throughput 2.53165K wps\n",
      "[Epoch 138 Batch 30/121] avg loss 0.0255054, throughput 2.60417K wps\n",
      "[Epoch 138 Batch 40/121] avg loss 0.0163514, throughput 2.5K wps\n",
      "[Epoch 138 Batch 50/121] avg loss 0.0301498, throughput 2.55753K wps\n",
      "[Epoch 138 Batch 60/121] avg loss 0.0300045, throughput 2.45098K wps\n",
      "[Epoch 138 Batch 70/121] avg loss 0.0340189, throughput 2.48138K wps\n",
      "[Epoch 138 Batch 80/121] avg loss 0.0425535, throughput 2.19299K wps\n",
      "[Epoch 138 Batch 90/121] avg loss 0.0288471, throughput 2.331K wps\n",
      "[Epoch 138 Batch 100/121] avg loss 0.0280549, throughput 2.51889K wps\n",
      "[Epoch 138 Batch 110/121] avg loss 0.0207698, throughput 2.42131K wps\n",
      "[Epoch 138 Batch 120/121] avg loss 0.0206784, throughput 2.67381K wps\n",
      "[Epoch 138] train avg loss 0.0271857, train avg r2 0.968006,throughput 2.46838K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 139 Batch 10/121] avg loss 0.0207658, throughput 2.38664K wps\n",
      "[Epoch 139 Batch 20/121] avg loss 0.0285697, throughput 2.55754K wps\n",
      "[Epoch 139 Batch 30/121] avg loss 0.0260637, throughput 2.60416K wps\n",
      "[Epoch 139 Batch 40/121] avg loss 0.0158524, throughput 2.49377K wps\n",
      "[Epoch 139 Batch 50/121] avg loss 0.0273301, throughput 2.52526K wps\n",
      "[Epoch 139 Batch 60/121] avg loss 0.0302845, throughput 2.45097K wps\n",
      "[Epoch 139 Batch 70/121] avg loss 0.0350627, throughput 2.46914K wps\n",
      "[Epoch 139 Batch 80/121] avg loss 0.0411112, throughput 2.19298K wps\n",
      "[Epoch 139 Batch 90/121] avg loss 0.0295625, throughput 2.32558K wps\n",
      "[Epoch 139 Batch 100/121] avg loss 0.0274944, throughput 2.53165K wps\n",
      "[Epoch 139 Batch 110/121] avg loss 0.0226968, throughput 2.4213K wps\n",
      "[Epoch 139 Batch 120/121] avg loss 0.0222558, throughput 2.68097K wps\n",
      "[Epoch 139] train avg loss 0.0272411, train avg r2 0.968071,throughput 2.46838K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 140 Batch 10/121] avg loss 0.0214894, throughput 2.38095K wps\n",
      "[Epoch 140 Batch 20/121] avg loss 0.0276705, throughput 2.53807K wps\n",
      "[Epoch 140 Batch 30/121] avg loss 0.0270927, throughput 2.60417K wps\n",
      "[Epoch 140 Batch 40/121] avg loss 0.0160324, throughput 2.5K wps\n",
      "[Epoch 140 Batch 50/121] avg loss 0.0277509, throughput 2.54452K wps\n",
      "[Epoch 140 Batch 60/121] avg loss 0.0319473, throughput 2.45097K wps\n",
      "[Epoch 140 Batch 70/121] avg loss 0.0349882, throughput 2.48757K wps\n",
      "[Epoch 140 Batch 80/121] avg loss 0.0425147, throughput 2.19298K wps\n",
      "[Epoch 140 Batch 90/121] avg loss 0.0306999, throughput 2.33645K wps\n",
      "[Epoch 140 Batch 100/121] avg loss 0.026111, throughput 2.51889K wps\n",
      "[Epoch 140 Batch 110/121] avg loss 0.0219539, throughput 2.42131K wps\n",
      "[Epoch 140 Batch 120/121] avg loss 0.0223605, throughput 2.68097K wps\n",
      "[Epoch 140] train avg loss 0.0275415, train avg r2 0.96814,throughput 2.46939K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 141 Batch 10/121] avg loss 0.0209391, throughput 2.38095K wps\n",
      "[Epoch 141 Batch 20/121] avg loss 0.0267334, throughput 2.54453K wps\n",
      "[Epoch 141 Batch 30/121] avg loss 0.0251385, throughput 2.59739K wps\n",
      "[Epoch 141 Batch 40/121] avg loss 0.0141711, throughput 2.49999K wps\n",
      "[Epoch 141 Batch 50/121] avg loss 0.0283074, throughput 2.52526K wps\n",
      "[Epoch 141 Batch 60/121] avg loss 0.0312857, throughput 2.44499K wps\n",
      "[Epoch 141 Batch 70/121] avg loss 0.0356666, throughput 2.47525K wps\n",
      "[Epoch 141 Batch 80/121] avg loss 0.0432763, throughput 2.19298K wps\n",
      "[Epoch 141 Batch 90/121] avg loss 0.0269276, throughput 2.331K wps\n",
      "[Epoch 141 Batch 100/121] avg loss 0.0273185, throughput 2.52525K wps\n",
      "[Epoch 141 Batch 110/121] avg loss 0.0208884, throughput 2.42131K wps\n",
      "[Epoch 141 Batch 120/121] avg loss 0.0192937, throughput 2.6738K wps\n",
      "[Epoch 141] train avg loss 0.0266533, train avg r2 0.968989,throughput 2.46486K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 142 Batch 10/121] avg loss 0.0225415, throughput 2.38663K wps\n",
      "[Epoch 142 Batch 20/121] avg loss 0.027388, throughput 2.55102K wps\n",
      "[Epoch 142 Batch 30/121] avg loss 0.0258541, throughput 2.58398K wps\n",
      "[Epoch 142 Batch 40/121] avg loss 0.014821, throughput 2.49377K wps\n",
      "[Epoch 142 Batch 50/121] avg loss 0.0254629, throughput 2.53806K wps\n",
      "[Epoch 142 Batch 60/121] avg loss 0.0318709, throughput 2.44499K wps\n",
      "[Epoch 142 Batch 70/121] avg loss 0.0394406, throughput 2.48139K wps\n",
      "[Epoch 142 Batch 80/121] avg loss 0.042021, throughput 2.1978K wps\n",
      "[Epoch 142 Batch 90/121] avg loss 0.028392, throughput 2.32559K wps\n",
      "[Epoch 142 Batch 100/121] avg loss 0.0278822, throughput 2.53164K wps\n",
      "[Epoch 142 Batch 110/121] avg loss 0.0213453, throughput 2.4213K wps\n",
      "[Epoch 142 Batch 120/121] avg loss 0.0201141, throughput 2.6738K wps\n",
      "[Epoch 142] train avg loss 0.0272514, train avg r2 0.968559,throughput 2.46737K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 143 Batch 10/121] avg loss 0.0201859, throughput 2.38663K wps\n",
      "[Epoch 143 Batch 20/121] avg loss 0.0294769, throughput 2.54453K wps\n",
      "[Epoch 143 Batch 30/121] avg loss 0.0250496, throughput 2.59068K wps\n",
      "[Epoch 143 Batch 40/121] avg loss 0.0144219, throughput 2.49377K wps\n",
      "[Epoch 143 Batch 50/121] avg loss 0.0272013, throughput 2.51889K wps\n",
      "[Epoch 143 Batch 60/121] avg loss 0.0298367, throughput 2.44498K wps\n",
      "[Epoch 143 Batch 70/121] avg loss 0.0370184, throughput 2.48139K wps\n",
      "[Epoch 143 Batch 80/121] avg loss 0.0428845, throughput 2.18818K wps\n",
      "[Epoch 143 Batch 90/121] avg loss 0.0286103, throughput 2.331K wps\n",
      "[Epoch 143 Batch 100/121] avg loss 0.02733, throughput 2.52524K wps\n",
      "[Epoch 143 Batch 110/121] avg loss 0.0218396, throughput 2.4213K wps\n",
      "[Epoch 143 Batch 120/121] avg loss 0.0218459, throughput 2.68097K wps\n",
      "[Epoch 143] train avg loss 0.027134, train avg r2 0.96873,throughput 2.46536K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 144 Batch 10/121] avg loss 0.0222914, throughput 2.38096K wps\n",
      "[Epoch 144 Batch 20/121] avg loss 0.0298602, throughput 2.54452K wps\n",
      "[Epoch 144 Batch 30/121] avg loss 0.0253193, throughput 2.59067K wps\n",
      "[Epoch 144 Batch 40/121] avg loss 0.0152573, throughput 2.49376K wps\n",
      "[Epoch 144 Batch 50/121] avg loss 0.0275489, throughput 2.53808K wps\n",
      "[Epoch 144 Batch 60/121] avg loss 0.0318941, throughput 2.45097K wps\n",
      "[Epoch 144 Batch 70/121] avg loss 0.035314, throughput 2.47525K wps\n",
      "[Epoch 144 Batch 80/121] avg loss 0.0432475, throughput 2.19299K wps\n",
      "[Epoch 144 Batch 90/121] avg loss 0.0285896, throughput 2.33099K wps\n",
      "[Epoch 144 Batch 100/121] avg loss 0.0289544, throughput 2.5189K wps\n",
      "[Epoch 144 Batch 110/121] avg loss 0.0223181, throughput 2.35295K wps\n",
      "[Epoch 144 Batch 120/121] avg loss 0.0206148, throughput 2.6178K wps\n",
      "[Epoch 144] train avg loss 0.0275914, train avg r2 0.967836,throughput 2.45635K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 145 Batch 10/121] avg loss 0.0226618, throughput 2.331K wps\n",
      "[Epoch 145 Batch 20/121] avg loss 0.0266335, throughput 2.55102K wps\n",
      "[Epoch 145 Batch 30/121] avg loss 0.0252148, throughput 2.60417K wps\n",
      "[Epoch 145 Batch 40/121] avg loss 0.014775, throughput 2.49999K wps\n",
      "[Epoch 145 Batch 50/121] avg loss 0.027203, throughput 2.54453K wps\n",
      "[Epoch 145 Batch 60/121] avg loss 0.0293242, throughput 2.44498K wps\n",
      "[Epoch 145 Batch 70/121] avg loss 0.0368758, throughput 2.48139K wps\n",
      "[Epoch 145 Batch 80/121] avg loss 0.0412387, throughput 2.19298K wps\n",
      "[Epoch 145 Batch 90/121] avg loss 0.0291724, throughput 2.32559K wps\n",
      "[Epoch 145 Batch 100/121] avg loss 0.0274553, throughput 2.52525K wps\n",
      "[Epoch 145 Batch 110/121] avg loss 0.0217436, throughput 2.4213K wps\n",
      "[Epoch 145 Batch 120/121] avg loss 0.0209588, throughput 2.67379K wps\n",
      "[Epoch 145] train avg loss 0.0269329, train avg r2 0.968997,throughput 2.46335K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 146 Batch 10/121] avg loss 0.0205986, throughput 2.38096K wps\n",
      "[Epoch 146 Batch 20/121] avg loss 0.0282024, throughput 2.55101K wps\n",
      "[Epoch 146 Batch 30/121] avg loss 0.0246553, throughput 2.58398K wps\n",
      "[Epoch 146 Batch 40/121] avg loss 0.0158189, throughput 2.49377K wps\n",
      "[Epoch 146 Batch 50/121] avg loss 0.0262298, throughput 2.53807K wps\n",
      "[Epoch 146 Batch 60/121] avg loss 0.0313381, throughput 2.43903K wps\n",
      "[Epoch 146 Batch 70/121] avg loss 0.035638, throughput 2.47524K wps\n",
      "[Epoch 146 Batch 80/121] avg loss 0.0404201, throughput 2.19298K wps\n",
      "[Epoch 146 Batch 90/121] avg loss 0.0289793, throughput 2.32558K wps\n",
      "[Epoch 146 Batch 100/121] avg loss 0.0286871, throughput 2.52524K wps\n",
      "[Epoch 146 Batch 110/121] avg loss 0.0222808, throughput 2.41546K wps\n",
      "[Epoch 146 Batch 120/121] avg loss 0.0200668, throughput 2.66667K wps\n",
      "[Epoch 146] train avg loss 0.0268983, train avg r2 0.968729,throughput 2.46486K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 147 Batch 10/121] avg loss 0.0226978, throughput 2.36967K wps\n",
      "[Epoch 147 Batch 20/121] avg loss 0.0271216, throughput 2.55101K wps\n",
      "[Epoch 147 Batch 30/121] avg loss 0.0251186, throughput 2.5974K wps\n",
      "[Epoch 147 Batch 40/121] avg loss 0.0174651, throughput 2.49376K wps\n",
      "[Epoch 147 Batch 50/121] avg loss 0.0292921, throughput 2.54453K wps\n",
      "[Epoch 147 Batch 60/121] avg loss 0.0300922, throughput 2.44499K wps\n",
      "[Epoch 147 Batch 70/121] avg loss 0.0377752, throughput 2.47523K wps\n",
      "[Epoch 147 Batch 80/121] avg loss 0.0433948, throughput 2.19299K wps\n",
      "[Epoch 147 Batch 90/121] avg loss 0.0281199, throughput 2.331K wps\n",
      "[Epoch 147 Batch 100/121] avg loss 0.0282336, throughput 2.52526K wps\n",
      "[Epoch 147 Batch 110/121] avg loss 0.0223119, throughput 2.42718K wps\n",
      "[Epoch 147 Batch 120/121] avg loss 0.0208038, throughput 2.68096K wps\n",
      "[Epoch 147] train avg loss 0.0277039, train avg r2 0.967617,throughput 2.46838K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 148 Batch 10/121] avg loss 0.0241219, throughput 2.38664K wps\n",
      "[Epoch 148 Batch 20/121] avg loss 0.0281442, throughput 2.54453K wps\n",
      "[Epoch 148 Batch 30/121] avg loss 0.024808, throughput 2.5974K wps\n",
      "[Epoch 148 Batch 40/121] avg loss 0.0157899, throughput 2.48139K wps\n",
      "[Epoch 148 Batch 50/121] avg loss 0.028505, throughput 2.54453K wps\n",
      "[Epoch 148 Batch 60/121] avg loss 0.0299012, throughput 2.44497K wps\n",
      "[Epoch 148 Batch 70/121] avg loss 0.0371065, throughput 2.48139K wps\n",
      "[Epoch 148 Batch 80/121] avg loss 0.0426715, throughput 2.1978K wps\n",
      "[Epoch 148 Batch 90/121] avg loss 0.0279091, throughput 2.331K wps\n",
      "[Epoch 148 Batch 100/121] avg loss 0.0278278, throughput 2.52525K wps\n",
      "[Epoch 148 Batch 110/121] avg loss 0.0208877, throughput 2.42131K wps\n",
      "[Epoch 148 Batch 120/121] avg loss 0.0217462, throughput 2.6738K wps\n",
      "[Epoch 148] train avg loss 0.0274442, train avg r2 0.968116,throughput 2.46838K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 149 Batch 10/121] avg loss 0.0206073, throughput 2.37529K wps\n",
      "[Epoch 149 Batch 20/121] avg loss 0.028091, throughput 2.55103K wps\n",
      "[Epoch 149 Batch 30/121] avg loss 0.0253104, throughput 2.61096K wps\n",
      "[Epoch 149 Batch 40/121] avg loss 0.0174152, throughput 2.50625K wps\n",
      "[Epoch 149 Batch 50/121] avg loss 0.027764, throughput 2.54454K wps\n",
      "[Epoch 149 Batch 60/121] avg loss 0.0355862, throughput 2.45097K wps\n",
      "[Epoch 149 Batch 70/121] avg loss 0.0353976, throughput 2.48756K wps\n",
      "[Epoch 149 Batch 80/121] avg loss 0.0427076, throughput 2.19299K wps\n",
      "[Epoch 149 Batch 90/121] avg loss 0.0278046, throughput 2.33101K wps\n",
      "[Epoch 149 Batch 100/121] avg loss 0.0282105, throughput 2.52525K wps\n",
      "[Epoch 149 Batch 110/121] avg loss 0.0224041, throughput 2.40963K wps\n",
      "[Epoch 149 Batch 120/121] avg loss 0.0194398, throughput 2.6738K wps\n",
      "[Epoch 149] train avg loss 0.0275505, train avg r2 0.968312,throughput 2.46939K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 150 Batch 10/121] avg loss 0.021089, throughput 2.38095K wps\n",
      "[Epoch 150 Batch 20/121] avg loss 0.0271982, throughput 2.55101K wps\n",
      "[Epoch 150 Batch 30/121] avg loss 0.0248515, throughput 2.59068K wps\n",
      "[Epoch 150 Batch 40/121] avg loss 0.0154936, throughput 2.48139K wps\n",
      "[Epoch 150 Batch 50/121] avg loss 0.0277961, throughput 2.53807K wps\n",
      "[Epoch 150 Batch 60/121] avg loss 0.0306199, throughput 2.45699K wps\n",
      "[Epoch 150 Batch 70/121] avg loss 0.0391212, throughput 2.47525K wps\n",
      "[Epoch 150 Batch 80/121] avg loss 0.0412076, throughput 2.1978K wps\n",
      "[Epoch 150 Batch 90/121] avg loss 0.0293654, throughput 2.32559K wps\n",
      "[Epoch 150 Batch 100/121] avg loss 0.029005, throughput 2.51889K wps\n",
      "[Epoch 150 Batch 110/121] avg loss 0.0235058, throughput 2.41546K wps\n",
      "[Epoch 150 Batch 120/121] avg loss 0.020976, throughput 2.67378K wps\n",
      "[Epoch 150] train avg loss 0.0275122, train avg r2 0.967789,throughput 2.46637K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 151 Batch 10/121] avg loss 0.0224213, throughput 2.38094K wps\n",
      "[Epoch 151 Batch 20/121] avg loss 0.0253034, throughput 2.54453K wps\n",
      "[Epoch 151 Batch 30/121] avg loss 0.0241278, throughput 2.60416K wps\n",
      "[Epoch 151 Batch 40/121] avg loss 0.0145291, throughput 2.5K wps\n",
      "[Epoch 151 Batch 50/121] avg loss 0.0275421, throughput 2.53807K wps\n",
      "[Epoch 151 Batch 60/121] avg loss 0.0321113, throughput 2.44499K wps\n",
      "[Epoch 151 Batch 70/121] avg loss 0.0352483, throughput 2.46913K wps\n",
      "[Epoch 151 Batch 80/121] avg loss 0.0422093, throughput 2.13675K wps\n",
      "[Epoch 151 Batch 90/121] avg loss 0.0304282, throughput 2.331K wps\n",
      "[Epoch 151 Batch 100/121] avg loss 0.0291553, throughput 2.47524K wps\n",
      "[Epoch 151 Batch 110/121] avg loss 0.0217529, throughput 2.38664K wps\n",
      "[Epoch 151 Batch 120/121] avg loss 0.0206687, throughput 2.68096K wps\n",
      "[Epoch 151] train avg loss 0.027115, train avg r2 0.968136,throughput 2.45436K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 152 Batch 10/121] avg loss 0.0204875, throughput 2.32557K wps\n",
      "[Epoch 152 Batch 20/121] avg loss 0.0274543, throughput 2.55101K wps\n",
      "[Epoch 152 Batch 30/121] avg loss 0.0238384, throughput 2.5974K wps\n",
      "[Epoch 152 Batch 40/121] avg loss 0.0168073, throughput 2.48757K wps\n",
      "[Epoch 152 Batch 50/121] avg loss 0.0267536, throughput 2.53807K wps\n",
      "[Epoch 152 Batch 60/121] avg loss 0.0299791, throughput 2.45699K wps\n",
      "[Epoch 152 Batch 70/121] avg loss 0.0375162, throughput 2.48756K wps\n",
      "[Epoch 152 Batch 80/121] avg loss 0.0420721, throughput 2.18341K wps\n",
      "[Epoch 152 Batch 90/121] avg loss 0.0276997, throughput 2.3419K wps\n",
      "[Epoch 152 Batch 100/121] avg loss 0.0267116, throughput 2.51257K wps\n",
      "[Epoch 152 Batch 110/121] avg loss 0.0215407, throughput 2.41546K wps\n",
      "[Epoch 152 Batch 120/121] avg loss 0.0211033, throughput 2.68817K wps\n",
      "[Epoch 152] train avg loss 0.0268265, train avg r2 0.968594,throughput 2.46285K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 153 Batch 10/121] avg loss 0.023367, throughput 2.38663K wps\n",
      "[Epoch 153 Batch 20/121] avg loss 0.0280398, throughput 2.53165K wps\n",
      "[Epoch 153 Batch 30/121] avg loss 0.0244468, throughput 2.5974K wps\n",
      "[Epoch 153 Batch 40/121] avg loss 0.0144307, throughput 2.49999K wps\n",
      "[Epoch 153 Batch 50/121] avg loss 0.0274111, throughput 2.53166K wps\n",
      "[Epoch 153 Batch 60/121] avg loss 0.0299984, throughput 2.44499K wps\n",
      "[Epoch 153 Batch 70/121] avg loss 0.0383441, throughput 2.48138K wps\n",
      "[Epoch 153 Batch 80/121] avg loss 0.0424674, throughput 2.17864K wps\n",
      "[Epoch 153 Batch 90/121] avg loss 0.0312324, throughput 2.33101K wps\n",
      "[Epoch 153 Batch 100/121] avg loss 0.0288004, throughput 2.5189K wps\n",
      "[Epoch 153 Batch 110/121] avg loss 0.0222929, throughput 2.42131K wps\n",
      "[Epoch 153 Batch 120/121] avg loss 0.0208511, throughput 2.67379K wps\n",
      "[Epoch 153] train avg loss 0.0276341, train avg r2 0.967926,throughput 2.46586K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 154 Batch 10/121] avg loss 0.0213646, throughput 2.38095K wps\n",
      "[Epoch 154 Batch 20/121] avg loss 0.0273399, throughput 2.54453K wps\n",
      "[Epoch 154 Batch 30/121] avg loss 0.0240921, throughput 2.60416K wps\n",
      "[Epoch 154 Batch 40/121] avg loss 0.0149482, throughput 2.5K wps\n",
      "[Epoch 154 Batch 50/121] avg loss 0.0265457, throughput 2.52525K wps\n",
      "[Epoch 154 Batch 60/121] avg loss 0.0294387, throughput 2.45097K wps\n",
      "[Epoch 154 Batch 70/121] avg loss 0.0371687, throughput 2.47524K wps\n",
      "[Epoch 154 Batch 80/121] avg loss 0.0415543, throughput 2.18819K wps\n",
      "[Epoch 154 Batch 90/121] avg loss 0.0291283, throughput 2.33645K wps\n",
      "[Epoch 154 Batch 100/121] avg loss 0.0271596, throughput 2.5189K wps\n",
      "[Epoch 154 Batch 110/121] avg loss 0.022262, throughput 2.4213K wps\n",
      "[Epoch 154 Batch 120/121] avg loss 0.0195552, throughput 2.68097K wps\n",
      "[Epoch 154] train avg loss 0.0267016, train avg r2 0.96889,throughput 2.46687K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 155 Batch 10/121] avg loss 0.0209279, throughput 2.38663K wps\n",
      "[Epoch 155 Batch 20/121] avg loss 0.0252973, throughput 2.53806K wps\n",
      "[Epoch 155 Batch 30/121] avg loss 0.0250154, throughput 2.59067K wps\n",
      "[Epoch 155 Batch 40/121] avg loss 0.0156766, throughput 2.50001K wps\n",
      "[Epoch 155 Batch 50/121] avg loss 0.028078, throughput 2.23214K wps\n",
      "[Epoch 155 Batch 60/121] avg loss 0.0331419, throughput 2.44499K wps\n",
      "[Epoch 155 Batch 70/121] avg loss 0.0326533, throughput 2.48139K wps\n",
      "[Epoch 155 Batch 80/121] avg loss 0.042176, throughput 2.19298K wps\n",
      "[Epoch 155 Batch 90/121] avg loss 0.0289341, throughput 2.331K wps\n",
      "[Epoch 155 Batch 100/121] avg loss 0.0284167, throughput 2.52526K wps\n",
      "[Epoch 155 Batch 110/121] avg loss 0.0207619, throughput 2.42718K wps\n",
      "[Epoch 155 Batch 120/121] avg loss 0.0205192, throughput 2.68816K wps\n",
      "[Epoch 155] train avg loss 0.0267894, train avg r2 0.969018,throughput 2.4405K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 156 Batch 10/121] avg loss 0.0217073, throughput 2.38664K wps\n",
      "[Epoch 156 Batch 20/121] avg loss 0.0271742, throughput 2.55102K wps\n",
      "[Epoch 156 Batch 30/121] avg loss 0.0232927, throughput 2.57732K wps\n",
      "[Epoch 156 Batch 40/121] avg loss 0.0150069, throughput 2.50627K wps\n",
      "[Epoch 156 Batch 50/121] avg loss 0.0256158, throughput 2.52525K wps\n",
      "[Epoch 156 Batch 60/121] avg loss 0.0303682, throughput 2.45699K wps\n",
      "[Epoch 156 Batch 70/121] avg loss 0.0366906, throughput 2.48139K wps\n",
      "[Epoch 156 Batch 80/121] avg loss 0.039742, throughput 2.19298K wps\n",
      "[Epoch 156 Batch 90/121] avg loss 0.0270318, throughput 2.33645K wps\n",
      "[Epoch 156 Batch 100/121] avg loss 0.0281621, throughput 2.51888K wps\n",
      "[Epoch 156 Batch 110/121] avg loss 0.0226902, throughput 2.42132K wps\n",
      "[Epoch 156 Batch 120/121] avg loss 0.0208358, throughput 2.66667K wps\n",
      "[Epoch 156] train avg loss 0.0265224, train avg r2 0.96897,throughput 2.46737K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 157 Batch 10/121] avg loss 0.0218779, throughput 2.38096K wps\n",
      "[Epoch 157 Batch 20/121] avg loss 0.0285545, throughput 2.53806K wps\n",
      "[Epoch 157 Batch 30/121] avg loss 0.0234619, throughput 2.58398K wps\n",
      "[Epoch 157 Batch 40/121] avg loss 0.0157634, throughput 2.49999K wps\n",
      "[Epoch 157 Batch 50/121] avg loss 0.0295089, throughput 2.54453K wps\n",
      "[Epoch 157 Batch 60/121] avg loss 0.0299162, throughput 2.45097K wps\n",
      "[Epoch 157 Batch 70/121] avg loss 0.0362735, throughput 2.47525K wps\n",
      "[Epoch 157 Batch 80/121] avg loss 0.0400657, throughput 2.19298K wps\n",
      "[Epoch 157 Batch 90/121] avg loss 0.0284788, throughput 2.331K wps\n",
      "[Epoch 157 Batch 100/121] avg loss 0.0272898, throughput 2.52525K wps\n",
      "[Epoch 157 Batch 110/121] avg loss 0.0223312, throughput 2.42718K wps\n",
      "[Epoch 157 Batch 120/121] avg loss 0.019268, throughput 2.68816K wps\n",
      "[Epoch 157] train avg loss 0.0268876, train avg r2 0.968746,throughput 2.46687K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 158 Batch 10/121] avg loss 0.0216358, throughput 2.38096K wps\n",
      "[Epoch 158 Batch 20/121] avg loss 0.0259989, throughput 2.55754K wps\n",
      "[Epoch 158 Batch 30/121] avg loss 0.0257479, throughput 2.60416K wps\n",
      "[Epoch 158 Batch 40/121] avg loss 0.0160159, throughput 2.5K wps\n",
      "[Epoch 158 Batch 50/121] avg loss 0.0269026, throughput 2.52525K wps\n",
      "[Epoch 158 Batch 60/121] avg loss 0.0301634, throughput 2.44498K wps\n",
      "[Epoch 158 Batch 70/121] avg loss 0.0365893, throughput 2.47525K wps\n",
      "[Epoch 158 Batch 80/121] avg loss 0.0430385, throughput 2.1978K wps\n",
      "[Epoch 158 Batch 90/121] avg loss 0.0284411, throughput 2.33099K wps\n",
      "[Epoch 158 Batch 100/121] avg loss 0.0262103, throughput 2.51889K wps\n",
      "[Epoch 158 Batch 110/121] avg loss 0.0213044, throughput 2.4213K wps\n",
      "[Epoch 158 Batch 120/121] avg loss 0.0196936, throughput 2.68816K wps\n",
      "[Epoch 158] train avg loss 0.0268038, train avg r2 0.969238,throughput 2.46737K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 159 Batch 10/121] avg loss 0.0221264, throughput 2.3753K wps\n",
      "[Epoch 159 Batch 20/121] avg loss 0.0264607, throughput 2.54453K wps\n",
      "[Epoch 159 Batch 30/121] avg loss 0.0228163, throughput 2.59739K wps\n",
      "[Epoch 159 Batch 40/121] avg loss 0.0158453, throughput 2.50626K wps\n",
      "[Epoch 159 Batch 50/121] avg loss 0.0274383, throughput 2.53165K wps\n",
      "[Epoch 159 Batch 60/121] avg loss 0.0301224, throughput 2.44497K wps\n",
      "[Epoch 159 Batch 70/121] avg loss 0.0363767, throughput 2.46915K wps\n",
      "[Epoch 159 Batch 80/121] avg loss 0.0431756, throughput 2.18818K wps\n",
      "[Epoch 159 Batch 90/121] avg loss 0.0277367, throughput 2.331K wps\n",
      "[Epoch 159 Batch 100/121] avg loss 0.0272323, throughput 2.52525K wps\n",
      "[Epoch 159 Batch 110/121] avg loss 0.0206014, throughput 2.41547K wps\n",
      "[Epoch 159 Batch 120/121] avg loss 0.0200802, throughput 2.68816K wps\n",
      "[Epoch 159] train avg loss 0.0266595, train avg r2 0.969196,throughput 2.46586K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "Total time cost 1203.52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.026659481309331035, 0.9691957021478629)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs, learning_rate = 160,0.001                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "max_len = 100\n",
    "train(net, train_dataloader, train_batch_size, learning_rate, ctx, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross valid avg train loss 0.18824321698726654\n",
    "K-fold cross valid avg train r2 0.9438172156947774\n",
    "K-fold cross valid avg test loss 0.3926431232375378\n",
    "K-fold cross valid avg test r2 0.7358792471471896\n",
    "\n",
    "K-fold cross valid avg train loss 0.21582339610382406\n",
    "K-fold cross valid avg train r2 0.9277954514182035\n",
    "K-fold cross valid avg test loss 0.2657981671905051\n",
    "K-fold cross valid avg test r2 0.8644268186992617\n",
    "\n",
    "K-fold cross valid avg train loss 0.13784621984952036\n",
    "K-fold cross valid avg train r2 0.95692521257325\n",
    "K-fold cross valid avg test loss 0.3605005411991168\n",
    "K-fold cross valid avg test r2 0.728506105563811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
