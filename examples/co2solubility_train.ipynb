{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:26.719687Z",
     "start_time": "2021-09-24T01:28:24.164690Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon,nd,autograd,npx\n",
    "import gluonnlp as nlp\n",
    "import nmt\n",
    "from gluonnlp.model.transformer import ParallelTransformer, get_transformer_encoder_decoder\n",
    "import pandas as pd \n",
    "nlp.utils.check_version('0.7.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:26.735687Z",
     "start_time": "2021-09-24T01:28:26.720688Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "random.seed(101)\n",
    "mx.random.seed(10001)\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "# parameters for dataset\n",
    "dataset = 'pubchem'\n",
    "src_lang, tgt_lang = 'random_smiles', 'rdkit_canonical_smiles'\n",
    "src_max_len, tgt_max_len = 100, 100\n",
    "\n",
    "# parameters for model\n",
    "num_units=128\n",
    "hidden_size=1024\n",
    "dropout=0\n",
    "epsilon=0.1\n",
    "num_layers=3\n",
    "num_heads=4\n",
    "scaled=True\n",
    "share_embed=True\n",
    "embed_size=128\n",
    "tie_weights=True\n",
    "embed_initializer=None\n",
    "magnitude = 3.0\n",
    "lr_update_factor = 0.5\n",
    "param_file = 'params/valid_best.params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:26.751687Z",
     "start_time": "2021-09-24T01:28:26.736690Z"
    }
   },
   "outputs": [],
   "source": [
    "def _load_vocab(file_path, **kwargs):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return nlp.Vocab.from_json(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:26.767689Z",
     "start_time": "2021-09-24T01:28:26.752689Z"
    }
   },
   "outputs": [],
   "source": [
    "src_vocab = _load_vocab('pubchem/vocab.random_smiles.json')\n",
    "tgt_vocab = _load_vocab('pubchem/vocab.rdkit_canonical_smiles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:26.798687Z",
     "start_time": "2021-09-24T01:28:26.768690Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:25:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "encoder1, decoder1, one_step_ahead_decoder1 = get_transformer_encoder_decoder(\n",
    "    units=num_units,\n",
    "    hidden_size=hidden_size,\n",
    "    dropout=dropout,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    max_src_length=src_max_len,\n",
    "    max_tgt_length=tgt_max_len,\n",
    "    scaled=scaled, prefix='transformer_1')\n",
    "\n",
    "encoder2, decoder2, one_step_ahead_decoder2 = get_transformer_encoder_decoder(\n",
    "    units=num_units,\n",
    "    hidden_size=hidden_size,\n",
    "    dropout=dropout,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    max_src_length=src_max_len,\n",
    "    max_tgt_length=tgt_max_len,\n",
    "    scaled=scaled,prefix='transformer_2')\n",
    "'''\n",
    "encoder3, decoder3, one_step_ahead_decoder3 = get_transformer_encoder_decoder(\n",
    "    units=num_units,\n",
    "    hidden_size=hidden_size,\n",
    "    dropout=dropout,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    max_src_length=src_max_len,\n",
    "    max_tgt_length=tgt_max_len,\n",
    "    scaled=scaled,prefix='transformer_3')\n",
    "'''\n",
    "model1 = nlp.model.translation.NMTModel(src_vocab=src_vocab,\n",
    "                 tgt_vocab=tgt_vocab,\n",
    "                 encoder=encoder1,\n",
    "                 decoder=decoder1,\n",
    "                 one_step_ahead_decoder=one_step_ahead_decoder1,\n",
    "                 embed_size=num_units,\n",
    "                 embed_initializer=None,\n",
    "                 prefix='transformer_1')\n",
    "model2 = nlp.model.translation.NMTModel(src_vocab=src_vocab,\n",
    "                 tgt_vocab=tgt_vocab,\n",
    "                 encoder=encoder2,\n",
    "                 decoder=decoder2,\n",
    "                 one_step_ahead_decoder=one_step_ahead_decoder2,\n",
    "                 embed_size=num_units,\n",
    "                 embed_initializer=None,\n",
    "                 prefix='transformer_2')\n",
    "'''\n",
    "model3 = nlp.model.translation.NMTModel(src_vocab=src_vocab,\n",
    "                 tgt_vocab=tgt_vocab,\n",
    "                 encoder=encoder3,\n",
    "                 decoder=decoder3,\n",
    "                 one_step_ahead_decoder=one_step_ahead_decoder3,\n",
    "                 embed_size=num_units,\n",
    "                 embed_initializer=None,\n",
    "                 prefix='transformer_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:26.814687Z",
     "start_time": "2021-09-24T01:28:26.799691Z"
    }
   },
   "outputs": [],
   "source": [
    "#model.initialize(init=mx.init.Xavier(magnitude=magnitude), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.842687Z",
     "start_time": "2021-09-24T01:28:26.815688Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:25:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU\n"
     ]
    }
   ],
   "source": [
    "#model1.load_parameters(param_file,ctx=ctx)\n",
    "#model2.load_parameters(param_file,ctx=ctx)\n",
    "model3.load_parameters(param_file,ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:49:19.490024Z",
     "start_time": "2020-09-02T08:49:19.487020Z"
    }
   },
   "source": [
    "model.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:49:19.498031Z",
     "start_time": "2020-09-02T08:49:19.491025Z"
    }
   },
   "source": [
    "def encode(model, src_seq, src_vocab,ctx):\n",
    "    src_sentence = src_vocab[src_seq.split()]\n",
    "    src_sentence.append(src_vocab[src_vocab.eos_token])\n",
    "    src_npy = np.array(src_sentence, dtype=np.int32)\n",
    "    src_nd = mx.nd.array(src_npy)\n",
    "    src_nd = src_nd.reshape((1, -1)).as_in_context(ctx)\n",
    "    src_valid_length = mx.nd.array([src_nd.shape[1]]).as_in_context(ctx)\n",
    "    enc_outputs = model.encode(src_nd,valid_length=src_valid_length)\n",
    "    return enc_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T08:49:19.523053Z",
     "start_time": "2020-09-02T08:49:19.499032Z"
    }
   },
   "source": [
    "for sentence in ['c 1 ( N = C ( N ) N ) s c c ( - c 2 c c ( C ) n ( C ) c 2 ) n 1', 'C ( C ( c 1 c c c ( C ( N O ) = O ) c c 1 ) C C ) C']:\n",
    "    e = encode(model, sentence,src_vocab,ctx)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.874687Z",
     "start_time": "2021-09-24T01:28:28.843688Z"
    }
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "def canonical_smile(sml):\n",
    "    try:\n",
    "        m = Chem.MolFromSmiles(sml)\n",
    "        #return Chem.MolToSmiles(m, canonical=True,isomericSmiles=False)\n",
    "        return Chem.MolToSmiles(m, canonical=True,isomericSmiles=True)\n",
    "    except:\n",
    "        print(sml)\n",
    "        return float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.890687Z",
     "start_time": "2021-09-24T01:28:28.875688Z"
    }
   },
   "outputs": [],
   "source": [
    "def no_split(sm):\n",
    "    arr = []\n",
    "    i = 0\n",
    "    try:\n",
    "        len(sm)\n",
    "    except:\n",
    "        print(sm)\n",
    "    while i < len(sm)-1:\n",
    "        arr.append(sm[i])\n",
    "        i += 1\n",
    "    if i == len(sm)-1:\n",
    "        arr.append(sm[i])\n",
    "    return ' '.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.906687Z",
     "start_time": "2021-09-24T01:28:28.891688Z"
    }
   },
   "outputs": [],
   "source": [
    "length_clip = nlp.data.ClipSequence(100)\n",
    "# Helper function to preprocess a single data point\n",
    "def preprocess(data):\n",
    "    # A token index or a list of token indices is\n",
    "    # returned according to the vocabulary.\n",
    "    src_sentence = src_vocab[length_clip(data.split())]\n",
    "    src_sentence.append(src_vocab[src_vocab.eos_token])\n",
    "    src_npy = np.array(src_sentence, dtype=np.int32)\n",
    "    src_nd = mx.nd.array(src_npy)\n",
    "    return src_nd\n",
    "\n",
    "# Helper function for getting the length\n",
    "def get_length(x):\n",
    "    return float(len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.922687Z",
     "start_time": "2021-09-24T01:28:28.907688Z"
    }
   },
   "outputs": [],
   "source": [
    "dropout = 0\n",
    "train_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.938687Z",
     "start_time": "2021-09-24T01:28:28.923688Z"
    }
   },
   "outputs": [],
   "source": [
    "class ILNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self,\n",
    "                 dropout,\n",
    "                 src_vocab=src_vocab,\n",
    "                 embed_size=embed_size,\n",
    "                 output_size=1,\n",
    "                 num_filters=(100, 200, 200, 200, 200, 100,100),\n",
    "                 ngram_filter_sizes=(1, 2, 3, 4, 5, 6,7),\n",
    "                 IL_num_filters=(100, 200, 200, 200, 200, 100, 100, 100, 100,100, 160),\n",
    "                 IL_ngram_filter_sizes=(1, 2, 3,4, 5, 6, 7, 8, 9, 10, 15),\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(ILNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            \n",
    "            self.num_filters = num_filters\n",
    "            self.IL_num_filters = IL_num_filters\n",
    "            '''\n",
    "            self.cation_src_embed = None\n",
    "            self.cation_encoder = None\n",
    "            self.cation_textcnn = nlp.model.ConvolutionalEncoder(\n",
    "                embed_size=embed_size,\n",
    "                num_filters=num_filters,\n",
    "                ngram_filter_sizes=ngram_filter_sizes,\n",
    "                conv_layer_activation='relu',\n",
    "                num_highway=1)\n",
    "            self.cation_dropout = gluon.nn.Dropout(dropout)\n",
    "\n",
    "            self.anion_src_embed = None\n",
    "            self.anion_encoder = None\n",
    "            self.anion_textcnn = nlp.model.ConvolutionalEncoder(\n",
    "                embed_size=embed_size,\n",
    "                num_filters=num_filters,\n",
    "                ngram_filter_sizes=ngram_filter_sizes,\n",
    "                conv_layer_activation='relu',\n",
    "                num_highway=1)\n",
    "            self.anion_dropuot = gluon.nn.Dropout(dropout)\n",
    "            '''\n",
    "            self.IL_src_embed = None\n",
    "            self.IL_encoder = None\n",
    "            self.IL_textcnn = nlp.model.ConvolutionalEncoder(\n",
    "                embed_size=embed_size,\n",
    "                num_filters=IL_num_filters,\n",
    "                ngram_filter_sizes=IL_ngram_filter_sizes,\n",
    "                conv_layer_activation='relu',\n",
    "                num_highway=1)\n",
    "            #self.IL_batchnorm = gluon.nn.BatchNorm()\n",
    "            '''\n",
    "            self.mlp = gluon.nn.HybridSequential()\n",
    "            with self.mlp.name_scope():\n",
    "                #self.mlp.add(gluon.nn.Dropout(dropout))\n",
    "                self.mlp.add(gluon.nn.Dense(1024))\n",
    "                #self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                #self.mlp.add(gluon.nn.Dropout(dropout))\n",
    "                \n",
    "                self.mlp.add(gluon.nn.Dense(2048))\n",
    "                self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                \n",
    "                self.mlp.add(gluon.nn.Dense(1024))\n",
    "                self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                \n",
    "                self.mlp.add(gluon.nn.Dense(512))\n",
    "                self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                self.mlp.add(gluon.nn.Dense(256))\n",
    "                self.mlp.add(gluon.nn.BatchNorm())\n",
    "                self.mlp.add(gluon.nn.Activation('relu'))\n",
    "                '''\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                #self.output.add(gluon.nn.Dropout(dropout))\n",
    "                #self.output.add(gluon.nn.Dense(2048))\n",
    "                #self.output.add(gluon.nn.Activation('relu'))\n",
    "                #self.output.add(gluon.nn.Dropout(dropout))\n",
    "                self.output.add(gluon.nn.Dense(1024))\n",
    "                self.output.add(gluon.nn.Activation('relu'))\n",
    "                self.output.add(gluon.nn.Dropout(dropout))\n",
    "                #self.output.add(gluon.nn.Dropout(dropout))\n",
    "                self.output.add(gluon.nn.Dense(512))\n",
    "                self.output.add(gluon.nn.Activation('relu'))\n",
    "                #self.output.add(gluon.nn.Dropout(dropout))\n",
    "                #self.output.add(gluon.nn.Dense(256))\n",
    "                #self.output.add(gluon.nn.Activation('relu'))\n",
    "                #self.output.add(gluon.nn.Dropout(dropout))\n",
    "                \n",
    "                \n",
    "                self.output.add(gluon.nn.Dense(output_size, flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F, IL_src_nd, IL_valid_length, T, P):  # pylint: disable=arguments-differ\n",
    "        '''\n",
    "        cation_src_embed_ = self.cation_src_embed(cation_src_nd)\n",
    "        cation_encoded, _ = self.cation_encoder(\n",
    "            cation_src_embed_,\n",
    "            valid_length=cation_valid_length)  # Shape(T, N, C)\n",
    "        cation_textcnn = self.cation_textcnn(\n",
    "            F.transpose(cation_encoded, axes=(1, 0, 2)))\n",
    "        cation_textcnn = self.cation_dropout(cation_textcnn)\n",
    "\n",
    "        anion_src_embed_ = self.anion_src_embed(anion_src_nd)\n",
    "        anion_encoded, _ = self.anion_encoder(\n",
    "            anion_src_embed_,\n",
    "            valid_length=anion_valid_length)  # Shape(T, N, C)\n",
    "        anion_textcnn = self.anion_textcnn(\n",
    "            F.transpose(anion_encoded, axes=(1, 0, 2)))\n",
    "        anion_textcnn = self.anion_dropuot(anion_textcnn)\n",
    "        '''\n",
    "        IL_src_embed_ = self.IL_src_embed(IL_src_nd)\n",
    "        IL_encoded, _ = self.IL_encoder(\n",
    "            IL_src_embed_,\n",
    "            valid_length=IL_valid_length)  # Shape(T, N, C)\n",
    "        IL_textcnn = self.IL_textcnn(\n",
    "            F.transpose(IL_encoded, axes=(1, 0, 2)))\n",
    "        #IL_textcnn = self.IL_batchnorm(IL_textcnn)\n",
    "        \n",
    "        T_ = F.reshape(T, shape=(-1, 1))\n",
    "        P_ = F.reshape(P, shape=(-1, 1))\n",
    "        \n",
    "        input_vecs = mx.symbol.concat(\n",
    "            F.reshape(IL_textcnn,\n",
    "                      shape=(-1, sum(self.IL_num_filters))),T_, P_)\n",
    "        \n",
    "        #mlp_out = self.mlp(IL_textcnn)\n",
    "\n",
    "        \n",
    "\n",
    "        #add_temp_press = mx.symbol.concat(IL_textcnn, T_, P_)\n",
    "        out = self.output(input_vecs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T08:19:36.073306Z",
     "start_time": "2021-03-19T08:19:36.058309Z"
    }
   },
   "source": [
    "def get_residual_block(prefix='res_block_', hidden=64):\n",
    "    block = gluon.nn.HybridSequential(prefix=prefix)\n",
    "    with block.name_scope():\n",
    "        block.add(gluon.nn.Dense(hidden, activation='relu', prefix='d1_'),\n",
    "                  gluon.nn.Dropout(0, prefix='dropout_'),\n",
    "                  gluon.nn.Dense(hidden, prefix='d2_'))\n",
    "    return block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T08:19:36.088307Z",
     "start_time": "2021-03-19T08:19:36.074309Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "class ILNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self,\n",
    "                 dropout,\n",
    "                 src_vocab=src_vocab,\n",
    "                 embed_size=embed_size,\n",
    "                 output_size=1,\n",
    "                 IL_num_filters=(100, 200, 200, 200, 200, 100, 100, 100, 100,100, 160, 160),\n",
    "                 IL_ngram_filter_sizes=(1, 2, 3,4, 5, 6, 7, 8, 9, 10, 15, 20),\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(ILNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            \n",
    "            self.IL_num_filters = IL_num_filters\n",
    "\n",
    "            self.IL_src_embed = None\n",
    "            self.IL_encoder = None\n",
    "            self.IL_textcnn = nlp.model.ConvolutionalEncoder(\n",
    "                embed_size=embed_size,\n",
    "                num_filters=IL_num_filters,\n",
    "                ngram_filter_sizes=IL_ngram_filter_sizes,\n",
    "                conv_layer_activation='relu',\n",
    "                num_highway=1)\n",
    "            #self.IL_batchnorm = gluon.nn.BatchNorm()\n",
    "            \n",
    "            self.IL_block1 = get_residual_block('IL_block1_',sum(self.IL_num_filters)+2)\n",
    "            self.IL_dropout = gluon.nn.Dropout(0)\n",
    "            self.IL_block2 = get_residual_block('IL_block2_', sum(self.IL_num_filters)+2)\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                #self.output.add(gluon.nn.Dense(512))\n",
    "                #self.output.add(gluon.nn.Activation('relu'))\n",
    "                #self.output.add(gluon.nn.Dropout(dropout))\n",
    "                       \n",
    "                self.output.add(gluon.nn.Dense(output_size, flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F, IL_src_nd, IL_valid_length, T, P):  # pylint: disable=arguments-differ\n",
    "\n",
    "        IL_src_embed_ = self.IL_src_embed(IL_src_nd)\n",
    "        IL_encoded, _ = self.IL_encoder(\n",
    "            IL_src_embed_,\n",
    "            valid_length=IL_valid_length)  # Shape(T, N, C)\n",
    "        IL_textcnn = self.IL_textcnn(\n",
    "            F.transpose(IL_encoded, axes=(1, 0, 2)))\n",
    "        #IL_textcnn = self.IL_batchnorm(IL_textcnn)\n",
    "        \n",
    "        T_ = F.reshape(T, shape=(-1, 1))\n",
    "        P_ = F.reshape(P, shape=(-1, 1))\n",
    "        \n",
    "        input_vecs = mx.symbol.concat(\n",
    "            F.reshape(IL_textcnn,\n",
    "                      shape=(-1, sum(self.IL_num_filters))),T_, P_)\n",
    "        \n",
    "        il_block1 = self.IL_block1(input_vecs)\n",
    "        il1 = (input_vecs + il_block1).relu()\n",
    "\n",
    "        il2 = self.IL_dropout(il1)\n",
    "        il_block2 = self.IL_block2(il2)\n",
    "        il_transformed = (il2 + il_block2).relu()\n",
    "        \n",
    "        out = self.output(il_transformed)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:28.954687Z",
     "start_time": "2021-09-24T01:28:28.939689Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILNet(\n",
      "  (IL_src_embed): HybridSequential(\n",
      "    (0): Embedding(72 -> 128, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (IL_encoder): TransformerEncoder(\n",
      "    (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): TransformerEncoderCell(\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(128 -> 128, linear)\n",
      "          (proj_key): Dense(128 -> 128, linear)\n",
      "          (proj_value): Dense(128 -> 128, linear)\n",
      "        )\n",
      "        (proj): Dense(128 -> 128, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(128 -> 1024, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(1024 -> 128, linear)\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "      )\n",
      "      (1): TransformerEncoderCell(\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(128 -> 128, linear)\n",
      "          (proj_key): Dense(128 -> 128, linear)\n",
      "          (proj_value): Dense(128 -> 128, linear)\n",
      "        )\n",
      "        (proj): Dense(128 -> 128, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(128 -> 1024, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(1024 -> 128, linear)\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "      )\n",
      "      (2): TransformerEncoderCell(\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(128 -> 128, linear)\n",
      "          (proj_key): Dense(128 -> 128, linear)\n",
      "          (proj_value): Dense(128 -> 128, linear)\n",
      "        )\n",
      "        (proj): Dense(128 -> 128, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(128 -> 1024, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(1024 -> 128, linear)\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=128)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (IL_textcnn): ConvolutionalEncoder(\n",
      "    (_convs): HybridConcurrent(\n",
      "      (0): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(1,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (1): HybridSequential(\n",
      "        (0): Conv1D(128 -> 200, kernel_size=(2,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (2): HybridSequential(\n",
      "        (0): Conv1D(128 -> 200, kernel_size=(3,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (3): HybridSequential(\n",
      "        (0): Conv1D(128 -> 200, kernel_size=(4,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (4): HybridSequential(\n",
      "        (0): Conv1D(128 -> 200, kernel_size=(5,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (5): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(6,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (6): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(7,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (7): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(8,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (8): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(9,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (9): HybridSequential(\n",
      "        (0): Conv1D(128 -> 100, kernel_size=(10,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "      (10): HybridSequential(\n",
      "        (0): Conv1D(128 -> 160, kernel_size=(15,), stride=(1,))\n",
      "        (1): HybridLambda(<lambda>)\n",
      "        (2): Activation(relu)\n",
      "      )\n",
      "    )\n",
      "    (_highways): Highway(\n",
      "      (hnet): HybridSequential(\n",
      "        (0): Dense(1560 -> 3120, linear)\n",
      "      )\n",
      "      (_activation): Activation(relu)\n",
      "    )\n",
      "  )\n",
      "  (output): HybridSequential(\n",
      "    (0): Dense(None -> 1024, linear)\n",
      "    (1): Activation(relu)\n",
      "    (2): Dropout(p = 0, axes=())\n",
      "    (3): Dense(None -> 512, linear)\n",
      "    (4): Activation(relu)\n",
      "    (5): Dense(None -> 1, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ILNet(dropout=dropout)\n",
    "#net.cation_encoder = model1.encoder\n",
    "#net.cation_src_embed =  model1.src_embed\n",
    "\n",
    "#net.anion_encoder = model2.encoder\n",
    "#net.anion_src_embed =  model2.src_embed\n",
    "\n",
    "net.IL_encoder = model3.encoder\n",
    "net.IL_src_embed =  model3.src_embed\n",
    "net.hybridize()\n",
    "print(net)\n",
    "#net.textcnn.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "#net.output.initialize(mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:29.158687Z",
     "start_time": "2021-09-24T01:28:29.128690Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def get_r2(label, pred, multioutput='uniform_average'):\n",
    "    label = label.asnumpy()\n",
    "    pred = pred.asnumpy()\n",
    "    r2 = metrics.r2_score(label,pred,multioutput=multioutput)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:29.190687Z",
     "start_time": "2021-09-24T01:28:29.175688Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dir = 'co2'\n",
    "\n",
    "def train(net, train_data, batch_size, learning_rate, context, epochs,log_interval=10, dev_data=None, fold=None ):\n",
    "    start_pipeline_time = time.time()\n",
    "    #net.cation_textcnn.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    #net.anion_textcnn.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    net.IL_textcnn.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    #net.IL_block1.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    #net.IL_block2.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    #net.IL_batchnorm.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    #net.mlp.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    net.output.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n",
    "    num_epoch_lr = 10\n",
    "    factor = 0.5\n",
    "    schedule = mx.lr_scheduler.FactorScheduler(base_lr = learning_rate, step=len(train_data)* num_epoch_lr,factor=factor)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'lr_scheduler': schedule})\n",
    "    #trainer = gluon.Trainer(net.collect_params(), 'adam',{'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.L1Loss()\n",
    "    # Training/Testing.\n",
    "    # Training/Testing.\n",
    "    best_epoch_L = 10000\n",
    "    for epoch in range(epochs):\n",
    "        # Epoch training stats.\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_r2 = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        r2_num = 0\n",
    "        epoch_wc = 0\n",
    "        # Log interval training stats.\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "        for i, ((IL_data, IL_length), T,P,label) in enumerate(train_data):\n",
    "            #cation_data = cation_data.as_in_context(context)\n",
    "            #cation_length = cation_length.as_in_context(context).astype(np.float32)\n",
    "            #anion_data = anion_data.as_in_context(context)\n",
    "            #anion_length = anion_length.as_in_context(context).astype(np.float32)\n",
    "            IL_data = IL_data.as_in_context(context)\n",
    "            IL_length = IL_length.as_in_context(context).astype(np.float32)\n",
    "            T = T.as_in_context(context)\n",
    "            P = P.as_in_context(context)\n",
    "            label = label.as_in_context(context)\n",
    "            wc = max_len\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += label.shape[0]\n",
    "            epoch_sent_num += label.shape[0]\n",
    "            with autograd.record():\n",
    "                output = net(IL_data, IL_length,T,P)\n",
    "                L = loss(output, label).sum()\n",
    "                r2 = get_r2(output,label)\n",
    "            L.backward()\n",
    "            # Update parameter.\n",
    "            trainer.step(batch_size)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            epoch_r2+=r2\n",
    "            r2_num+=1\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print('[Epoch %d Batch %d/%d] avg loss %g, throughput %gK wps' % (\n",
    "                    epoch, i + 1, len(train_data),\n",
    "                    log_interval_L / log_interval_sent_num,\n",
    "                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n",
    "                # Clear log interval training stats.\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "        \n",
    "        if  (epoch_L/ epoch_sent_num) < best_epoch_L:\n",
    "            best_epoch_L = epoch_L\n",
    "            save_path = os.path.join(save_dir, 'co2_best.params')\n",
    "            net.save_parameters(save_path)\n",
    "         \n",
    "        \n",
    "        print('[Epoch %d] train avg loss %g, train avg r2 %g,'\n",
    "              'throughput %gK wps' % (\n",
    "                  epoch, epoch_L / epoch_sent_num, epoch_r2 / r2_num,\n",
    "                  epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))\n",
    "        print('learning rate:',trainer.learning_rate)\n",
    "        '''\n",
    "        if epoch + 1 >= (epochs * 2) // 3:\n",
    "            new_lr = trainer.learning_rate * lr_update_factor\n",
    "            trainer.set_learning_rate(new_lr)\n",
    "        '''\n",
    "    print('Total time cost %.2fs'%(time.time()-start_pipeline_time))\n",
    "    return epoch_L / epoch_sent_num, epoch_r2 / r2_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:29.205687Z",
     "start_time": "2021-09-24T01:28:29.191690Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloader(train_dataset):\n",
    "\n",
    "    # Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, pad_val=0, ret_length=True),\n",
    "        nlp.data.batchify.Stack(dtype='float32'),nlp.data.batchify.Stack(dtype='float32'),nlp.data.batchify.Stack(dtype='float32'))\n",
    "\n",
    "    \n",
    "    # Construct a DataLoader object for both the training and test data\n",
    "    train_dataloader = gluon.data.DataLoader(dataset=train_dataset,\n",
    "                                             batchify_fn=batchify_fn,batch_size = train_batch_size)\n",
    "\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T01:28:32.331699Z",
     "start_time": "2021-09-24T01:28:30.834687Z"
    }
   },
   "outputs": [],
   "source": [
    "co2_database = pd.read_excel('co2solubility_S.xlsx',sheet_name='co2solubility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_IL_smiles = co2_database['IL SMILES'].map(canonical_smile).map(no_split).map(preprocess)\n",
    "train_T =co2_database['normalized_T']\n",
    "train_P = co2_database['normalized_P']\n",
    "train_lngamma = co2_database['x_CO2']\n",
    "train_dataset = gluon.data.SimpleDataset(gluon.data.ArrayDataset(train_IL_smiles,train_T,train_P,train_lngamma))\n",
    "train_dataloader = get_dataloader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T02:48:36.705010Z",
     "start_time": "2021-09-24T01:28:32.333699Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:25:10] ../src/operator/cudnn_ops.cc:292: Auto-tuning cuDNN op, set MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable\n",
      "[19:25:13] ../src/operator/cudnn_ops.cc:292: Auto-tuning cuDNN op, set MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 10/159] avg loss 0.389161, throughput 0.0231707K wps\n",
      "[Epoch 0 Batch 20/159] avg loss 0.270941, throughput 0.0359712K wps\n",
      "[Epoch 0 Batch 30/159] avg loss 0.186116, throughput 0.28335K wps\n",
      "[Epoch 0 Batch 40/159] avg loss 0.161043, throughput 0.101478K wps\n",
      "[Epoch 0 Batch 50/159] avg loss 0.214668, throughput 4.34953K wps\n",
      "[Epoch 0 Batch 60/159] avg loss 0.182208, throughput 0.351153K wps\n",
      "[Epoch 0 Batch 70/159] avg loss 0.16249, throughput 0.331789K wps\n",
      "[Epoch 0 Batch 80/159] avg loss 0.126245, throughput 0.0821547K wps\n",
      "[Epoch 0 Batch 90/159] avg loss 0.0826961, throughput 0.148672K wps\n",
      "[Epoch 0 Batch 100/159] avg loss 0.107482, throughput 0.156152K wps\n",
      "[Epoch 0 Batch 110/159] avg loss 0.110348, throughput 0.431307K wps\n",
      "[Epoch 0 Batch 120/159] avg loss 0.128347, throughput 5.35326K wps\n",
      "[Epoch 0 Batch 130/159] avg loss 0.0915517, throughput 0.383933K wps\n",
      "[Epoch 0 Batch 140/159] avg loss 0.136331, throughput 0.294566K wps\n",
      "[Epoch 0 Batch 150/159] avg loss 0.107551, throughput 0.379067K wps\n",
      "[Epoch 0] train avg loss 0.161971, train avg r2 -128.051,throughput 0.120973K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 1 Batch 10/159] avg loss 0.271005, throughput 3.04021K wps\n",
      "[Epoch 1 Batch 20/159] avg loss 0.136207, throughput 3.84148K wps\n",
      "[Epoch 1 Batch 30/159] avg loss 0.137792, throughput 4.20334K wps\n",
      "[Epoch 1 Batch 40/159] avg loss 0.116423, throughput 4.21577K wps\n",
      "[Epoch 1 Batch 50/159] avg loss 0.146796, throughput 4.30785K wps\n",
      "[Epoch 1 Batch 60/159] avg loss 0.0898731, throughput 4.17965K wps\n",
      "[Epoch 1 Batch 70/159] avg loss 0.069926, throughput 4.36866K wps\n",
      "[Epoch 1 Batch 80/159] avg loss 0.0600036, throughput 4.54534K wps\n",
      "[Epoch 1 Batch 90/159] avg loss 0.0456639, throughput 5.0083K wps\n",
      "[Epoch 1 Batch 100/159] avg loss 0.0411661, throughput 5.01818K wps\n",
      "[Epoch 1 Batch 110/159] avg loss 0.0487487, throughput 5.35102K wps\n",
      "[Epoch 1 Batch 120/159] avg loss 0.0635184, throughput 5.28229K wps\n",
      "[Epoch 1 Batch 130/159] avg loss 0.0554969, throughput 5.38046K wps\n",
      "[Epoch 1 Batch 140/159] avg loss 0.0520811, throughput 5.45802K wps\n",
      "[Epoch 1 Batch 150/159] avg loss 0.0441207, throughput 5.31891K wps\n",
      "[Epoch 1] train avg loss 0.0919119, train avg r2 -76.2755,throughput 4.56886K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 2 Batch 10/159] avg loss 0.275797, throughput 3.69047K wps\n",
      "[Epoch 2 Batch 20/159] avg loss 0.121288, throughput 4.17803K wps\n",
      "[Epoch 2 Batch 30/159] avg loss 0.0992671, throughput 4.26615K wps\n",
      "[Epoch 2 Batch 40/159] avg loss 0.0821455, throughput 4.32009K wps\n",
      "[Epoch 2 Batch 50/159] avg loss 0.104877, throughput 4.27904K wps\n",
      "[Epoch 2 Batch 60/159] avg loss 0.0734228, throughput 4.29846K wps\n",
      "[Epoch 2 Batch 70/159] avg loss 0.0667391, throughput 4.46621K wps\n",
      "[Epoch 2 Batch 80/159] avg loss 0.0614899, throughput 4.51446K wps\n",
      "[Epoch 2 Batch 90/159] avg loss 0.0466909, throughput 4.85398K wps\n",
      "[Epoch 2 Batch 100/159] avg loss 0.0476543, throughput 4.74626K wps\n",
      "[Epoch 2 Batch 110/159] avg loss 0.0717304, throughput 5.46085K wps\n",
      "[Epoch 2 Batch 120/159] avg loss 0.0678071, throughput 5.36239K wps\n",
      "[Epoch 2 Batch 130/159] avg loss 0.0552726, throughput 5.49274K wps\n",
      "[Epoch 2 Batch 140/159] avg loss 0.0512609, throughput 5.4378K wps\n",
      "[Epoch 2 Batch 150/159] avg loss 0.0414112, throughput 5.29911K wps\n",
      "[Epoch 2] train avg loss 0.0848997, train avg r2 -25.7678,throughput 4.68848K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 3 Batch 10/159] avg loss 0.253206, throughput 3.78144K wps\n",
      "[Epoch 3 Batch 20/159] avg loss 0.109655, throughput 4.20632K wps\n",
      "[Epoch 3 Batch 30/159] avg loss 0.0912276, throughput 4.28794K wps\n",
      "[Epoch 3 Batch 40/159] avg loss 0.0633833, throughput 4.37163K wps\n",
      "[Epoch 3 Batch 50/159] avg loss 0.0644595, throughput 4.4391K wps\n",
      "[Epoch 3 Batch 60/159] avg loss 0.0729004, throughput 4.46448K wps\n",
      "[Epoch 3 Batch 70/159] avg loss 0.0460544, throughput 4.4354K wps\n",
      "[Epoch 3 Batch 80/159] avg loss 0.0498312, throughput 4.33307K wps\n",
      "[Epoch 3 Batch 90/159] avg loss 0.0396297, throughput 4.35018K wps\n",
      "[Epoch 3 Batch 100/159] avg loss 0.039144, throughput 4.55143K wps\n",
      "[Epoch 3 Batch 110/159] avg loss 0.0615371, throughput 5.1621K wps\n",
      "[Epoch 3 Batch 120/159] avg loss 0.0628267, throughput 5.32461K wps\n",
      "[Epoch 3 Batch 130/159] avg loss 0.0526307, throughput 5.18372K wps\n",
      "[Epoch 3 Batch 140/159] avg loss 0.0512703, throughput 5.33156K wps\n",
      "[Epoch 3 Batch 150/159] avg loss 0.0389279, throughput 5.40315K wps\n",
      "[Epoch 3] train avg loss 0.0740312, train avg r2 -13.1423,throughput 4.6466K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 4 Batch 10/159] avg loss 0.259924, throughput 3.75498K wps\n",
      "[Epoch 4 Batch 20/159] avg loss 0.11647, throughput 4.19822K wps\n",
      "[Epoch 4 Batch 30/159] avg loss 0.0963334, throughput 4.16958K wps\n",
      "[Epoch 4 Batch 40/159] avg loss 0.0634062, throughput 4.26504K wps\n",
      "[Epoch 4 Batch 50/159] avg loss 0.0440399, throughput 4.25035K wps\n",
      "[Epoch 4 Batch 60/159] avg loss 0.0494105, throughput 4.25571K wps\n",
      "[Epoch 4 Batch 70/159] avg loss 0.0286641, throughput 4.42342K wps\n",
      "[Epoch 4 Batch 80/159] avg loss 0.0462599, throughput 4.59298K wps\n",
      "[Epoch 4 Batch 90/159] avg loss 0.0391296, throughput 4.93589K wps\n",
      "[Epoch 4 Batch 100/159] avg loss 0.0376797, throughput 5.04797K wps\n",
      "[Epoch 4 Batch 110/159] avg loss 0.0553488, throughput 4.98395K wps\n",
      "[Epoch 4 Batch 120/159] avg loss 0.0599769, throughput 5.06291K wps\n",
      "[Epoch 4 Batch 130/159] avg loss 0.049638, throughput 5.14471K wps\n",
      "[Epoch 4 Batch 140/159] avg loss 0.0505018, throughput 5.23827K wps\n",
      "[Epoch 4 Batch 150/159] avg loss 0.0369756, throughput 5.50306K wps\n",
      "[Epoch 4] train avg loss 0.0700001, train avg r2 -14.6594,throughput 4.65541K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 5 Batch 10/159] avg loss 0.260755, throughput 3.66293K wps\n",
      "[Epoch 5 Batch 20/159] avg loss 0.14032, throughput 4.14356K wps\n",
      "[Epoch 5 Batch 30/159] avg loss 0.0618072, throughput 4.19378K wps\n",
      "[Epoch 5 Batch 40/159] avg loss 0.0655366, throughput 4.28108K wps\n",
      "[Epoch 5 Batch 50/159] avg loss 0.046775, throughput 4.08088K wps\n",
      "[Epoch 5 Batch 60/159] avg loss 0.0615761, throughput 4.08741K wps\n",
      "[Epoch 5 Batch 70/159] avg loss 0.0294873, throughput 4.20009K wps\n",
      "[Epoch 5 Batch 80/159] avg loss 0.0416741, throughput 4.41625K wps\n",
      "[Epoch 5 Batch 90/159] avg loss 0.0407844, throughput 4.70929K wps\n",
      "[Epoch 5 Batch 100/159] avg loss 0.0390219, throughput 4.73645K wps\n",
      "[Epoch 5 Batch 110/159] avg loss 0.0589147, throughput 4.97114K wps\n",
      "[Epoch 5 Batch 120/159] avg loss 0.0626867, throughput 4.97826K wps\n",
      "[Epoch 5 Batch 130/159] avg loss 0.0473478, throughput 5.08016K wps\n",
      "[Epoch 5 Batch 140/159] avg loss 0.0504662, throughput 5.21096K wps\n",
      "[Epoch 5 Batch 150/159] avg loss 0.0392662, throughput 5.21915K wps\n",
      "[Epoch 5] train avg loss 0.0707477, train avg r2 -16.7682,throughput 4.52389K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 6 Batch 10/159] avg loss 0.273955, throughput 3.66158K wps\n",
      "[Epoch 6 Batch 20/159] avg loss 0.136906, throughput 4.16108K wps\n",
      "[Epoch 6 Batch 30/159] avg loss 0.0849519, throughput 4.26851K wps\n",
      "[Epoch 6 Batch 40/159] avg loss 0.060304, throughput 4.30689K wps\n",
      "[Epoch 6 Batch 50/159] avg loss 0.043547, throughput 4.3006K wps\n",
      "[Epoch 6 Batch 60/159] avg loss 0.0489203, throughput 4.27919K wps\n",
      "[Epoch 6 Batch 70/159] avg loss 0.0308398, throughput 4.39668K wps\n",
      "[Epoch 6 Batch 80/159] avg loss 0.0451314, throughput 4.58151K wps\n",
      "[Epoch 6 Batch 90/159] avg loss 0.0379102, throughput 4.2821K wps\n",
      "[Epoch 6 Batch 100/159] avg loss 0.0391468, throughput 4.46796K wps\n",
      "[Epoch 6 Batch 110/159] avg loss 0.0565032, throughput 5.03099K wps\n",
      "[Epoch 6 Batch 120/159] avg loss 0.0594085, throughput 5.24333K wps\n",
      "[Epoch 6 Batch 130/159] avg loss 0.0470134, throughput 5.07903K wps\n",
      "[Epoch 6 Batch 140/159] avg loss 0.0472768, throughput 5.20743K wps\n",
      "[Epoch 6 Batch 150/159] avg loss 0.0389257, throughput 5.38104K wps\n",
      "[Epoch 6] train avg loss 0.0711353, train avg r2 -10.3706,throughput 4.57441K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 7 Batch 10/159] avg loss 0.270749, throughput 3.76599K wps\n",
      "[Epoch 7 Batch 20/159] avg loss 0.131268, throughput 4.09364K wps\n",
      "[Epoch 7 Batch 30/159] avg loss 0.0899506, throughput 4.18364K wps\n",
      "[Epoch 7 Batch 40/159] avg loss 0.0568706, throughput 4.30097K wps\n",
      "[Epoch 7 Batch 50/159] avg loss 0.0441453, throughput 4.27902K wps\n",
      "[Epoch 7 Batch 60/159] avg loss 0.0521604, throughput 4.13736K wps\n",
      "[Epoch 7 Batch 70/159] avg loss 0.0325974, throughput 4.25789K wps\n",
      "[Epoch 7 Batch 80/159] avg loss 0.0463515, throughput 4.37704K wps\n",
      "[Epoch 7 Batch 90/159] avg loss 0.0389539, throughput 4.88311K wps\n",
      "[Epoch 7 Batch 100/159] avg loss 0.0418356, throughput 5.08493K wps\n",
      "[Epoch 7 Batch 110/159] avg loss 0.0570891, throughput 5.09191K wps\n",
      "[Epoch 7 Batch 120/159] avg loss 0.0618343, throughput 4.9409K wps\n",
      "[Epoch 7 Batch 130/159] avg loss 0.0483931, throughput 5.25231K wps\n",
      "[Epoch 7 Batch 140/159] avg loss 0.0479388, throughput 5.35667K wps\n",
      "[Epoch 7 Batch 150/159] avg loss 0.0375091, throughput 5.26609K wps\n",
      "[Epoch 7] train avg loss 0.0716214, train avg r2 -7.90009,throughput 4.61419K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 8 Batch 10/159] avg loss 0.264129, throughput 3.76017K wps\n",
      "[Epoch 8 Batch 20/159] avg loss 0.138495, throughput 4.20485K wps\n",
      "[Epoch 8 Batch 30/159] avg loss 0.0790291, throughput 4.37345K wps\n",
      "[Epoch 8 Batch 40/159] avg loss 0.0619086, throughput 4.37944K wps\n",
      "[Epoch 8 Batch 50/159] avg loss 0.0495306, throughput 4.31588K wps\n",
      "[Epoch 8 Batch 60/159] avg loss 0.0478572, throughput 4.31963K wps\n",
      "[Epoch 8 Batch 70/159] avg loss 0.0368702, throughput 4.44179K wps\n",
      "[Epoch 8 Batch 80/159] avg loss 0.0454222, throughput 4.64559K wps\n",
      "[Epoch 8 Batch 90/159] avg loss 0.0432506, throughput 4.93251K wps\n",
      "[Epoch 8 Batch 100/159] avg loss 0.0462376, throughput 5.03643K wps\n",
      "[Epoch 8 Batch 110/159] avg loss 0.0515467, throughput 5.21041K wps\n",
      "[Epoch 8 Batch 120/159] avg loss 0.0574751, throughput 5.22147K wps\n",
      "[Epoch 8 Batch 130/159] avg loss 0.0465626, throughput 5.28759K wps\n",
      "[Epoch 8 Batch 140/159] avg loss 0.0452345, throughput 5.60455K wps\n",
      "[Epoch 8 Batch 150/159] avg loss 0.0366198, throughput 5.43764K wps\n",
      "[Epoch 8] train avg loss 0.0711198, train avg r2 -8.70263,throughput 4.72067K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 9 Batch 10/159] avg loss 0.263443, throughput 3.74141K wps\n",
      "[Epoch 9 Batch 20/159] avg loss 0.140525, throughput 4.18215K wps\n",
      "[Epoch 9 Batch 30/159] avg loss 0.0765737, throughput 4.28206K wps\n",
      "[Epoch 9 Batch 40/159] avg loss 0.0584944, throughput 4.25998K wps\n",
      "[Epoch 9 Batch 50/159] avg loss 0.042368, throughput 4.34083K wps\n",
      "[Epoch 9 Batch 60/159] avg loss 0.0458786, throughput 4.45693K wps\n",
      "[Epoch 9 Batch 70/159] avg loss 0.0293545, throughput 4.44525K wps\n",
      "[Epoch 9 Batch 80/159] avg loss 0.0457007, throughput 4.65637K wps\n",
      "[Epoch 9 Batch 90/159] avg loss 0.0395121, throughput 5.01174K wps\n",
      "[Epoch 9 Batch 100/159] avg loss 0.0439707, throughput 4.39441K wps\n",
      "[Epoch 9 Batch 110/159] avg loss 0.0520795, throughput 4.69098K wps\n",
      "[Epoch 9 Batch 120/159] avg loss 0.0587411, throughput 4.97109K wps\n",
      "[Epoch 9 Batch 130/159] avg loss 0.0468096, throughput 5.41816K wps\n",
      "[Epoch 9 Batch 140/159] avg loss 0.0456953, throughput 5.45174K wps\n",
      "[Epoch 9 Batch 150/159] avg loss 0.0370469, throughput 5.43845K wps\n",
      "[Epoch 9] train avg loss 0.0695738, train avg r2 -8.61544,throughput 4.65181K wps\n",
      "learning rate: 0.001\n",
      "[Epoch 10 Batch 10/159] avg loss 0.262455, throughput 3.73855K wps\n",
      "[Epoch 10 Batch 20/159] avg loss 0.182263, throughput 4.15721K wps\n",
      "[Epoch 10 Batch 30/159] avg loss 0.0817928, throughput 4.38874K wps\n",
      "[Epoch 10 Batch 40/159] avg loss 0.0793781, throughput 4.39543K wps\n",
      "[Epoch 10 Batch 50/159] avg loss 0.0366853, throughput 4.3676K wps\n",
      "[Epoch 10 Batch 60/159] avg loss 0.0429581, throughput 4.48604K wps\n",
      "[Epoch 10 Batch 70/159] avg loss 0.024304, throughput 4.09253K wps\n",
      "[Epoch 10 Batch 80/159] avg loss 0.0404255, throughput 4.44365K wps\n",
      "[Epoch 10 Batch 90/159] avg loss 0.0440088, throughput 4.77581K wps\n",
      "[Epoch 10 Batch 100/159] avg loss 0.0370361, throughput 4.8917K wps\n",
      "[Epoch 10 Batch 110/159] avg loss 0.0443079, throughput 5.23285K wps\n",
      "[Epoch 10 Batch 120/159] avg loss 0.0529456, throughput 5.30275K wps\n",
      "[Epoch 10 Batch 130/159] avg loss 0.0434283, throughput 5.20559K wps\n",
      "[Epoch 10 Batch 140/159] avg loss 0.0442419, throughput 5.29328K wps\n",
      "[Epoch 10 Batch 150/159] avg loss 0.0362642, throughput 5.34671K wps\n",
      "[Epoch 10] train avg loss 0.071359, train avg r2 -9.59105,throughput 4.66504K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 11 Batch 10/159] avg loss 0.251137, throughput 3.74285K wps\n",
      "[Epoch 11 Batch 20/159] avg loss 0.177852, throughput 4.167K wps\n",
      "[Epoch 11 Batch 30/159] avg loss 0.0778046, throughput 4.26284K wps\n",
      "[Epoch 11 Batch 40/159] avg loss 0.0704724, throughput 4.41232K wps\n",
      "[Epoch 11 Batch 50/159] avg loss 0.039449, throughput 4.38434K wps\n",
      "[Epoch 11 Batch 60/159] avg loss 0.0417556, throughput 4.42183K wps\n",
      "[Epoch 11 Batch 70/159] avg loss 0.0232795, throughput 4.40571K wps\n",
      "[Epoch 11 Batch 80/159] avg loss 0.0407684, throughput 4.69912K wps\n",
      "[Epoch 11 Batch 90/159] avg loss 0.043824, throughput 5.14064K wps\n",
      "[Epoch 11 Batch 100/159] avg loss 0.035067, throughput 5.03961K wps\n",
      "[Epoch 11 Batch 110/159] avg loss 0.0469614, throughput 5.32745K wps\n",
      "[Epoch 11 Batch 120/159] avg loss 0.0536284, throughput 5.2204K wps\n",
      "[Epoch 11 Batch 130/159] avg loss 0.0435118, throughput 5.25825K wps\n",
      "[Epoch 11 Batch 140/159] avg loss 0.0438747, throughput 5.45823K wps\n",
      "[Epoch 11 Batch 150/159] avg loss 0.0357852, throughput 5.54935K wps\n",
      "[Epoch 11] train avg loss 0.0695428, train avg r2 -8.66502,throughput 4.74902K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 12 Batch 10/159] avg loss 0.251046, throughput 3.71068K wps\n",
      "[Epoch 12 Batch 20/159] avg loss 0.180145, throughput 4.12168K wps\n",
      "[Epoch 12 Batch 30/159] avg loss 0.0798635, throughput 4.25433K wps\n",
      "[Epoch 12 Batch 40/159] avg loss 0.071842, throughput 4.24041K wps\n",
      "[Epoch 12 Batch 50/159] avg loss 0.0387154, throughput 4.2587K wps\n",
      "[Epoch 12 Batch 60/159] avg loss 0.0420431, throughput 4.2545K wps\n",
      "[Epoch 12 Batch 70/159] avg loss 0.0225764, throughput 4.46731K wps\n",
      "[Epoch 12 Batch 80/159] avg loss 0.0405804, throughput 4.71105K wps\n",
      "[Epoch 12 Batch 90/159] avg loss 0.0412536, throughput 4.8607K wps\n",
      "[Epoch 12 Batch 100/159] avg loss 0.0343759, throughput 4.84465K wps\n",
      "[Epoch 12 Batch 110/159] avg loss 0.0460049, throughput 4.54708K wps\n",
      "[Epoch 12 Batch 120/159] avg loss 0.0524062, throughput 4.6238K wps\n",
      "[Epoch 12 Batch 130/159] avg loss 0.0422668, throughput 4.9861K wps\n",
      "[Epoch 12 Batch 140/159] avg loss 0.0451505, throughput 5.14245K wps\n",
      "[Epoch 12 Batch 150/159] avg loss 0.0357211, throughput 5.296K wps\n",
      "[Epoch 12] train avg loss 0.0694383, train avg r2 -8.71604,throughput 4.56358K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 13 Batch 10/159] avg loss 0.250549, throughput 3.70337K wps\n",
      "[Epoch 13 Batch 20/159] avg loss 0.17689, throughput 4.25501K wps\n",
      "[Epoch 13 Batch 30/159] avg loss 0.071261, throughput 4.33186K wps\n",
      "[Epoch 13 Batch 40/159] avg loss 0.0725035, throughput 4.30973K wps\n",
      "[Epoch 13 Batch 50/159] avg loss 0.0410143, throughput 4.34936K wps\n",
      "[Epoch 13 Batch 60/159] avg loss 0.0422065, throughput 4.36425K wps\n",
      "[Epoch 13 Batch 70/159] avg loss 0.0238734, throughput 4.39671K wps\n",
      "[Epoch 13 Batch 80/159] avg loss 0.0414305, throughput 4.62611K wps\n",
      "[Epoch 13 Batch 90/159] avg loss 0.0480476, throughput 4.71725K wps\n",
      "[Epoch 13 Batch 100/159] avg loss 0.0318497, throughput 4.92014K wps\n",
      "[Epoch 13 Batch 110/159] avg loss 0.0445238, throughput 5.25518K wps\n",
      "[Epoch 13 Batch 120/159] avg loss 0.0523448, throughput 5.31565K wps\n",
      "[Epoch 13 Batch 130/159] avg loss 0.0411718, throughput 5.21924K wps\n",
      "[Epoch 13 Batch 140/159] avg loss 0.045568, throughput 5.2499K wps\n",
      "[Epoch 13 Batch 150/159] avg loss 0.0356479, throughput 5.45586K wps\n",
      "[Epoch 13] train avg loss 0.0691014, train avg r2 -7.5705,throughput 4.69916K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 14 Batch 10/159] avg loss 0.249623, throughput 3.77837K wps\n",
      "[Epoch 14 Batch 20/159] avg loss 0.182541, throughput 4.15163K wps\n",
      "[Epoch 14 Batch 30/159] avg loss 0.0839112, throughput 4.27339K wps\n",
      "[Epoch 14 Batch 40/159] avg loss 0.0716713, throughput 4.35002K wps\n",
      "[Epoch 14 Batch 50/159] avg loss 0.0364292, throughput 4.33239K wps\n",
      "[Epoch 14 Batch 60/159] avg loss 0.0410806, throughput 4.36059K wps\n",
      "[Epoch 14 Batch 70/159] avg loss 0.0222393, throughput 4.38567K wps\n",
      "[Epoch 14 Batch 80/159] avg loss 0.0401094, throughput 4.55351K wps\n",
      "[Epoch 14 Batch 90/159] avg loss 0.0412554, throughput 4.93328K wps\n",
      "[Epoch 14 Batch 100/159] avg loss 0.0329382, throughput 5.13876K wps\n",
      "[Epoch 14 Batch 110/159] avg loss 0.0459716, throughput 5.2031K wps\n",
      "[Epoch 14 Batch 120/159] avg loss 0.0524722, throughput 5.26905K wps\n",
      "[Epoch 14 Batch 130/159] avg loss 0.0415119, throughput 5.14972K wps\n",
      "[Epoch 14 Batch 140/159] avg loss 0.0445387, throughput 5.3468K wps\n",
      "[Epoch 14 Batch 150/159] avg loss 0.0360534, throughput 5.46823K wps\n",
      "[Epoch 14] train avg loss 0.0693579, train avg r2 -7.522,throughput 4.70559K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 15 Batch 10/159] avg loss 0.248567, throughput 3.77854K wps\n",
      "[Epoch 15 Batch 20/159] avg loss 0.184732, throughput 4.20788K wps\n",
      "[Epoch 15 Batch 30/159] avg loss 0.0870586, throughput 4.26768K wps\n",
      "[Epoch 15 Batch 40/159] avg loss 0.0715158, throughput 4.40452K wps\n",
      "[Epoch 15 Batch 50/159] avg loss 0.0360564, throughput 4.44678K wps\n",
      "[Epoch 15 Batch 60/159] avg loss 0.0401078, throughput 4.43811K wps\n",
      "[Epoch 15 Batch 70/159] avg loss 0.0220546, throughput 4.46643K wps\n",
      "[Epoch 15 Batch 80/159] avg loss 0.0395041, throughput 4.67719K wps\n",
      "[Epoch 15 Batch 90/159] avg loss 0.0408329, throughput 5.07383K wps\n",
      "[Epoch 15 Batch 100/159] avg loss 0.0329658, throughput 5.10047K wps\n",
      "[Epoch 15 Batch 110/159] avg loss 0.0459675, throughput 5.26613K wps\n",
      "[Epoch 15 Batch 120/159] avg loss 0.0524088, throughput 4.703K wps\n",
      "[Epoch 15 Batch 130/159] avg loss 0.0411464, throughput 4.62272K wps\n",
      "[Epoch 15 Batch 140/159] avg loss 0.0444569, throughput 4.86332K wps\n",
      "[Epoch 15 Batch 150/159] avg loss 0.0364142, throughput 5.58931K wps\n",
      "[Epoch 15] train avg loss 0.0694585, train avg r2 -7.60046,throughput 4.67614K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 16 Batch 10/159] avg loss 0.24846, throughput 3.80386K wps\n",
      "[Epoch 16 Batch 20/159] avg loss 0.184968, throughput 4.1864K wps\n",
      "[Epoch 16 Batch 30/159] avg loss 0.0825711, throughput 4.46565K wps\n",
      "[Epoch 16 Batch 40/159] avg loss 0.0700591, throughput 4.5152K wps\n",
      "[Epoch 16 Batch 50/159] avg loss 0.0420178, throughput 4.49291K wps\n",
      "[Epoch 16 Batch 60/159] avg loss 0.0427154, throughput 4.51968K wps\n",
      "[Epoch 16 Batch 70/159] avg loss 0.0230263, throughput 4.48655K wps\n",
      "[Epoch 16 Batch 80/159] avg loss 0.0401151, throughput 4.71397K wps\n",
      "[Epoch 16 Batch 90/159] avg loss 0.0441268, throughput 5.25484K wps\n",
      "[Epoch 16 Batch 100/159] avg loss 0.0316692, throughput 5.00878K wps\n",
      "[Epoch 16 Batch 110/159] avg loss 0.0452826, throughput 5.19483K wps\n",
      "[Epoch 16 Batch 120/159] avg loss 0.0526373, throughput 5.30034K wps\n",
      "[Epoch 16 Batch 130/159] avg loss 0.0408973, throughput 5.35791K wps\n",
      "[Epoch 16 Batch 140/159] avg loss 0.0444124, throughput 5.41985K wps\n",
      "[Epoch 16 Batch 150/159] avg loss 0.0362159, throughput 5.54474K wps\n",
      "[Epoch 16] train avg loss 0.0697179, train avg r2 -7.81683,throughput 4.80263K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 17 Batch 10/159] avg loss 0.25106, throughput 3.65519K wps\n",
      "[Epoch 17 Batch 20/159] avg loss 0.184642, throughput 4.05244K wps\n",
      "[Epoch 17 Batch 30/159] avg loss 0.0870946, throughput 4.12871K wps\n",
      "[Epoch 17 Batch 40/159] avg loss 0.068138, throughput 4.2313K wps\n",
      "[Epoch 17 Batch 50/159] avg loss 0.0350213, throughput 4.23851K wps\n",
      "[Epoch 17 Batch 60/159] avg loss 0.0421922, throughput 4.34209K wps\n",
      "[Epoch 17 Batch 70/159] avg loss 0.0232585, throughput 4.47595K wps\n",
      "[Epoch 17 Batch 80/159] avg loss 0.0389052, throughput 4.43508K wps\n",
      "[Epoch 17 Batch 90/159] avg loss 0.0412705, throughput 4.76724K wps\n",
      "[Epoch 17 Batch 100/159] avg loss 0.0334108, throughput 4.85572K wps\n",
      "[Epoch 17 Batch 110/159] avg loss 0.0459059, throughput 5.27559K wps\n",
      "[Epoch 17 Batch 120/159] avg loss 0.0520949, throughput 5.02057K wps\n",
      "[Epoch 17 Batch 130/159] avg loss 0.040596, throughput 5.13412K wps\n",
      "[Epoch 17 Batch 140/159] avg loss 0.0441765, throughput 5.23174K wps\n",
      "[Epoch 17 Batch 150/159] avg loss 0.036391, throughput 5.01559K wps\n",
      "[Epoch 17] train avg loss 0.0695389, train avg r2 -6.12071,throughput 4.5891K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 18 Batch 10/159] avg loss 0.245577, throughput 3.5993K wps\n",
      "[Epoch 18 Batch 20/159] avg loss 0.182343, throughput 3.98048K wps\n",
      "[Epoch 18 Batch 30/159] avg loss 0.083355, throughput 4.18783K wps\n",
      "[Epoch 18 Batch 40/159] avg loss 0.073017, throughput 4.09012K wps\n",
      "[Epoch 18 Batch 50/159] avg loss 0.0380744, throughput 4.19157K wps\n",
      "[Epoch 18 Batch 60/159] avg loss 0.0427009, throughput 4.37394K wps\n",
      "[Epoch 18 Batch 70/159] avg loss 0.0227983, throughput 4.41393K wps\n",
      "[Epoch 18 Batch 80/159] avg loss 0.0390278, throughput 4.69097K wps\n",
      "[Epoch 18 Batch 90/159] avg loss 0.0414509, throughput 4.94484K wps\n",
      "[Epoch 18 Batch 100/159] avg loss 0.0327188, throughput 5.02943K wps\n",
      "[Epoch 18 Batch 110/159] avg loss 0.0461901, throughput 5.18796K wps\n",
      "[Epoch 18 Batch 120/159] avg loss 0.0524055, throughput 5.24502K wps\n",
      "[Epoch 18 Batch 130/159] avg loss 0.0406261, throughput 4.81456K wps\n",
      "[Epoch 18 Batch 140/159] avg loss 0.0436417, throughput 4.79691K wps\n",
      "[Epoch 18 Batch 150/159] avg loss 0.0363921, throughput 4.90291K wps\n",
      "[Epoch 18] train avg loss 0.0692323, train avg r2 -6.33015,throughput 4.56882K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 19 Batch 10/159] avg loss 0.245943, throughput 3.75277K wps\n",
      "[Epoch 19 Batch 20/159] avg loss 0.185213, throughput 4.116K wps\n",
      "[Epoch 19 Batch 30/159] avg loss 0.0901103, throughput 4.27862K wps\n",
      "[Epoch 19 Batch 40/159] avg loss 0.069771, throughput 4.25535K wps\n",
      "[Epoch 19 Batch 50/159] avg loss 0.0342299, throughput 4.26859K wps\n",
      "[Epoch 19 Batch 60/159] avg loss 0.0405472, throughput 4.40381K wps\n",
      "[Epoch 19 Batch 70/159] avg loss 0.022449, throughput 4.35862K wps\n",
      "[Epoch 19 Batch 80/159] avg loss 0.0383548, throughput 4.56205K wps\n",
      "[Epoch 19 Batch 90/159] avg loss 0.0411486, throughput 4.98134K wps\n",
      "[Epoch 19 Batch 100/159] avg loss 0.0324353, throughput 5.17614K wps\n",
      "[Epoch 19 Batch 110/159] avg loss 0.0454666, throughput 5.4326K wps\n",
      "[Epoch 19 Batch 120/159] avg loss 0.0506604, throughput 5.44921K wps\n",
      "[Epoch 19 Batch 130/159] avg loss 0.0403901, throughput 5.37853K wps\n",
      "[Epoch 19 Batch 140/159] avg loss 0.0432485, throughput 5.6912K wps\n",
      "[Epoch 19 Batch 150/159] avg loss 0.0364622, throughput 5.70784K wps\n",
      "[Epoch 19] train avg loss 0.0690306, train avg r2 -6.39753,throughput 4.76159K wps\n",
      "learning rate: 0.0005\n",
      "[Epoch 20 Batch 10/159] avg loss 0.243485, throughput 3.81054K wps\n",
      "[Epoch 20 Batch 20/159] avg loss 0.202261, throughput 4.20477K wps\n",
      "[Epoch 20 Batch 30/159] avg loss 0.135377, throughput 4.29997K wps\n",
      "[Epoch 20 Batch 40/159] avg loss 0.0994238, throughput 4.12933K wps\n",
      "[Epoch 20 Batch 50/159] avg loss 0.0585854, throughput 4.24811K wps\n",
      "[Epoch 20 Batch 60/159] avg loss 0.0518633, throughput 4.17189K wps\n",
      "[Epoch 20 Batch 70/159] avg loss 0.0262396, throughput 4.43448K wps\n",
      "[Epoch 20 Batch 80/159] avg loss 0.0407312, throughput 4.45726K wps\n",
      "[Epoch 20 Batch 90/159] avg loss 0.0461879, throughput 4.8153K wps\n",
      "[Epoch 20 Batch 100/159] avg loss 0.0337859, throughput 4.98938K wps\n",
      "[Epoch 20 Batch 110/159] avg loss 0.0465915, throughput 5.00786K wps\n",
      "[Epoch 20 Batch 120/159] avg loss 0.0454672, throughput 5.34659K wps\n",
      "[Epoch 20 Batch 130/159] avg loss 0.0399294, throughput 4.90961K wps\n",
      "[Epoch 20 Batch 140/159] avg loss 0.0450298, throughput 5.20488K wps\n",
      "[Epoch 20 Batch 150/159] avg loss 0.0425672, throughput 5.48539K wps\n",
      "[Epoch 20] train avg loss 0.0783434, train avg r2 -5.60969,throughput 4.62822K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 21 Batch 10/159] avg loss 0.233409, throughput 3.65463K wps\n",
      "[Epoch 21 Batch 20/159] avg loss 0.190305, throughput 4.15579K wps\n",
      "[Epoch 21 Batch 30/159] avg loss 0.119815, throughput 4.23037K wps\n",
      "[Epoch 21 Batch 40/159] avg loss 0.0846861, throughput 4.3674K wps\n",
      "[Epoch 21 Batch 50/159] avg loss 0.0437978, throughput 4.31724K wps\n",
      "[Epoch 21 Batch 60/159] avg loss 0.0469934, throughput 4.24199K wps\n",
      "[Epoch 21 Batch 70/159] avg loss 0.033953, throughput 4.24674K wps\n",
      "[Epoch 21 Batch 80/159] avg loss 0.0401151, throughput 4.52769K wps\n",
      "[Epoch 21 Batch 90/159] avg loss 0.0556701, throughput 4.78547K wps\n",
      "[Epoch 21 Batch 100/159] avg loss 0.0316499, throughput 5.06594K wps\n",
      "[Epoch 21 Batch 110/159] avg loss 0.0452277, throughput 5.32139K wps\n",
      "[Epoch 21 Batch 120/159] avg loss 0.0444112, throughput 5.32125K wps\n",
      "[Epoch 21 Batch 130/159] avg loss 0.0392822, throughput 5.56231K wps\n",
      "[Epoch 21 Batch 140/159] avg loss 0.0465978, throughput 5.26605K wps\n",
      "[Epoch 21 Batch 150/159] avg loss 0.0448164, throughput 4.66097K wps\n",
      "[Epoch 21] train avg loss 0.0748766, train avg r2 -7.07898,throughput 4.61276K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 22 Batch 10/159] avg loss 0.230922, throughput 3.67926K wps\n",
      "[Epoch 22 Batch 20/159] avg loss 0.188527, throughput 4.21696K wps\n",
      "[Epoch 22 Batch 30/159] avg loss 0.122319, throughput 4.25381K wps\n",
      "[Epoch 22 Batch 40/159] avg loss 0.0882782, throughput 4.26801K wps\n",
      "[Epoch 22 Batch 50/159] avg loss 0.041853, throughput 4.35499K wps\n",
      "[Epoch 22 Batch 60/159] avg loss 0.0455028, throughput 4.42579K wps\n",
      "[Epoch 22 Batch 70/159] avg loss 0.032646, throughput 4.49429K wps\n",
      "[Epoch 22 Batch 80/159] avg loss 0.0395945, throughput 4.64377K wps\n",
      "[Epoch 22 Batch 90/159] avg loss 0.0544832, throughput 4.80601K wps\n",
      "[Epoch 22 Batch 100/159] avg loss 0.0313509, throughput 5.0342K wps\n",
      "[Epoch 22 Batch 110/159] avg loss 0.0449743, throughput 5.1819K wps\n",
      "[Epoch 22 Batch 120/159] avg loss 0.0442919, throughput 5.15483K wps\n",
      "[Epoch 22 Batch 130/159] avg loss 0.0387869, throughput 5.13905K wps\n",
      "[Epoch 22 Batch 140/159] avg loss 0.0461585, throughput 5.29591K wps\n",
      "[Epoch 22 Batch 150/159] avg loss 0.0438933, throughput 5.35493K wps\n",
      "[Epoch 22] train avg loss 0.0743714, train avg r2 -5.69074,throughput 4.68991K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 23 Batch 10/159] avg loss 0.22982, throughput 3.7944K wps\n",
      "[Epoch 23 Batch 20/159] avg loss 0.185692, throughput 4.06043K wps\n",
      "[Epoch 23 Batch 30/159] avg loss 0.108916, throughput 4.21032K wps\n",
      "[Epoch 23 Batch 40/159] avg loss 0.078131, throughput 4.322K wps\n",
      "[Epoch 23 Batch 50/159] avg loss 0.0356226, throughput 4.48648K wps\n",
      "[Epoch 23 Batch 60/159] avg loss 0.0355997, throughput 4.45169K wps\n",
      "[Epoch 23 Batch 70/159] avg loss 0.0221992, throughput 4.4415K wps\n",
      "[Epoch 23 Batch 80/159] avg loss 0.0394428, throughput 4.36893K wps\n",
      "[Epoch 23 Batch 90/159] avg loss 0.0433212, throughput 4.73759K wps\n",
      "[Epoch 23 Batch 100/159] avg loss 0.0307902, throughput 4.85479K wps\n",
      "[Epoch 23 Batch 110/159] avg loss 0.0448421, throughput 5.02318K wps\n",
      "[Epoch 23 Batch 120/159] avg loss 0.047831, throughput 4.92045K wps\n",
      "[Epoch 23 Batch 130/159] avg loss 0.0392179, throughput 5.10507K wps\n",
      "[Epoch 23 Batch 140/159] avg loss 0.0439126, throughput 5.22881K wps\n",
      "[Epoch 23 Batch 150/159] avg loss 0.0396649, throughput 5.28022K wps\n",
      "[Epoch 23] train avg loss 0.0696478, train avg r2 -11.241,throughput 4.64155K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 24 Batch 10/159] avg loss 0.246552, throughput 3.80187K wps\n",
      "[Epoch 24 Batch 20/159] avg loss 0.184591, throughput 4.14709K wps\n",
      "[Epoch 24 Batch 30/159] avg loss 0.0992527, throughput 4.44392K wps\n",
      "[Epoch 24 Batch 40/159] avg loss 0.0691289, throughput 4.46208K wps\n",
      "[Epoch 24 Batch 50/159] avg loss 0.0377817, throughput 4.34896K wps\n",
      "[Epoch 24 Batch 60/159] avg loss 0.0392736, throughput 4.44153K wps\n",
      "[Epoch 24 Batch 70/159] avg loss 0.0234182, throughput 4.5277K wps\n",
      "[Epoch 24 Batch 80/159] avg loss 0.0406906, throughput 4.61172K wps\n",
      "[Epoch 24 Batch 90/159] avg loss 0.0430673, throughput 5.04373K wps\n",
      "[Epoch 24 Batch 100/159] avg loss 0.0302893, throughput 4.95394K wps\n",
      "[Epoch 24 Batch 110/159] avg loss 0.0434626, throughput 5.11541K wps\n",
      "[Epoch 24 Batch 120/159] avg loss 0.0471828, throughput 5.40441K wps\n",
      "[Epoch 24 Batch 130/159] avg loss 0.039661, throughput 5.31095K wps\n",
      "[Epoch 24 Batch 140/159] avg loss 0.0440483, throughput 5.595K wps\n",
      "[Epoch 24 Batch 150/159] avg loss 0.0408295, throughput 5.44185K wps\n",
      "[Epoch 24] train avg loss 0.0700966, train avg r2 -7.32773,throughput 4.72884K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 25 Batch 10/159] avg loss 0.238035, throughput 3.48692K wps\n",
      "[Epoch 25 Batch 20/159] avg loss 0.18206, throughput 4.20146K wps\n",
      "[Epoch 25 Batch 30/159] avg loss 0.101055, throughput 4.13657K wps\n",
      "[Epoch 25 Batch 40/159] avg loss 0.073091, throughput 4.2717K wps\n",
      "[Epoch 25 Batch 50/159] avg loss 0.0351287, throughput 4.3404K wps\n",
      "[Epoch 25 Batch 60/159] avg loss 0.0367127, throughput 4.44717K wps\n",
      "[Epoch 25 Batch 70/159] avg loss 0.0226235, throughput 4.51019K wps\n",
      "[Epoch 25 Batch 80/159] avg loss 0.0392541, throughput 4.65737K wps\n",
      "[Epoch 25 Batch 90/159] avg loss 0.0461449, throughput 5.00772K wps\n",
      "[Epoch 25 Batch 100/159] avg loss 0.0293519, throughput 5.04514K wps\n",
      "[Epoch 25 Batch 110/159] avg loss 0.0424133, throughput 5.38153K wps\n",
      "[Epoch 25 Batch 120/159] avg loss 0.0461253, throughput 5.29267K wps\n",
      "[Epoch 25 Batch 130/159] avg loss 0.0384606, throughput 5.29995K wps\n",
      "[Epoch 25 Batch 140/159] avg loss 0.0441067, throughput 5.44992K wps\n",
      "[Epoch 25 Batch 150/159] avg loss 0.0398373, throughput 5.49642K wps\n",
      "[Epoch 25] train avg loss 0.069282, train avg r2 -7.86711,throughput 4.70557K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 26 Batch 10/159] avg loss 0.233843, throughput 3.8171K wps\n",
      "[Epoch 26 Batch 20/159] avg loss 0.187268, throughput 4.24572K wps\n",
      "[Epoch 26 Batch 30/159] avg loss 0.116755, throughput 4.28223K wps\n",
      "[Epoch 26 Batch 40/159] avg loss 0.0823246, throughput 4.2814K wps\n",
      "[Epoch 26 Batch 50/159] avg loss 0.0398482, throughput 4.34631K wps\n",
      "[Epoch 26 Batch 60/159] avg loss 0.0444944, throughput 4.39636K wps\n",
      "[Epoch 26 Batch 70/159] avg loss 0.0327087, throughput 4.49966K wps\n",
      "[Epoch 26 Batch 80/159] avg loss 0.0389009, throughput 4.60516K wps\n",
      "[Epoch 26 Batch 90/159] avg loss 0.0548028, throughput 5.0622K wps\n",
      "[Epoch 26 Batch 100/159] avg loss 0.0302034, throughput 5.12179K wps\n",
      "[Epoch 26 Batch 110/159] avg loss 0.0442577, throughput 5.33504K wps\n",
      "[Epoch 26 Batch 120/159] avg loss 0.0442658, throughput 5.37945K wps\n",
      "[Epoch 26 Batch 130/159] avg loss 0.038225, throughput 5.36924K wps\n",
      "[Epoch 26 Batch 140/159] avg loss 0.0449369, throughput 5.38489K wps\n",
      "[Epoch 26 Batch 150/159] avg loss 0.0433167, throughput 5.38073K wps\n",
      "[Epoch 26] train avg loss 0.0734122, train avg r2 -6.09637,throughput 4.75754K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 27 Batch 10/159] avg loss 0.223973, throughput 3.74585K wps\n",
      "[Epoch 27 Batch 20/159] avg loss 0.165142, throughput 4.08384K wps\n",
      "[Epoch 27 Batch 30/159] avg loss 0.0714624, throughput 4.29779K wps\n",
      "[Epoch 27 Batch 40/159] avg loss 0.0445101, throughput 4.42599K wps\n",
      "[Epoch 27 Batch 50/159] avg loss 0.0298328, throughput 4.34483K wps\n",
      "[Epoch 27 Batch 60/159] avg loss 0.0364881, throughput 4.3457K wps\n",
      "[Epoch 27 Batch 70/159] avg loss 0.0285726, throughput 4.38051K wps\n",
      "[Epoch 27 Batch 80/159] avg loss 0.0455978, throughput 4.55735K wps\n",
      "[Epoch 27 Batch 90/159] avg loss 0.0371162, throughput 5.09484K wps\n",
      "[Epoch 27 Batch 100/159] avg loss 0.0300756, throughput 4.88606K wps\n",
      "[Epoch 27 Batch 110/159] avg loss 0.038714, throughput 5.27072K wps\n",
      "[Epoch 27 Batch 120/159] avg loss 0.0475196, throughput 5.18642K wps\n",
      "[Epoch 27 Batch 130/159] avg loss 0.038302, throughput 5.2389K wps\n",
      "[Epoch 27 Batch 140/159] avg loss 0.042636, throughput 5.3507K wps\n",
      "[Epoch 27 Batch 150/159] avg loss 0.0383573, throughput 5.32743K wps\n",
      "[Epoch 27] train avg loss 0.063146, train avg r2 -7.36209,throughput 4.69277K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 28 Batch 10/159] avg loss 0.230359, throughput 3.43628K wps\n",
      "[Epoch 28 Batch 20/159] avg loss 0.174361, throughput 3.79972K wps\n",
      "[Epoch 28 Batch 30/159] avg loss 0.0767996, throughput 4.2282K wps\n",
      "[Epoch 28 Batch 40/159] avg loss 0.0493003, throughput 4.3292K wps\n",
      "[Epoch 28 Batch 50/159] avg loss 0.0309738, throughput 4.29425K wps\n",
      "[Epoch 28 Batch 60/159] avg loss 0.0366693, throughput 4.32503K wps\n",
      "[Epoch 28 Batch 70/159] avg loss 0.0303753, throughput 4.20819K wps\n",
      "[Epoch 28 Batch 80/159] avg loss 0.046188, throughput 4.5079K wps\n",
      "[Epoch 28 Batch 90/159] avg loss 0.0401346, throughput 5.01076K wps\n",
      "[Epoch 28 Batch 100/159] avg loss 0.0296524, throughput 4.93226K wps\n",
      "[Epoch 28 Batch 110/159] avg loss 0.0361351, throughput 4.89518K wps\n",
      "[Epoch 28 Batch 120/159] avg loss 0.0461801, throughput 4.96353K wps\n",
      "[Epoch 28 Batch 130/159] avg loss 0.0383993, throughput 5.10039K wps\n",
      "[Epoch 28 Batch 140/159] avg loss 0.044904, throughput 5.11925K wps\n",
      "[Epoch 28 Batch 150/159] avg loss 0.0396335, throughput 5.19595K wps\n",
      "[Epoch 28] train avg loss 0.0653195, train avg r2 -6.84525,throughput 4.53077K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 29 Batch 10/159] avg loss 0.22516, throughput 3.70504K wps\n",
      "[Epoch 29 Batch 20/159] avg loss 0.18722, throughput 4.00699K wps\n",
      "[Epoch 29 Batch 30/159] avg loss 0.120947, throughput 4.21196K wps\n",
      "[Epoch 29 Batch 40/159] avg loss 0.0870728, throughput 4.28505K wps\n",
      "[Epoch 29 Batch 50/159] avg loss 0.0453204, throughput 4.20604K wps\n",
      "[Epoch 29 Batch 60/159] avg loss 0.0455145, throughput 4.2002K wps\n",
      "[Epoch 29 Batch 70/159] avg loss 0.0322481, throughput 4.29295K wps\n",
      "[Epoch 29 Batch 80/159] avg loss 0.0385681, throughput 4.43539K wps\n",
      "[Epoch 29 Batch 90/159] avg loss 0.0528513, throughput 4.84076K wps\n",
      "[Epoch 29 Batch 100/159] avg loss 0.0316485, throughput 4.79803K wps\n",
      "[Epoch 29 Batch 110/159] avg loss 0.0444185, throughput 5.19431K wps\n",
      "[Epoch 29 Batch 120/159] avg loss 0.0446751, throughput 5.11574K wps\n",
      "[Epoch 29 Batch 130/159] avg loss 0.0380885, throughput 5.21648K wps\n",
      "[Epoch 29 Batch 140/159] avg loss 0.0459037, throughput 5.21886K wps\n",
      "[Epoch 29 Batch 150/159] avg loss 0.0410531, throughput 5.30658K wps\n",
      "[Epoch 29] train avg loss 0.0734821, train avg r2 -8.61951,throughput 4.59863K wps\n",
      "learning rate: 0.00025\n",
      "[Epoch 30 Batch 10/159] avg loss 0.230881, throughput 3.64538K wps\n",
      "[Epoch 30 Batch 20/159] avg loss 0.201655, throughput 3.96208K wps\n",
      "[Epoch 30 Batch 30/159] avg loss 0.147947, throughput 4.13666K wps\n",
      "[Epoch 30 Batch 40/159] avg loss 0.127214, throughput 4.24447K wps\n",
      "[Epoch 30 Batch 50/159] avg loss 0.0868463, throughput 4.33485K wps\n",
      "[Epoch 30 Batch 60/159] avg loss 0.0617029, throughput 4.40163K wps\n",
      "[Epoch 30 Batch 70/159] avg loss 0.0397657, throughput 4.40207K wps\n",
      "[Epoch 30 Batch 80/159] avg loss 0.0499446, throughput 4.56335K wps\n",
      "[Epoch 30 Batch 90/159] avg loss 0.0567189, throughput 5.01488K wps\n",
      "[Epoch 30 Batch 100/159] avg loss 0.0369244, throughput 5.09022K wps\n",
      "[Epoch 30 Batch 110/159] avg loss 0.0537734, throughput 5.30516K wps\n",
      "[Epoch 30 Batch 120/159] avg loss 0.0477146, throughput 5.21545K wps\n",
      "[Epoch 30 Batch 130/159] avg loss 0.040373, throughput 5.14094K wps\n",
      "[Epoch 30 Batch 140/159] avg loss 0.051473, throughput 5.22801K wps\n",
      "[Epoch 30 Batch 150/159] avg loss 0.0538113, throughput 5.50649K wps\n",
      "[Epoch 30] train avg loss 0.0875846, train avg r2 -4.49354,throughput 4.66421K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 31 Batch 10/159] avg loss 0.199275, throughput 3.59893K wps\n",
      "[Epoch 31 Batch 20/159] avg loss 0.176521, throughput 3.77447K wps\n",
      "[Epoch 31 Batch 30/159] avg loss 0.126196, throughput 3.97976K wps\n",
      "[Epoch 31 Batch 40/159] avg loss 0.104113, throughput 4.4255K wps\n",
      "[Epoch 31 Batch 50/159] avg loss 0.0649326, throughput 4.29212K wps\n",
      "[Epoch 31 Batch 60/159] avg loss 0.0451726, throughput 4.27009K wps\n",
      "[Epoch 31 Batch 70/159] avg loss 0.0272492, throughput 4.22462K wps\n",
      "[Epoch 31 Batch 80/159] avg loss 0.0453069, throughput 4.52559K wps\n",
      "[Epoch 31 Batch 90/159] avg loss 0.055802, throughput 4.96707K wps\n",
      "[Epoch 31 Batch 100/159] avg loss 0.038767, throughput 5.0763K wps\n",
      "[Epoch 31 Batch 110/159] avg loss 0.0563228, throughput 5.18318K wps\n",
      "[Epoch 31 Batch 120/159] avg loss 0.0468984, throughput 5.25176K wps\n",
      "[Epoch 31 Batch 130/159] avg loss 0.0428499, throughput 5.43303K wps\n",
      "[Epoch 31 Batch 140/159] avg loss 0.0547018, throughput 5.44903K wps\n",
      "[Epoch 31 Batch 150/159] avg loss 0.0571833, throughput 5.31979K wps\n",
      "[Epoch 31] train avg loss 0.078546, train avg r2 -3.89479,throughput 4.60777K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 32 Batch 10/159] avg loss 0.19618, throughput 3.46686K wps\n",
      "[Epoch 32 Batch 20/159] avg loss 0.172309, throughput 3.9665K wps\n",
      "[Epoch 32 Batch 30/159] avg loss 0.121325, throughput 4.10324K wps\n",
      "[Epoch 32 Batch 40/159] avg loss 0.099663, throughput 4.16705K wps\n",
      "[Epoch 32 Batch 50/159] avg loss 0.0606605, throughput 4.1332K wps\n",
      "[Epoch 32 Batch 60/159] avg loss 0.0421462, throughput 4.33985K wps\n",
      "[Epoch 32 Batch 70/159] avg loss 0.0246529, throughput 4.3133K wps\n",
      "[Epoch 32 Batch 80/159] avg loss 0.0439555, throughput 4.54876K wps\n",
      "[Epoch 32 Batch 90/159] avg loss 0.0551095, throughput 5.16436K wps\n",
      "[Epoch 32 Batch 100/159] avg loss 0.038702, throughput 4.95735K wps\n",
      "[Epoch 32 Batch 110/159] avg loss 0.0571969, throughput 5.12649K wps\n",
      "[Epoch 32 Batch 120/159] avg loss 0.0469797, throughput 5.31948K wps\n",
      "[Epoch 32 Batch 130/159] avg loss 0.0435516, throughput 5.30685K wps\n",
      "[Epoch 32 Batch 140/159] avg loss 0.0550441, throughput 5.38279K wps\n",
      "[Epoch 32 Batch 150/159] avg loss 0.0571175, throughput 5.33341K wps\n",
      "[Epoch 32] train avg loss 0.0768402, train avg r2 -3.84328,throughput 4.6091K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 33 Batch 10/159] avg loss 0.196192, throughput 3.75397K wps\n",
      "[Epoch 33 Batch 20/159] avg loss 0.172489, throughput 4.30275K wps\n",
      "[Epoch 33 Batch 30/159] avg loss 0.12151, throughput 4.48574K wps\n",
      "[Epoch 33 Batch 40/159] avg loss 0.100103, throughput 4.37215K wps\n",
      "[Epoch 33 Batch 50/159] avg loss 0.0609524, throughput 4.39936K wps\n",
      "[Epoch 33 Batch 60/159] avg loss 0.0420098, throughput 4.49179K wps\n",
      "[Epoch 33 Batch 70/159] avg loss 0.0246337, throughput 4.4147K wps\n",
      "[Epoch 33 Batch 80/159] avg loss 0.0438911, throughput 4.60005K wps\n",
      "[Epoch 33 Batch 90/159] avg loss 0.0558063, throughput 4.89193K wps\n",
      "[Epoch 33 Batch 100/159] avg loss 0.0392428, throughput 5.00568K wps\n",
      "[Epoch 33 Batch 110/159] avg loss 0.0580765, throughput 5.27849K wps\n",
      "[Epoch 33 Batch 120/159] avg loss 0.0476741, throughput 5.35806K wps\n",
      "[Epoch 33 Batch 130/159] avg loss 0.0441357, throughput 5.27207K wps\n",
      "[Epoch 33 Batch 140/159] avg loss 0.0562011, throughput 5.45101K wps\n",
      "[Epoch 33 Batch 150/159] avg loss 0.0581019, throughput 5.23798K wps\n",
      "[Epoch 33] train avg loss 0.0773349, train avg r2 -3.74386,throughput 4.75488K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 34 Batch 10/159] avg loss 0.194268, throughput 3.72584K wps\n",
      "[Epoch 34 Batch 20/159] avg loss 0.171005, throughput 3.90872K wps\n",
      "[Epoch 34 Batch 30/159] avg loss 0.119967, throughput 3.943K wps\n",
      "[Epoch 34 Batch 40/159] avg loss 0.0949453, throughput 4.07905K wps\n",
      "[Epoch 34 Batch 50/159] avg loss 0.0472092, throughput 4.29947K wps\n",
      "[Epoch 34 Batch 60/159] avg loss 0.0412734, throughput 4.32268K wps\n",
      "[Epoch 34 Batch 70/159] avg loss 0.0230091, throughput 4.30262K wps\n",
      "[Epoch 34 Batch 80/159] avg loss 0.0402686, throughput 4.51648K wps\n",
      "[Epoch 34 Batch 90/159] avg loss 0.0503978, throughput 4.97218K wps\n",
      "[Epoch 34 Batch 100/159] avg loss 0.0304385, throughput 4.94835K wps\n",
      "[Epoch 34 Batch 110/159] avg loss 0.0480665, throughput 5.20454K wps\n",
      "[Epoch 34 Batch 120/159] avg loss 0.0431739, throughput 5.32676K wps\n",
      "[Epoch 34 Batch 130/159] avg loss 0.0399414, throughput 5.32917K wps\n",
      "[Epoch 34 Batch 140/159] avg loss 0.0482157, throughput 5.47574K wps\n",
      "[Epoch 34 Batch 150/159] avg loss 0.0490279, throughput 5.6397K wps\n",
      "[Epoch 34] train avg loss 0.0717905, train avg r2 -4.79052,throughput 4.6357K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 35 Batch 10/159] avg loss 0.202938, throughput 3.72987K wps\n",
      "[Epoch 35 Batch 20/159] avg loss 0.175699, throughput 4.19443K wps\n",
      "[Epoch 35 Batch 30/159] avg loss 0.121063, throughput 4.27909K wps\n",
      "[Epoch 35 Batch 40/159] avg loss 0.0990213, throughput 4.43899K wps\n",
      "[Epoch 35 Batch 50/159] avg loss 0.0559356, throughput 4.37702K wps\n",
      "[Epoch 35 Batch 60/159] avg loss 0.0400751, throughput 4.40167K wps\n",
      "[Epoch 35 Batch 70/159] avg loss 0.0224517, throughput 4.43601K wps\n",
      "[Epoch 35 Batch 80/159] avg loss 0.0407142, throughput 4.72275K wps\n",
      "[Epoch 35 Batch 90/159] avg loss 0.0466646, throughput 5.09724K wps\n",
      "[Epoch 35 Batch 100/159] avg loss 0.0302456, throughput 5.12734K wps\n",
      "[Epoch 35 Batch 110/159] avg loss 0.04987, throughput 5.19407K wps\n",
      "[Epoch 35 Batch 120/159] avg loss 0.0441836, throughput 5.3066K wps\n",
      "[Epoch 35 Batch 130/159] avg loss 0.0416462, throughput 5.2437K wps\n",
      "[Epoch 35 Batch 140/159] avg loss 0.0516757, throughput 5.42718K wps\n",
      "[Epoch 35 Batch 150/159] avg loss 0.053731, throughput 5.43076K wps\n",
      "[Epoch 35] train avg loss 0.0742383, train avg r2 -4.10213,throughput 4.74413K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 36 Batch 10/159] avg loss 0.197148, throughput 3.75429K wps\n",
      "[Epoch 36 Batch 20/159] avg loss 0.173072, throughput 4.19024K wps\n",
      "[Epoch 36 Batch 30/159] avg loss 0.121276, throughput 4.40952K wps\n",
      "[Epoch 36 Batch 40/159] avg loss 0.100427, throughput 4.38706K wps\n",
      "[Epoch 36 Batch 50/159] avg loss 0.061858, throughput 4.35771K wps\n",
      "[Epoch 36 Batch 60/159] avg loss 0.0425665, throughput 4.37443K wps\n",
      "[Epoch 36 Batch 70/159] avg loss 0.0259369, throughput 4.48726K wps\n",
      "[Epoch 36 Batch 80/159] avg loss 0.0440828, throughput 4.65596K wps\n",
      "[Epoch 36 Batch 90/159] avg loss 0.054398, throughput 5.04824K wps\n",
      "[Epoch 36 Batch 100/159] avg loss 0.0391274, throughput 4.88733K wps\n",
      "[Epoch 36 Batch 110/159] avg loss 0.0584688, throughput 5.29434K wps\n",
      "[Epoch 36 Batch 120/159] avg loss 0.0479225, throughput 5.1728K wps\n",
      "[Epoch 36 Batch 130/159] avg loss 0.0437486, throughput 5.32484K wps\n",
      "[Epoch 36 Batch 140/159] avg loss 0.0557443, throughput 5.21397K wps\n",
      "[Epoch 36 Batch 150/159] avg loss 0.0571678, throughput 5.45286K wps\n",
      "[Epoch 36] train avg loss 0.0774512, train avg r2 -3.98427,throughput 4.71891K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 37 Batch 10/159] avg loss 0.193092, throughput 3.73011K wps\n",
      "[Epoch 37 Batch 20/159] avg loss 0.168995, throughput 4.16979K wps\n",
      "[Epoch 37 Batch 30/159] avg loss 0.117859, throughput 4.25197K wps\n",
      "[Epoch 37 Batch 40/159] avg loss 0.0969478, throughput 3.90026K wps\n",
      "[Epoch 37 Batch 50/159] avg loss 0.0589119, throughput 4.04384K wps\n",
      "[Epoch 37 Batch 60/159] avg loss 0.0412613, throughput 4.3807K wps\n",
      "[Epoch 37 Batch 70/159] avg loss 0.0250332, throughput 4.43367K wps\n",
      "[Epoch 37 Batch 80/159] avg loss 0.0436969, throughput 4.57932K wps\n",
      "[Epoch 37 Batch 90/159] avg loss 0.0543153, throughput 4.70303K wps\n",
      "[Epoch 37 Batch 100/159] avg loss 0.0384632, throughput 4.85298K wps\n",
      "[Epoch 37 Batch 110/159] avg loss 0.0588104, throughput 5.10883K wps\n",
      "[Epoch 37 Batch 120/159] avg loss 0.0481708, throughput 4.99716K wps\n",
      "[Epoch 37 Batch 130/159] avg loss 0.0442989, throughput 5.21303K wps\n",
      "[Epoch 37 Batch 140/159] avg loss 0.0565976, throughput 5.28449K wps\n",
      "[Epoch 37 Batch 150/159] avg loss 0.0578028, throughput 5.50963K wps\n",
      "[Epoch 37] train avg loss 0.0763041, train avg r2 -3.77383,throughput 4.60727K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 38 Batch 10/159] avg loss 0.192472, throughput 3.73938K wps\n",
      "[Epoch 38 Batch 20/159] avg loss 0.169699, throughput 4.12993K wps\n",
      "[Epoch 38 Batch 30/159] avg loss 0.118701, throughput 4.11793K wps\n",
      "[Epoch 38 Batch 40/159] avg loss 0.0975545, throughput 4.184K wps\n",
      "[Epoch 38 Batch 50/159] avg loss 0.0597754, throughput 4.11437K wps\n",
      "[Epoch 38 Batch 60/159] avg loss 0.0417362, throughput 4.29676K wps\n",
      "[Epoch 38 Batch 70/159] avg loss 0.0252306, throughput 4.30341K wps\n",
      "[Epoch 38 Batch 80/159] avg loss 0.0441827, throughput 4.43255K wps\n",
      "[Epoch 38 Batch 90/159] avg loss 0.0552026, throughput 4.75603K wps\n",
      "[Epoch 38 Batch 100/159] avg loss 0.0390809, throughput 4.8724K wps\n",
      "[Epoch 38 Batch 110/159] avg loss 0.059766, throughput 5.06211K wps\n",
      "[Epoch 38 Batch 120/159] avg loss 0.0491055, throughput 5.05268K wps\n",
      "[Epoch 38 Batch 130/159] avg loss 0.044986, throughput 4.87322K wps\n",
      "[Epoch 38 Batch 140/159] avg loss 0.0575374, throughput 5.10266K wps\n",
      "[Epoch 38 Batch 150/159] avg loss 0.0585959, throughput 5.29212K wps\n",
      "[Epoch 38] train avg loss 0.0769698, train avg r2 -3.7196,throughput 4.5547K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 39 Batch 10/159] avg loss 0.190738, throughput 3.58173K wps\n",
      "[Epoch 39 Batch 20/159] avg loss 0.168181, throughput 4.11434K wps\n",
      "[Epoch 39 Batch 30/159] avg loss 0.116985, throughput 4.17805K wps\n",
      "[Epoch 39 Batch 40/159] avg loss 0.0955262, throughput 4.17938K wps\n",
      "[Epoch 39 Batch 50/159] avg loss 0.0583384, throughput 4.37759K wps\n",
      "[Epoch 39 Batch 60/159] avg loss 0.0409748, throughput 4.41103K wps\n",
      "[Epoch 39 Batch 70/159] avg loss 0.0252455, throughput 4.41722K wps\n",
      "[Epoch 39 Batch 80/159] avg loss 0.0444177, throughput 4.52672K wps\n",
      "[Epoch 39 Batch 90/159] avg loss 0.0558456, throughput 4.94975K wps\n",
      "[Epoch 39 Batch 100/159] avg loss 0.0394227, throughput 4.7423K wps\n",
      "[Epoch 39 Batch 110/159] avg loss 0.0604284, throughput 5.16614K wps\n",
      "[Epoch 39 Batch 120/159] avg loss 0.0497721, throughput 4.91128K wps\n",
      "[Epoch 39 Batch 130/159] avg loss 0.0452377, throughput 5.18525K wps\n",
      "[Epoch 39 Batch 140/159] avg loss 0.0580053, throughput 5.38801K wps\n",
      "[Epoch 39 Batch 150/159] avg loss 0.0589445, throughput 5.25825K wps\n",
      "[Epoch 39] train avg loss 0.0766464, train avg r2 -3.66326,throughput 4.61755K wps\n",
      "learning rate: 0.000125\n",
      "[Epoch 40 Batch 10/159] avg loss 0.188848, throughput 3.58799K wps\n",
      "[Epoch 40 Batch 20/159] avg loss 0.171113, throughput 3.99893K wps\n",
      "[Epoch 40 Batch 30/159] avg loss 0.128945, throughput 4.27723K wps\n",
      "[Epoch 40 Batch 40/159] avg loss 0.114251, throughput 3.86486K wps\n",
      "[Epoch 40 Batch 50/159] avg loss 0.0859522, throughput 3.94899K wps\n",
      "[Epoch 40 Batch 60/159] avg loss 0.0701742, throughput 4.35228K wps\n",
      "[Epoch 40 Batch 70/159] avg loss 0.051788, throughput 4.56631K wps\n",
      "[Epoch 40 Batch 80/159] avg loss 0.0542583, throughput 4.56979K wps\n",
      "[Epoch 40 Batch 90/159] avg loss 0.0426698, throughput 4.94942K wps\n",
      "[Epoch 40 Batch 100/159] avg loss 0.0380939, throughput 4.94233K wps\n",
      "[Epoch 40 Batch 110/159] avg loss 0.0636813, throughput 5.15776K wps\n",
      "[Epoch 40 Batch 120/159] avg loss 0.0500794, throughput 5.51119K wps\n",
      "[Epoch 40 Batch 130/159] avg loss 0.0444145, throughput 5.43763K wps\n",
      "[Epoch 40 Batch 140/159] avg loss 0.0618322, throughput 5.62104K wps\n",
      "[Epoch 40 Batch 150/159] avg loss 0.0643215, throughput 5.60177K wps\n",
      "[Epoch 40] train avg loss 0.0852787, train avg r2 -3.39009,throughput 4.65399K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 41 Batch 10/159] avg loss 0.169694, throughput 3.79939K wps\n",
      "[Epoch 41 Batch 20/159] avg loss 0.151604, throughput 4.30492K wps\n",
      "[Epoch 41 Batch 30/159] avg loss 0.109088, throughput 4.43586K wps\n",
      "[Epoch 41 Batch 40/159] avg loss 0.0916265, throughput 4.42043K wps\n",
      "[Epoch 41 Batch 50/159] avg loss 0.0652114, throughput 4.43799K wps\n",
      "[Epoch 41 Batch 60/159] avg loss 0.050043, throughput 4.49168K wps\n",
      "[Epoch 41 Batch 70/159] avg loss 0.0334568, throughput 4.43776K wps\n",
      "[Epoch 41 Batch 80/159] avg loss 0.0470847, throughput 4.60692K wps\n",
      "[Epoch 41 Batch 90/159] avg loss 0.0521893, throughput 5.10981K wps\n",
      "[Epoch 41 Batch 100/159] avg loss 0.0437536, throughput 5.33775K wps\n",
      "[Epoch 41 Batch 110/159] avg loss 0.0675846, throughput 5.48379K wps\n",
      "[Epoch 41 Batch 120/159] avg loss 0.0538425, throughput 5.02982K wps\n",
      "[Epoch 41 Batch 130/159] avg loss 0.0491611, throughput 4.77255K wps\n",
      "[Epoch 41 Batch 140/159] avg loss 0.0681648, throughput 5.17574K wps\n",
      "[Epoch 41 Batch 150/159] avg loss 0.0697465, throughput 5.24702K wps\n",
      "[Epoch 41] train avg loss 0.0787817, train avg r2 -3.10838,throughput 4.74928K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 42 Batch 10/159] avg loss 0.163405, throughput 3.80611K wps\n",
      "[Epoch 42 Batch 20/159] avg loss 0.145373, throughput 4.19574K wps\n",
      "[Epoch 42 Batch 30/159] avg loss 0.10265, throughput 4.24562K wps\n",
      "[Epoch 42 Batch 40/159] avg loss 0.0849036, throughput 4.28323K wps\n",
      "[Epoch 42 Batch 50/159] avg loss 0.0589466, throughput 4.323K wps\n",
      "[Epoch 42 Batch 60/159] avg loss 0.0450262, throughput 4.44254K wps\n",
      "[Epoch 42 Batch 70/159] avg loss 0.0294655, throughput 4.42422K wps\n",
      "[Epoch 42 Batch 80/159] avg loss 0.0459169, throughput 4.59624K wps\n",
      "[Epoch 42 Batch 90/159] avg loss 0.053988, throughput 5.19537K wps\n",
      "[Epoch 42 Batch 100/159] avg loss 0.0446961, throughput 5.00668K wps\n",
      "[Epoch 42 Batch 110/159] avg loss 0.0677844, throughput 5.28398K wps\n",
      "[Epoch 42 Batch 120/159] avg loss 0.0551166, throughput 5.39587K wps\n",
      "[Epoch 42 Batch 130/159] avg loss 0.0506101, throughput 5.32467K wps\n",
      "[Epoch 42 Batch 140/159] avg loss 0.069869, throughput 5.41702K wps\n",
      "[Epoch 42 Batch 150/159] avg loss 0.0716822, throughput 5.55114K wps\n",
      "[Epoch 42] train avg loss 0.0768062, train avg r2 -2.95549,throughput 4.75087K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 43 Batch 10/159] avg loss 0.161137, throughput 3.79474K wps\n",
      "[Epoch 43 Batch 20/159] avg loss 0.142899, throughput 4.15408K wps\n",
      "[Epoch 43 Batch 30/159] avg loss 0.100242, throughput 4.36267K wps\n",
      "[Epoch 43 Batch 40/159] avg loss 0.0827345, throughput 4.44733K wps\n",
      "[Epoch 43 Batch 50/159] avg loss 0.0568492, throughput 4.17407K wps\n",
      "[Epoch 43 Batch 60/159] avg loss 0.043733, throughput 3.99531K wps\n",
      "[Epoch 43 Batch 70/159] avg loss 0.0281801, throughput 4.13347K wps\n",
      "[Epoch 43 Batch 80/159] avg loss 0.0454352, throughput 4.74239K wps\n",
      "[Epoch 43 Batch 90/159] avg loss 0.0543429, throughput 5.04883K wps\n",
      "[Epoch 43 Batch 100/159] avg loss 0.0449366, throughput 5.09111K wps\n",
      "[Epoch 43 Batch 110/159] avg loss 0.067746, throughput 5.29923K wps\n",
      "[Epoch 43 Batch 120/159] avg loss 0.055638, throughput 5.25458K wps\n",
      "[Epoch 43 Batch 130/159] avg loss 0.0509783, throughput 5.36063K wps\n",
      "[Epoch 43 Batch 140/159] avg loss 0.0703945, throughput 5.5144K wps\n",
      "[Epoch 43 Batch 150/159] avg loss 0.0723543, throughput 5.482K wps\n",
      "[Epoch 43] train avg loss 0.0760866, train avg r2 -2.88201,throughput 4.70515K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 44 Batch 10/159] avg loss 0.160048, throughput 3.70425K wps\n",
      "[Epoch 44 Batch 20/159] avg loss 0.141801, throughput 4.18601K wps\n",
      "[Epoch 44 Batch 30/159] avg loss 0.0992449, throughput 4.32827K wps\n",
      "[Epoch 44 Batch 40/159] avg loss 0.0818531, throughput 4.37584K wps\n",
      "[Epoch 44 Batch 50/159] avg loss 0.0560324, throughput 4.35124K wps\n",
      "[Epoch 44 Batch 60/159] avg loss 0.043321, throughput 4.38333K wps\n",
      "[Epoch 44 Batch 70/159] avg loss 0.0275963, throughput 4.41574K wps\n",
      "[Epoch 44 Batch 80/159] avg loss 0.0452433, throughput 4.67642K wps\n",
      "[Epoch 44 Batch 90/159] avg loss 0.0545927, throughput 5.07354K wps\n",
      "[Epoch 44 Batch 100/159] avg loss 0.0451326, throughput 5.05786K wps\n",
      "[Epoch 44 Batch 110/159] avg loss 0.0680596, throughput 5.4017K wps\n",
      "[Epoch 44 Batch 120/159] avg loss 0.0560875, throughput 5.21721K wps\n",
      "[Epoch 44 Batch 130/159] avg loss 0.0512284, throughput 5.43532K wps\n",
      "[Epoch 44 Batch 140/159] avg loss 0.0708541, throughput 5.50187K wps\n",
      "[Epoch 44 Batch 150/159] avg loss 0.072762, throughput 5.48978K wps\n",
      "[Epoch 44] train avg loss 0.0758677, train avg r2 -2.80284,throughput 4.75404K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 45 Batch 10/159] avg loss 0.159314, throughput 3.68818K wps\n",
      "[Epoch 45 Batch 20/159] avg loss 0.141128, throughput 4.13463K wps\n",
      "[Epoch 45 Batch 30/159] avg loss 0.0984751, throughput 4.30603K wps\n",
      "[Epoch 45 Batch 40/159] avg loss 0.0810974, throughput 4.3569K wps\n",
      "[Epoch 45 Batch 50/159] avg loss 0.0554239, throughput 4.26854K wps\n",
      "[Epoch 45 Batch 60/159] avg loss 0.0431678, throughput 4.44277K wps\n",
      "[Epoch 45 Batch 70/159] avg loss 0.0273997, throughput 4.54326K wps\n",
      "[Epoch 45 Batch 80/159] avg loss 0.0451979, throughput 4.64917K wps\n",
      "[Epoch 45 Batch 90/159] avg loss 0.0544262, throughput 5.1152K wps\n",
      "[Epoch 45 Batch 100/159] avg loss 0.0449833, throughput 4.87467K wps\n",
      "[Epoch 45 Batch 110/159] avg loss 0.0681562, throughput 5.3794K wps\n",
      "[Epoch 45 Batch 120/159] avg loss 0.0563284, throughput 5.24506K wps\n",
      "[Epoch 45 Batch 130/159] avg loss 0.0513747, throughput 5.16247K wps\n",
      "[Epoch 45 Batch 140/159] avg loss 0.0711441, throughput 5.41574K wps\n",
      "[Epoch 45 Batch 150/159] avg loss 0.0730153, throughput 5.19501K wps\n",
      "[Epoch 45] train avg loss 0.0756811, train avg r2 -2.75336,throughput 4.70014K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 46 Batch 10/159] avg loss 0.158705, throughput 3.70353K wps\n",
      "[Epoch 46 Batch 20/159] avg loss 0.140632, throughput 4.09792K wps\n",
      "[Epoch 46 Batch 30/159] avg loss 0.0979619, throughput 4.24561K wps\n",
      "[Epoch 46 Batch 40/159] avg loss 0.0805545, throughput 4.25169K wps\n",
      "[Epoch 46 Batch 50/159] avg loss 0.0549876, throughput 4.25473K wps\n",
      "[Epoch 46 Batch 60/159] avg loss 0.0430531, throughput 4.2294K wps\n",
      "[Epoch 46 Batch 70/159] avg loss 0.0272848, throughput 3.92291K wps\n",
      "[Epoch 46 Batch 80/159] avg loss 0.0452182, throughput 4.1744K wps\n",
      "[Epoch 46 Batch 90/159] avg loss 0.0542084, throughput 4.86148K wps\n",
      "[Epoch 46 Batch 100/159] avg loss 0.0449152, throughput 5.03402K wps\n",
      "[Epoch 46 Batch 110/159] avg loss 0.0682942, throughput 5.05254K wps\n",
      "[Epoch 46 Batch 120/159] avg loss 0.0563764, throughput 5.23969K wps\n",
      "[Epoch 46 Batch 130/159] avg loss 0.0514221, throughput 5.08399K wps\n",
      "[Epoch 46 Batch 140/159] avg loss 0.0712926, throughput 5.20823K wps\n",
      "[Epoch 46 Batch 150/159] avg loss 0.0731467, throughput 5.51213K wps\n",
      "[Epoch 46] train avg loss 0.0755302, train avg r2 -2.68263,throughput 4.57378K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 47 Batch 10/159] avg loss 0.158091, throughput 3.67037K wps\n",
      "[Epoch 47 Batch 20/159] avg loss 0.140171, throughput 3.96175K wps\n",
      "[Epoch 47 Batch 30/159] avg loss 0.097447, throughput 4.15583K wps\n",
      "[Epoch 47 Batch 40/159] avg loss 0.0798306, throughput 4.21177K wps\n",
      "[Epoch 47 Batch 50/159] avg loss 0.0537887, throughput 4.31938K wps\n",
      "[Epoch 47 Batch 60/159] avg loss 0.0421707, throughput 4.28398K wps\n",
      "[Epoch 47 Batch 70/159] avg loss 0.0261771, throughput 4.41667K wps\n",
      "[Epoch 47 Batch 80/159] avg loss 0.0449181, throughput 4.57537K wps\n",
      "[Epoch 47 Batch 90/159] avg loss 0.0550406, throughput 4.93884K wps\n",
      "[Epoch 47 Batch 100/159] avg loss 0.0438558, throughput 5.0863K wps\n",
      "[Epoch 47 Batch 110/159] avg loss 0.0664317, throughput 5.20856K wps\n",
      "[Epoch 47 Batch 120/159] avg loss 0.0560561, throughput 5.2482K wps\n",
      "[Epoch 47 Batch 130/159] avg loss 0.0510051, throughput 5.24869K wps\n",
      "[Epoch 47 Batch 140/159] avg loss 0.0702446, throughput 5.39689K wps\n",
      "[Epoch 47 Batch 150/159] avg loss 0.0726271, throughput 5.59097K wps\n",
      "[Epoch 47] train avg loss 0.0748246, train avg r2 -2.69748,throughput 4.64592K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 48 Batch 10/159] avg loss 0.159409, throughput 3.58318K wps\n",
      "[Epoch 48 Batch 20/159] avg loss 0.14088, throughput 4.0364K wps\n",
      "[Epoch 48 Batch 30/159] avg loss 0.0981076, throughput 4.23514K wps\n",
      "[Epoch 48 Batch 40/159] avg loss 0.0806845, throughput 4.29818K wps\n",
      "[Epoch 48 Batch 50/159] avg loss 0.0549906, throughput 4.40289K wps\n",
      "[Epoch 48 Batch 60/159] avg loss 0.0430353, throughput 4.29425K wps\n",
      "[Epoch 48 Batch 70/159] avg loss 0.0267082, throughput 4.48046K wps\n",
      "[Epoch 48 Batch 80/159] avg loss 0.0450828, throughput 4.63695K wps\n",
      "[Epoch 48 Batch 90/159] avg loss 0.05489, throughput 4.94743K wps\n",
      "[Epoch 48 Batch 100/159] avg loss 0.0450405, throughput 4.95046K wps\n",
      "[Epoch 48 Batch 110/159] avg loss 0.0684815, throughput 5.29812K wps\n",
      "[Epoch 48 Batch 120/159] avg loss 0.0565065, throughput 5.29023K wps\n",
      "[Epoch 48 Batch 130/159] avg loss 0.0516817, throughput 5.35499K wps\n",
      "[Epoch 48 Batch 140/159] avg loss 0.0714049, throughput 5.29244K wps\n",
      "[Epoch 48 Batch 150/159] avg loss 0.0733262, throughput 5.37394K wps\n",
      "[Epoch 48] train avg loss 0.0756759, train avg r2 -2.657,throughput 4.68388K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 49 Batch 10/159] avg loss 0.157565, throughput 3.69322K wps\n",
      "[Epoch 49 Batch 20/159] avg loss 0.139833, throughput 4.1793K wps\n",
      "[Epoch 49 Batch 30/159] avg loss 0.0971703, throughput 4.29323K wps\n",
      "[Epoch 49 Batch 40/159] avg loss 0.0796502, throughput 4.32194K wps\n",
      "[Epoch 49 Batch 50/159] avg loss 0.0544754, throughput 4.31123K wps\n",
      "[Epoch 49 Batch 60/159] avg loss 0.0427769, throughput 4.36581K wps\n",
      "[Epoch 49 Batch 70/159] avg loss 0.0269476, throughput 4.11554K wps\n",
      "[Epoch 49 Batch 80/159] avg loss 0.0452193, throughput 4.16871K wps\n",
      "[Epoch 49 Batch 90/159] avg loss 0.0541411, throughput 4.6943K wps\n",
      "[Epoch 49 Batch 100/159] avg loss 0.0447907, throughput 4.9448K wps\n",
      "[Epoch 49 Batch 110/159] avg loss 0.068462, throughput 5.24763K wps\n",
      "[Epoch 49 Batch 120/159] avg loss 0.0565994, throughput 5.21789K wps\n",
      "[Epoch 49 Batch 130/159] avg loss 0.0515212, throughput 5.39712K wps\n",
      "[Epoch 49 Batch 140/159] avg loss 0.0714869, throughput 5.18159K wps\n",
      "[Epoch 49 Batch 150/159] avg loss 0.0731174, throughput 5.32408K wps\n",
      "[Epoch 49] train avg loss 0.0752595, train avg r2 -2.66275,throughput 4.6215K wps\n",
      "learning rate: 6.25e-05\n",
      "[Epoch 50 Batch 10/159] avg loss 0.156601, throughput 3.64606K wps\n",
      "[Epoch 50 Batch 20/159] avg loss 0.141089, throughput 4.10484K wps\n",
      "[Epoch 50 Batch 30/159] avg loss 0.103891, throughput 4.19153K wps\n",
      "[Epoch 50 Batch 40/159] avg loss 0.0914086, throughput 4.19968K wps\n",
      "[Epoch 50 Batch 50/159] avg loss 0.0710702, throughput 4.29189K wps\n",
      "[Epoch 50 Batch 60/159] avg loss 0.06052, throughput 4.30356K wps\n",
      "[Epoch 50 Batch 70/159] avg loss 0.0456896, throughput 4.32648K wps\n",
      "[Epoch 50 Batch 80/159] avg loss 0.0543236, throughput 4.56866K wps\n",
      "[Epoch 50 Batch 90/159] avg loss 0.035816, throughput 5.09043K wps\n",
      "[Epoch 50 Batch 100/159] avg loss 0.0347717, throughput 5.15206K wps\n",
      "[Epoch 50 Batch 110/159] avg loss 0.0654438, throughput 5.32984K wps\n",
      "[Epoch 50 Batch 120/159] avg loss 0.0533927, throughput 5.13076K wps\n",
      "[Epoch 50 Batch 130/159] avg loss 0.0489748, throughput 5.41359K wps\n",
      "[Epoch 50 Batch 140/159] avg loss 0.0706036, throughput 5.33586K wps\n",
      "[Epoch 50 Batch 150/159] avg loss 0.0753138, throughput 5.56709K wps\n",
      "[Epoch 50] train avg loss 0.0785809, train avg r2 -2.40806,throughput 4.68622K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 51 Batch 10/159] avg loss 0.147227, throughput 3.80888K wps\n",
      "[Epoch 51 Batch 20/159] avg loss 0.131213, throughput 4.08323K wps\n",
      "[Epoch 51 Batch 30/159] avg loss 0.0937108, throughput 4.26576K wps\n",
      "[Epoch 51 Batch 40/159] avg loss 0.0797536, throughput 4.44294K wps\n",
      "[Epoch 51 Batch 50/159] avg loss 0.0612568, throughput 4.37591K wps\n",
      "[Epoch 51 Batch 60/159] avg loss 0.0505784, throughput 4.36053K wps\n",
      "[Epoch 51 Batch 70/159] avg loss 0.037068, throughput 4.37156K wps\n",
      "[Epoch 51 Batch 80/159] avg loss 0.0495958, throughput 4.62607K wps\n",
      "[Epoch 51 Batch 90/159] avg loss 0.0414368, throughput 5.14922K wps\n",
      "[Epoch 51 Batch 100/159] avg loss 0.0375782, throughput 5.1399K wps\n",
      "[Epoch 51 Batch 110/159] avg loss 0.0685667, throughput 5.07518K wps\n",
      "[Epoch 51 Batch 120/159] avg loss 0.0568807, throughput 5.21939K wps\n",
      "[Epoch 51 Batch 130/159] avg loss 0.0521574, throughput 5.19734K wps\n",
      "[Epoch 51 Batch 140/159] avg loss 0.0759988, throughput 5.43632K wps\n",
      "[Epoch 51 Batch 150/159] avg loss 0.0790738, throughput 5.46832K wps\n",
      "[Epoch 51] train avg loss 0.0759271, train avg r2 -2.26726,throughput 4.7218K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 52 Batch 10/159] avg loss 0.142535, throughput 3.6783K wps\n",
      "[Epoch 52 Batch 20/159] avg loss 0.126767, throughput 4.15602K wps\n",
      "[Epoch 52 Batch 30/159] avg loss 0.0889395, throughput 4.42092K wps\n",
      "[Epoch 52 Batch 40/159] avg loss 0.0739321, throughput 4.45385K wps\n",
      "[Epoch 52 Batch 50/159] avg loss 0.05669, throughput 4.33507K wps\n",
      "[Epoch 52 Batch 60/159] avg loss 0.0458598, throughput 4.41798K wps\n",
      "[Epoch 52 Batch 70/159] avg loss 0.0337549, throughput 4.49147K wps\n",
      "[Epoch 52 Batch 80/159] avg loss 0.0483485, throughput 4.56378K wps\n",
      "[Epoch 52 Batch 90/159] avg loss 0.0438007, throughput 4.2985K wps\n",
      "[Epoch 52 Batch 100/159] avg loss 0.0387042, throughput 4.39537K wps\n",
      "[Epoch 52 Batch 110/159] avg loss 0.0694627, throughput 5.035K wps\n",
      "[Epoch 52 Batch 120/159] avg loss 0.0585476, throughput 5.30478K wps\n",
      "[Epoch 52 Batch 130/159] avg loss 0.0536391, throughput 5.37392K wps\n",
      "[Epoch 52 Batch 140/159] avg loss 0.0784204, throughput 5.44367K wps\n",
      "[Epoch 52 Batch 150/159] avg loss 0.0807319, throughput 5.2605K wps\n",
      "[Epoch 52] train avg loss 0.0746692, train avg r2 -2.18594,throughput 4.64137K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 53 Batch 10/159] avg loss 0.140252, throughput 3.69936K wps\n",
      "[Epoch 53 Batch 20/159] avg loss 0.124461, throughput 4.1965K wps\n",
      "[Epoch 53 Batch 30/159] avg loss 0.0864813, throughput 4.31477K wps\n",
      "[Epoch 53 Batch 40/159] avg loss 0.0711565, throughput 4.29178K wps\n",
      "[Epoch 53 Batch 50/159] avg loss 0.0544533, throughput 4.31802K wps\n",
      "[Epoch 53 Batch 60/159] avg loss 0.0437876, throughput 4.21271K wps\n",
      "[Epoch 53 Batch 70/159] avg loss 0.0324788, throughput 4.08202K wps\n",
      "[Epoch 53 Batch 80/159] avg loss 0.0478398, throughput 4.21624K wps\n",
      "[Epoch 53 Batch 90/159] avg loss 0.0448103, throughput 4.77927K wps\n",
      "[Epoch 53 Batch 100/159] avg loss 0.0393077, throughput 4.67857K wps\n",
      "[Epoch 53 Batch 110/159] avg loss 0.0698956, throughput 5.19834K wps\n",
      "[Epoch 53 Batch 120/159] avg loss 0.0593043, throughput 5.03352K wps\n",
      "[Epoch 53 Batch 130/159] avg loss 0.0543521, throughput 4.95401K wps\n",
      "[Epoch 53 Batch 140/159] avg loss 0.0795179, throughput 5.0329K wps\n",
      "[Epoch 53 Batch 150/159] avg loss 0.0815647, throughput 5.18734K wps\n",
      "[Epoch 53] train avg loss 0.0740676, train avg r2 -2.13165,throughput 4.54363K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 54 Batch 10/159] avg loss 0.139035, throughput 3.54495K wps\n",
      "[Epoch 54 Batch 20/159] avg loss 0.123234, throughput 3.89394K wps\n",
      "[Epoch 54 Batch 30/159] avg loss 0.0851626, throughput 4.01341K wps\n",
      "[Epoch 54 Batch 40/159] avg loss 0.0697423, throughput 4.02684K wps\n",
      "[Epoch 54 Batch 50/159] avg loss 0.0532351, throughput 4.18121K wps\n",
      "[Epoch 54 Batch 60/159] avg loss 0.0427938, throughput 4.07986K wps\n",
      "[Epoch 54 Batch 70/159] avg loss 0.0317948, throughput 4.00091K wps\n",
      "[Epoch 54 Batch 80/159] avg loss 0.0475415, throughput 4.33563K wps\n",
      "[Epoch 54 Batch 90/159] avg loss 0.0452916, throughput 4.6073K wps\n",
      "[Epoch 54 Batch 100/159] avg loss 0.039711, throughput 4.66698K wps\n",
      "[Epoch 54 Batch 110/159] avg loss 0.070306, throughput 4.90261K wps\n",
      "[Epoch 54 Batch 120/159] avg loss 0.0597114, throughput 4.86641K wps\n",
      "[Epoch 54 Batch 130/159] avg loss 0.0547095, throughput 4.97317K wps\n",
      "[Epoch 54 Batch 140/159] avg loss 0.0799773, throughput 4.96009K wps\n",
      "[Epoch 54 Batch 150/159] avg loss 0.082094, throughput 5.03488K wps\n",
      "[Epoch 54] train avg loss 0.0737574, train avg r2 -2.08771,throughput 4.40854K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 55 Batch 10/159] avg loss 0.138303, throughput 3.60399K wps\n",
      "[Epoch 55 Batch 20/159] avg loss 0.122395, throughput 3.86997K wps\n",
      "[Epoch 55 Batch 30/159] avg loss 0.084293, throughput 4.0585K wps\n",
      "[Epoch 55 Batch 40/159] avg loss 0.0689052, throughput 4.24481K wps\n",
      "[Epoch 55 Batch 50/159] avg loss 0.052469, throughput 4.15099K wps\n",
      "[Epoch 55 Batch 60/159] avg loss 0.0422835, throughput 4.12281K wps\n",
      "[Epoch 55 Batch 70/159] avg loss 0.0312883, throughput 4.14869K wps\n",
      "[Epoch 55 Batch 80/159] avg loss 0.047323, throughput 3.9671K wps\n",
      "[Epoch 55 Batch 90/159] avg loss 0.0457338, throughput 4.32485K wps\n",
      "[Epoch 55 Batch 100/159] avg loss 0.0399333, throughput 4.72309K wps\n",
      "[Epoch 55 Batch 110/159] avg loss 0.0702569, throughput 4.98383K wps\n",
      "[Epoch 55 Batch 120/159] avg loss 0.0597284, throughput 4.92083K wps\n",
      "[Epoch 55 Batch 130/159] avg loss 0.054861, throughput 4.92554K wps\n",
      "[Epoch 55 Batch 140/159] avg loss 0.0801177, throughput 5.0401K wps\n",
      "[Epoch 55 Batch 150/159] avg loss 0.0823924, throughput 5.22922K wps\n",
      "[Epoch 55] train avg loss 0.0735121, train avg r2 -2.06655,throughput 4.41422K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 56 Batch 10/159] avg loss 0.13791, throughput 3.57935K wps\n",
      "[Epoch 56 Batch 20/159] avg loss 0.121932, throughput 4.05349K wps\n",
      "[Epoch 56 Batch 30/159] avg loss 0.0838529, throughput 3.98691K wps\n",
      "[Epoch 56 Batch 40/159] avg loss 0.0685437, throughput 4.08891K wps\n",
      "[Epoch 56 Batch 50/159] avg loss 0.0521328, throughput 4.14879K wps\n",
      "[Epoch 56 Batch 60/159] avg loss 0.0422367, throughput 4.14495K wps\n",
      "[Epoch 56 Batch 70/159] avg loss 0.0310557, throughput 4.11411K wps\n",
      "[Epoch 56 Batch 80/159] avg loss 0.0472351, throughput 4.38117K wps\n",
      "[Epoch 56 Batch 90/159] avg loss 0.0455421, throughput 4.82152K wps\n",
      "[Epoch 56 Batch 100/159] avg loss 0.0400046, throughput 4.94522K wps\n",
      "[Epoch 56 Batch 110/159] avg loss 0.0705962, throughput 5.02203K wps\n",
      "[Epoch 56 Batch 120/159] avg loss 0.0600362, throughput 4.97239K wps\n",
      "[Epoch 56 Batch 130/159] avg loss 0.0550979, throughput 4.86854K wps\n",
      "[Epoch 56 Batch 140/159] avg loss 0.0804476, throughput 5.05757K wps\n",
      "[Epoch 56 Batch 150/159] avg loss 0.0827414, throughput 5.15854K wps\n",
      "[Epoch 56] train avg loss 0.0734705, train avg r2 -2.0359,throughput 4.48152K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 57 Batch 10/159] avg loss 0.137471, throughput 3.60706K wps\n",
      "[Epoch 57 Batch 20/159] avg loss 0.121461, throughput 3.94059K wps\n",
      "[Epoch 57 Batch 30/159] avg loss 0.0834018, throughput 4.18413K wps\n",
      "[Epoch 57 Batch 40/159] avg loss 0.0681001, throughput 4.22254K wps\n",
      "[Epoch 57 Batch 50/159] avg loss 0.0517621, throughput 4.20594K wps\n",
      "[Epoch 57 Batch 60/159] avg loss 0.0419927, throughput 4.28058K wps\n",
      "[Epoch 57 Batch 70/159] avg loss 0.0307563, throughput 4.37771K wps\n",
      "[Epoch 57 Batch 80/159] avg loss 0.0471418, throughput 4.49018K wps\n",
      "[Epoch 57 Batch 90/159] avg loss 0.0457422, throughput 4.77605K wps\n",
      "[Epoch 57 Batch 100/159] avg loss 0.0401837, throughput 4.8749K wps\n",
      "[Epoch 57 Batch 110/159] avg loss 0.0708442, throughput 5.04351K wps\n",
      "[Epoch 57 Batch 120/159] avg loss 0.060222, throughput 4.89781K wps\n",
      "[Epoch 57 Batch 130/159] avg loss 0.055323, throughput 5.03175K wps\n",
      "[Epoch 57 Batch 140/159] avg loss 0.0807499, throughput 5.20803K wps\n",
      "[Epoch 57 Batch 150/159] avg loss 0.0830139, throughput 5.44398K wps\n",
      "[Epoch 57] train avg loss 0.0734101, train avg r2 -2.02237,throughput 4.55784K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 58 Batch 10/159] avg loss 0.137061, throughput 3.61394K wps\n",
      "[Epoch 58 Batch 20/159] avg loss 0.121066, throughput 4.11557K wps\n",
      "[Epoch 58 Batch 30/159] avg loss 0.0830148, throughput 4.27394K wps\n",
      "[Epoch 58 Batch 40/159] avg loss 0.0677101, throughput 4.28199K wps\n",
      "[Epoch 58 Batch 50/159] avg loss 0.0514225, throughput 4.28382K wps\n",
      "[Epoch 58 Batch 60/159] avg loss 0.0417894, throughput 4.43532K wps\n",
      "[Epoch 58 Batch 70/159] avg loss 0.0305947, throughput 4.05901K wps\n",
      "[Epoch 58 Batch 80/159] avg loss 0.0470793, throughput 4.18502K wps\n",
      "[Epoch 58 Batch 90/159] avg loss 0.0457947, throughput 4.62485K wps\n",
      "[Epoch 58 Batch 100/159] avg loss 0.0401965, throughput 4.19359K wps\n",
      "[Epoch 58 Batch 110/159] avg loss 0.0708718, throughput 5.42371K wps\n",
      "[Epoch 58 Batch 120/159] avg loss 0.0603402, throughput 5.29669K wps\n",
      "[Epoch 58 Batch 130/159] avg loss 0.0554019, throughput 5.51258K wps\n",
      "[Epoch 58 Batch 140/159] avg loss 0.0809335, throughput 5.3674K wps\n",
      "[Epoch 58 Batch 150/159] avg loss 0.0831621, throughput 5.51113K wps\n",
      "[Epoch 58] train avg loss 0.0733103, train avg r2 -1.99304,throughput 4.58342K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 59 Batch 10/159] avg loss 0.13671, throughput 3.64462K wps\n",
      "[Epoch 59 Batch 20/159] avg loss 0.120735, throughput 4.0946K wps\n",
      "[Epoch 59 Batch 30/159] avg loss 0.0827284, throughput 4.20752K wps\n",
      "[Epoch 59 Batch 40/159] avg loss 0.06746, throughput 4.17851K wps\n",
      "[Epoch 59 Batch 50/159] avg loss 0.0512531, throughput 4.32732K wps\n",
      "[Epoch 59 Batch 60/159] avg loss 0.0416783, throughput 4.27482K wps\n",
      "[Epoch 59 Batch 70/159] avg loss 0.0304459, throughput 4.22144K wps\n",
      "[Epoch 59 Batch 80/159] avg loss 0.0470437, throughput 4.51745K wps\n",
      "[Epoch 59 Batch 90/159] avg loss 0.0458347, throughput 4.89454K wps\n",
      "[Epoch 59 Batch 100/159] avg loss 0.0401645, throughput 5.00197K wps\n",
      "[Epoch 59 Batch 110/159] avg loss 0.0708805, throughput 5.11112K wps\n",
      "[Epoch 59 Batch 120/159] avg loss 0.0603985, throughput 4.89654K wps\n",
      "[Epoch 59 Batch 130/159] avg loss 0.0554982, throughput 5.19646K wps\n",
      "[Epoch 59 Batch 140/159] avg loss 0.0811212, throughput 5.26548K wps\n",
      "[Epoch 59 Batch 150/159] avg loss 0.0832894, throughput 5.19012K wps\n",
      "[Epoch 59] train avg loss 0.0732433, train avg r2 -1.97631,throughput 4.60354K wps\n",
      "learning rate: 3.125e-05\n",
      "[Epoch 60 Batch 10/159] avg loss 0.135943, throughput 3.75376K wps\n",
      "[Epoch 60 Batch 20/159] avg loss 0.121003, throughput 4.13944K wps\n",
      "[Epoch 60 Batch 30/159] avg loss 0.0859527, throughput 4.2983K wps\n",
      "[Epoch 60 Batch 40/159] avg loss 0.0738297, throughput 4.412K wps\n",
      "[Epoch 60 Batch 50/159] avg loss 0.0601863, throughput 4.36849K wps\n",
      "[Epoch 60 Batch 60/159] avg loss 0.0508378, throughput 4.24451K wps\n",
      "[Epoch 60 Batch 70/159] avg loss 0.0400172, throughput 4.28838K wps\n",
      "[Epoch 60 Batch 80/159] avg loss 0.0524464, throughput 4.72637K wps\n",
      "[Epoch 60 Batch 90/159] avg loss 0.0372805, throughput 5.00858K wps\n",
      "[Epoch 60 Batch 100/159] avg loss 0.0330568, throughput 5.1167K wps\n",
      "[Epoch 60 Batch 110/159] avg loss 0.0633052, throughput 5.23093K wps\n",
      "[Epoch 60 Batch 120/159] avg loss 0.0570373, throughput 5.13798K wps\n",
      "[Epoch 60 Batch 130/159] avg loss 0.0542893, throughput 5.33375K wps\n",
      "[Epoch 60 Batch 140/159] avg loss 0.079941, throughput 5.34911K wps\n",
      "[Epoch 60 Batch 150/159] avg loss 0.0842965, throughput 5.47524K wps\n",
      "[Epoch 60] train avg loss 0.0743454, train avg r2 -1.83968,throughput 4.71418K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 61 Batch 10/159] avg loss 0.130888, throughput 3.69969K wps\n",
      "[Epoch 61 Batch 20/159] avg loss 0.11522, throughput 4.08097K wps\n",
      "[Epoch 61 Batch 30/159] avg loss 0.0795401, throughput 4.22227K wps\n",
      "[Epoch 61 Batch 40/159] avg loss 0.0676579, throughput 4.3692K wps\n",
      "[Epoch 61 Batch 50/159] avg loss 0.0547796, throughput 4.2812K wps\n",
      "[Epoch 61 Batch 60/159] avg loss 0.0464855, throughput 4.31533K wps\n",
      "[Epoch 61 Batch 70/159] avg loss 0.0362217, throughput 4.4126K wps\n",
      "[Epoch 61 Batch 80/159] avg loss 0.050316, throughput 4.03436K wps\n",
      "[Epoch 61 Batch 90/159] avg loss 0.0388229, throughput 4.4601K wps\n",
      "[Epoch 61 Batch 100/159] avg loss 0.0348554, throughput 4.92456K wps\n",
      "[Epoch 61 Batch 110/159] avg loss 0.0665978, throughput 5.23825K wps\n",
      "[Epoch 61 Batch 120/159] avg loss 0.0588901, throughput 5.30154K wps\n",
      "[Epoch 61 Batch 130/159] avg loss 0.0560057, throughput 5.33604K wps\n",
      "[Epoch 61 Batch 140/159] avg loss 0.0824505, throughput 5.49621K wps\n",
      "[Epoch 61 Batch 150/159] avg loss 0.0862203, throughput 5.51495K wps\n",
      "[Epoch 61] train avg loss 0.0729376, train avg r2 -1.78053,throughput 4.63374K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 62 Batch 10/159] avg loss 0.128549, throughput 3.70964K wps\n",
      "[Epoch 62 Batch 20/159] avg loss 0.112919, throughput 4.15137K wps\n",
      "[Epoch 62 Batch 30/159] avg loss 0.0770257, throughput 4.38432K wps\n",
      "[Epoch 62 Batch 40/159] avg loss 0.0648671, throughput 4.32675K wps\n",
      "[Epoch 62 Batch 50/159] avg loss 0.0525665, throughput 4.26837K wps\n",
      "[Epoch 62 Batch 60/159] avg loss 0.0443783, throughput 4.36197K wps\n",
      "[Epoch 62 Batch 70/159] avg loss 0.0346056, throughput 4.50008K wps\n",
      "[Epoch 62 Batch 80/159] avg loss 0.0495262, throughput 4.65533K wps\n",
      "[Epoch 62 Batch 90/159] avg loss 0.0399415, throughput 5.02964K wps\n",
      "[Epoch 62 Batch 100/159] avg loss 0.0356752, throughput 5.00101K wps\n",
      "[Epoch 62 Batch 110/159] avg loss 0.0677646, throughput 5.30712K wps\n",
      "[Epoch 62 Batch 120/159] avg loss 0.0600871, throughput 5.27268K wps\n",
      "[Epoch 62 Batch 130/159] avg loss 0.0570338, throughput 5.34941K wps\n",
      "[Epoch 62 Batch 140/159] avg loss 0.0841034, throughput 5.21642K wps\n",
      "[Epoch 62 Batch 150/159] avg loss 0.0874543, throughput 5.62652K wps\n",
      "[Epoch 62] train avg loss 0.0724919, train avg r2 -1.74315,throughput 4.7264K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 63 Batch 10/159] avg loss 0.127113, throughput 3.70332K wps\n",
      "[Epoch 63 Batch 20/159] avg loss 0.11149, throughput 4.17321K wps\n",
      "[Epoch 63 Batch 30/159] avg loss 0.0755012, throughput 4.2688K wps\n",
      "[Epoch 63 Batch 40/159] avg loss 0.0631636, throughput 4.30351K wps\n",
      "[Epoch 63 Batch 50/159] avg loss 0.0512562, throughput 4.32476K wps\n",
      "[Epoch 63 Batch 60/159] avg loss 0.0431566, throughput 4.32909K wps\n",
      "[Epoch 63 Batch 70/159] avg loss 0.0337076, throughput 4.3395K wps\n",
      "[Epoch 63 Batch 80/159] avg loss 0.049077, throughput 4.59968K wps\n",
      "[Epoch 63 Batch 90/159] avg loss 0.0405832, throughput 4.93418K wps\n",
      "[Epoch 63 Batch 100/159] avg loss 0.0361986, throughput 4.95859K wps\n",
      "[Epoch 63 Batch 110/159] avg loss 0.0683747, throughput 5.17224K wps\n",
      "[Epoch 63 Batch 120/159] avg loss 0.0607576, throughput 5.11699K wps\n",
      "[Epoch 63 Batch 130/159] avg loss 0.0576001, throughput 4.99982K wps\n",
      "[Epoch 63 Batch 140/159] avg loss 0.0850025, throughput 5.26281K wps\n",
      "[Epoch 63 Batch 150/159] avg loss 0.0881612, throughput 5.35161K wps\n",
      "[Epoch 63] train avg loss 0.0722028, train avg r2 -1.71751,throughput 4.65242K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 64 Batch 10/159] avg loss 0.126224, throughput 3.7383K wps\n",
      "[Epoch 64 Batch 20/159] avg loss 0.110568, throughput 4.05912K wps\n",
      "[Epoch 64 Batch 30/159] avg loss 0.0745316, throughput 4.22405K wps\n",
      "[Epoch 64 Batch 40/159] avg loss 0.0621252, throughput 4.21611K wps\n",
      "[Epoch 64 Batch 50/159] avg loss 0.0504148, throughput 4.32053K wps\n",
      "[Epoch 64 Batch 60/159] avg loss 0.0424442, throughput 4.24316K wps\n",
      "[Epoch 64 Batch 70/159] avg loss 0.0331123, throughput 4.30197K wps\n",
      "[Epoch 64 Batch 80/159] avg loss 0.0487468, throughput 4.48602K wps\n",
      "[Epoch 64 Batch 90/159] avg loss 0.041032, throughput 4.35431K wps\n",
      "[Epoch 64 Batch 100/159] avg loss 0.0365161, throughput 4.5991K wps\n",
      "[Epoch 64 Batch 110/159] avg loss 0.0687266, throughput 4.83895K wps\n",
      "[Epoch 64 Batch 120/159] avg loss 0.0612747, throughput 5.16031K wps\n",
      "[Epoch 64 Batch 130/159] avg loss 0.058069, throughput 5.11657K wps\n",
      "[Epoch 64 Batch 140/159] avg loss 0.0856033, throughput 5.38169K wps\n",
      "[Epoch 64 Batch 150/159] avg loss 0.0886724, throughput 5.23863K wps\n",
      "[Epoch 64] train avg loss 0.0720407, train avg r2 -1.70472,throughput 4.55222K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 65 Batch 10/159] avg loss 0.125594, throughput 3.78167K wps\n",
      "[Epoch 65 Batch 20/159] avg loss 0.10991, throughput 4.1356K wps\n",
      "[Epoch 65 Batch 30/159] avg loss 0.0738531, throughput 4.24703K wps\n",
      "[Epoch 65 Batch 40/159] avg loss 0.0613707, throughput 4.00089K wps\n",
      "[Epoch 65 Batch 50/159] avg loss 0.049768, throughput 4.28811K wps\n",
      "[Epoch 65 Batch 60/159] avg loss 0.041956, throughput 4.28495K wps\n",
      "[Epoch 65 Batch 70/159] avg loss 0.0326936, throughput 4.4743K wps\n",
      "[Epoch 65 Batch 80/159] avg loss 0.0485047, throughput 4.42159K wps\n",
      "[Epoch 65 Batch 90/159] avg loss 0.0412977, throughput 4.7356K wps\n",
      "[Epoch 65 Batch 100/159] avg loss 0.0367417, throughput 5.09802K wps\n",
      "[Epoch 65 Batch 110/159] avg loss 0.0689374, throughput 4.97635K wps\n",
      "[Epoch 65 Batch 120/159] avg loss 0.0615495, throughput 4.96113K wps\n",
      "[Epoch 65 Batch 130/159] avg loss 0.0583373, throughput 5.13421K wps\n",
      "[Epoch 65 Batch 140/159] avg loss 0.0859685, throughput 5.29485K wps\n",
      "[Epoch 65 Batch 150/159] avg loss 0.0890326, throughput 5.19571K wps\n",
      "[Epoch 65] train avg loss 0.0718988, train avg r2 -1.68899,throughput 4.59926K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 66 Batch 10/159] avg loss 0.125151, throughput 3.68134K wps\n",
      "[Epoch 66 Batch 20/159] avg loss 0.109423, throughput 4.08142K wps\n",
      "[Epoch 66 Batch 30/159] avg loss 0.0733319, throughput 4.2255K wps\n",
      "[Epoch 66 Batch 40/159] avg loss 0.0607259, throughput 4.36159K wps\n",
      "[Epoch 66 Batch 50/159] avg loss 0.049163, throughput 4.2811K wps\n",
      "[Epoch 66 Batch 60/159] avg loss 0.0414062, throughput 4.30773K wps\n",
      "[Epoch 66 Batch 70/159] avg loss 0.0322372, throughput 4.34278K wps\n",
      "[Epoch 66 Batch 80/159] avg loss 0.0482083, throughput 4.53488K wps\n",
      "[Epoch 66 Batch 90/159] avg loss 0.0417702, throughput 4.98147K wps\n",
      "[Epoch 66 Batch 100/159] avg loss 0.0371083, throughput 4.89602K wps\n",
      "[Epoch 66 Batch 110/159] avg loss 0.0694048, throughput 5.27341K wps\n",
      "[Epoch 66 Batch 120/159] avg loss 0.0618586, throughput 5.24688K wps\n",
      "[Epoch 66 Batch 130/159] avg loss 0.0585656, throughput 5.14729K wps\n",
      "[Epoch 66 Batch 140/159] avg loss 0.0862029, throughput 5.33368K wps\n",
      "[Epoch 66 Batch 150/159] avg loss 0.0892272, throughput 5.37573K wps\n",
      "[Epoch 66] train avg loss 0.0717982, train avg r2 -1.68314,throughput 4.65601K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 67 Batch 10/159] avg loss 0.124912, throughput 3.80054K wps\n",
      "[Epoch 67 Batch 20/159] avg loss 0.109163, throughput 4.13437K wps\n",
      "[Epoch 67 Batch 30/159] avg loss 0.073118, throughput 4.23422K wps\n",
      "[Epoch 67 Batch 40/159] avg loss 0.0605081, throughput 4.23478K wps\n",
      "[Epoch 67 Batch 50/159] avg loss 0.0489119, throughput 4.39038K wps\n",
      "[Epoch 67 Batch 60/159] avg loss 0.0411841, throughput 4.36564K wps\n",
      "[Epoch 67 Batch 70/159] avg loss 0.03193, throughput 4.28837K wps\n",
      "[Epoch 67 Batch 80/159] avg loss 0.0479257, throughput 4.41712K wps\n",
      "[Epoch 67 Batch 90/159] avg loss 0.0419033, throughput 4.727K wps\n",
      "[Epoch 67 Batch 100/159] avg loss 0.0371835, throughput 4.4687K wps\n",
      "[Epoch 67 Batch 110/159] avg loss 0.0694496, throughput 4.78995K wps\n",
      "[Epoch 67 Batch 120/159] avg loss 0.0619182, throughput 5.24009K wps\n",
      "[Epoch 67 Batch 130/159] avg loss 0.0586306, throughput 5.26726K wps\n",
      "[Epoch 67 Batch 140/159] avg loss 0.0862288, throughput 5.53989K wps\n",
      "[Epoch 67 Batch 150/159] avg loss 0.0893311, throughput 5.35333K wps\n",
      "[Epoch 67] train avg loss 0.0717052, train avg r2 -1.67101,throughput 4.62221K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 68 Batch 10/159] avg loss 0.124642, throughput 3.68307K wps\n",
      "[Epoch 68 Batch 20/159] avg loss 0.108786, throughput 4.15867K wps\n",
      "[Epoch 68 Batch 30/159] avg loss 0.072671, throughput 4.271K wps\n",
      "[Epoch 68 Batch 40/159] avg loss 0.0599766, throughput 4.30519K wps\n",
      "[Epoch 68 Batch 50/159] avg loss 0.0483516, throughput 4.33338K wps\n",
      "[Epoch 68 Batch 60/159] avg loss 0.0406441, throughput 4.36954K wps\n",
      "[Epoch 68 Batch 70/159] avg loss 0.0314343, throughput 4.4102K wps\n",
      "[Epoch 68 Batch 80/159] avg loss 0.047614, throughput 4.58721K wps\n",
      "[Epoch 68 Batch 90/159] avg loss 0.042115, throughput 4.66309K wps\n",
      "[Epoch 68 Batch 100/159] avg loss 0.0371999, throughput 4.84436K wps\n",
      "[Epoch 68 Batch 110/159] avg loss 0.0694288, throughput 5.31925K wps\n",
      "[Epoch 68 Batch 120/159] avg loss 0.0620236, throughput 4.92776K wps\n",
      "[Epoch 68 Batch 130/159] avg loss 0.0587177, throughput 5.24191K wps\n",
      "[Epoch 68 Batch 140/159] avg loss 0.0862268, throughput 5.26083K wps\n",
      "[Epoch 68 Batch 150/159] avg loss 0.0892733, throughput 5.40275K wps\n",
      "[Epoch 68] train avg loss 0.0714882, train avg r2 -1.64685,throughput 4.6384K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 69 Batch 10/159] avg loss 0.124288, throughput 3.66539K wps\n",
      "[Epoch 69 Batch 20/159] avg loss 0.108289, throughput 3.99008K wps\n",
      "[Epoch 69 Batch 30/159] avg loss 0.0720913, throughput 4.10259K wps\n",
      "[Epoch 69 Batch 40/159] avg loss 0.0592692, throughput 4.23682K wps\n",
      "[Epoch 69 Batch 50/159] avg loss 0.0476875, throughput 4.2855K wps\n",
      "[Epoch 69 Batch 60/159] avg loss 0.0398566, throughput 4.22566K wps\n",
      "[Epoch 69 Batch 70/159] avg loss 0.0307591, throughput 4.34161K wps\n",
      "[Epoch 69 Batch 80/159] avg loss 0.0469877, throughput 4.49264K wps\n",
      "[Epoch 69 Batch 90/159] avg loss 0.0419492, throughput 4.73358K wps\n",
      "[Epoch 69 Batch 100/159] avg loss 0.037298, throughput 4.90428K wps\n",
      "[Epoch 69 Batch 110/159] avg loss 0.0693524, throughput 5.23835K wps\n",
      "[Epoch 69 Batch 120/159] avg loss 0.0617724, throughput 5.37775K wps\n",
      "[Epoch 69 Batch 130/159] avg loss 0.0584375, throughput 5.21241K wps\n",
      "[Epoch 69 Batch 140/159] avg loss 0.0858127, throughput 5.23456K wps\n",
      "[Epoch 69 Batch 150/159] avg loss 0.0889551, throughput 5.23495K wps\n",
      "[Epoch 69] train avg loss 0.0710566, train avg r2 -1.59487,throughput 4.60188K wps\n",
      "learning rate: 1.5625e-05\n",
      "[Epoch 70 Batch 10/159] avg loss 0.123358, throughput 3.67674K wps\n",
      "[Epoch 70 Batch 20/159] avg loss 0.10731, throughput 4.08544K wps\n",
      "[Epoch 70 Batch 30/159] avg loss 0.0725441, throughput 4.20646K wps\n",
      "[Epoch 70 Batch 40/159] avg loss 0.0612442, throughput 4.15739K wps\n",
      "[Epoch 70 Batch 50/159] avg loss 0.0509178, throughput 4.24137K wps\n",
      "[Epoch 70 Batch 60/159] avg loss 0.0429567, throughput 4.20229K wps\n",
      "[Epoch 70 Batch 70/159] avg loss 0.0341949, throughput 4.28129K wps\n",
      "[Epoch 70 Batch 80/159] avg loss 0.0492261, throughput 4.55368K wps\n",
      "[Epoch 70 Batch 90/159] avg loss 0.0371728, throughput 4.80532K wps\n",
      "[Epoch 70 Batch 100/159] avg loss 0.0329851, throughput 4.295K wps\n",
      "[Epoch 70 Batch 110/159] avg loss 0.0635341, throughput 4.58187K wps\n",
      "[Epoch 70 Batch 120/159] avg loss 0.0589355, throughput 4.92055K wps\n",
      "[Epoch 70 Batch 130/159] avg loss 0.0569947, throughput 5.15917K wps\n",
      "[Epoch 70 Batch 140/159] avg loss 0.0843177, throughput 5.28416K wps\n",
      "[Epoch 70 Batch 150/159] avg loss 0.0889411, throughput 5.49557K wps\n",
      "[Epoch 70] train avg loss 0.0705704, train avg r2 -1.44113,throughput 4.53111K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 71 Batch 10/159] avg loss 0.119734, throughput 3.57746K wps\n",
      "[Epoch 71 Batch 20/159] avg loss 0.101469, throughput 4.07383K wps\n",
      "[Epoch 71 Batch 30/159] avg loss 0.0635022, throughput 4.12621K wps\n",
      "[Epoch 71 Batch 40/159] avg loss 0.0504417, throughput 4.23467K wps\n",
      "[Epoch 71 Batch 50/159] avg loss 0.0394127, throughput 4.26842K wps\n",
      "[Epoch 71 Batch 60/159] avg loss 0.0335454, throughput 4.25306K wps\n",
      "[Epoch 71 Batch 70/159] avg loss 0.0237093, throughput 4.24744K wps\n",
      "[Epoch 71 Batch 80/159] avg loss 0.0409964, throughput 4.51253K wps\n",
      "[Epoch 71 Batch 90/159] avg loss 0.0396242, throughput 4.85717K wps\n",
      "[Epoch 71 Batch 100/159] avg loss 0.0334168, throughput 4.97781K wps\n",
      "[Epoch 71 Batch 110/159] avg loss 0.0624843, throughput 5.22053K wps\n",
      "[Epoch 71 Batch 120/159] avg loss 0.0579806, throughput 5.26175K wps\n",
      "[Epoch 71 Batch 130/159] avg loss 0.0555843, throughput 5.15449K wps\n",
      "[Epoch 71 Batch 140/159] avg loss 0.082274, throughput 5.25313K wps\n",
      "[Epoch 71 Batch 150/159] avg loss 0.0873745, throughput 5.39196K wps\n",
      "[Epoch 71] train avg loss 0.0657639, train avg r2 -1.05853,throughput 4.60249K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 72 Batch 10/159] avg loss 0.111021, throughput 3.72322K wps\n",
      "[Epoch 72 Batch 20/159] avg loss 0.0855243, throughput 3.98767K wps\n",
      "[Epoch 72 Batch 30/159] avg loss 0.0471252, throughput 4.21009K wps\n",
      "[Epoch 72 Batch 40/159] avg loss 0.0387229, throughput 4.05082K wps\n",
      "[Epoch 72 Batch 50/159] avg loss 0.0299456, throughput 4.20406K wps\n",
      "[Epoch 72 Batch 60/159] avg loss 0.032051, throughput 4.17485K wps\n",
      "[Epoch 72 Batch 70/159] avg loss 0.0213657, throughput 4.22758K wps\n",
      "[Epoch 72 Batch 80/159] avg loss 0.0403741, throughput 4.27115K wps\n",
      "[Epoch 72 Batch 90/159] avg loss 0.0363127, throughput 4.54649K wps\n",
      "[Epoch 72 Batch 100/159] avg loss 0.0319889, throughput 4.54708K wps\n",
      "[Epoch 72 Batch 110/159] avg loss 0.0599596, throughput 4.83455K wps\n",
      "[Epoch 72 Batch 120/159] avg loss 0.055583, throughput 4.96765K wps\n",
      "[Epoch 72 Batch 130/159] avg loss 0.0531196, throughput 4.86648K wps\n",
      "[Epoch 72 Batch 140/159] avg loss 0.0788816, throughput 5.00508K wps\n",
      "[Epoch 72 Batch 150/159] avg loss 0.0844372, throughput 5.11536K wps\n",
      "[Epoch 72] train avg loss 0.0600929, train avg r2 -0.778268,throughput 4.46387K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 73 Batch 10/159] avg loss 0.104442, throughput 3.71358K wps\n",
      "[Epoch 73 Batch 20/159] avg loss 0.0754914, throughput 4.04538K wps\n",
      "[Epoch 73 Batch 30/159] avg loss 0.0366543, throughput 4.20005K wps\n",
      "[Epoch 73 Batch 40/159] avg loss 0.0341359, throughput 4.22535K wps\n",
      "[Epoch 73 Batch 50/159] avg loss 0.0276594, throughput 4.34592K wps\n",
      "[Epoch 73 Batch 60/159] avg loss 0.0318546, throughput 4.35929K wps\n",
      "[Epoch 73 Batch 70/159] avg loss 0.0213187, throughput 4.29901K wps\n",
      "[Epoch 73 Batch 80/159] avg loss 0.0417323, throughput 4.57854K wps\n",
      "[Epoch 73 Batch 90/159] avg loss 0.0341495, throughput 4.825K wps\n",
      "[Epoch 73 Batch 100/159] avg loss 0.0319768, throughput 4.39765K wps\n",
      "[Epoch 73 Batch 110/159] avg loss 0.0579076, throughput 4.67749K wps\n",
      "[Epoch 73 Batch 120/159] avg loss 0.0531062, throughput 4.87624K wps\n",
      "[Epoch 73 Batch 130/159] avg loss 0.0503228, throughput 5.06486K wps\n",
      "[Epoch 73 Batch 140/159] avg loss 0.0748172, throughput 5.0199K wps\n",
      "[Epoch 73 Batch 150/159] avg loss 0.0808895, throughput 5.36885K wps\n",
      "[Epoch 73] train avg loss 0.0566517, train avg r2 -0.673349,throughput 4.54156K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 74 Batch 10/159] avg loss 0.100004, throughput 3.56766K wps\n",
      "[Epoch 74 Batch 20/159] avg loss 0.069905, throughput 4.05207K wps\n",
      "[Epoch 74 Batch 30/159] avg loss 0.0313732, throughput 4.13412K wps\n",
      "[Epoch 74 Batch 40/159] avg loss 0.0333829, throughput 4.32363K wps\n",
      "[Epoch 74 Batch 50/159] avg loss 0.0271002, throughput 4.30023K wps\n",
      "[Epoch 74 Batch 60/159] avg loss 0.0292119, throughput 4.20443K wps\n",
      "[Epoch 74 Batch 70/159] avg loss 0.0208335, throughput 4.29606K wps\n",
      "[Epoch 74 Batch 80/159] avg loss 0.0416997, throughput 4.5403K wps\n",
      "[Epoch 74 Batch 90/159] avg loss 0.0322634, throughput 4.74854K wps\n",
      "[Epoch 74 Batch 100/159] avg loss 0.0318062, throughput 4.88206K wps\n",
      "[Epoch 74 Batch 110/159] avg loss 0.0558711, throughput 5.24034K wps\n",
      "[Epoch 74 Batch 120/159] avg loss 0.0511411, throughput 5.06429K wps\n",
      "[Epoch 74 Batch 130/159] avg loss 0.0481237, throughput 5.13826K wps\n",
      "[Epoch 74 Batch 140/159] avg loss 0.0713389, throughput 5.30137K wps\n",
      "[Epoch 74 Batch 150/159] avg loss 0.0773697, throughput 5.29148K wps\n",
      "[Epoch 74] train avg loss 0.0541284, train avg r2 -0.512523,throughput 4.58887K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 75 Batch 10/159] avg loss 0.0939475, throughput 3.57131K wps\n",
      "[Epoch 75 Batch 20/159] avg loss 0.0636405, throughput 3.92881K wps\n",
      "[Epoch 75 Batch 30/159] avg loss 0.0257791, throughput 4.15925K wps\n",
      "[Epoch 75 Batch 40/159] avg loss 0.0346145, throughput 4.154K wps\n",
      "[Epoch 75 Batch 50/159] avg loss 0.026838, throughput 4.32104K wps\n",
      "[Epoch 75 Batch 60/159] avg loss 0.0278433, throughput 4.23215K wps\n",
      "[Epoch 75 Batch 70/159] avg loss 0.0212506, throughput 4.36846K wps\n",
      "[Epoch 75 Batch 80/159] avg loss 0.0420638, throughput 4.55714K wps\n",
      "[Epoch 75 Batch 90/159] avg loss 0.0302739, throughput 4.77228K wps\n",
      "[Epoch 75 Batch 100/159] avg loss 0.0316666, throughput 4.96117K wps\n",
      "[Epoch 75 Batch 110/159] avg loss 0.053948, throughput 5.1207K wps\n",
      "[Epoch 75 Batch 120/159] avg loss 0.0494241, throughput 5.24151K wps\n",
      "[Epoch 75 Batch 130/159] avg loss 0.046096, throughput 5.27461K wps\n",
      "[Epoch 75 Batch 140/159] avg loss 0.0679273, throughput 5.28873K wps\n",
      "[Epoch 75 Batch 150/159] avg loss 0.0738108, throughput 5.4251K wps\n",
      "[Epoch 75] train avg loss 0.0518127, train avg r2 -0.477012,throughput 4.60062K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 76 Batch 10/159] avg loss 0.0896048, throughput 3.6474K wps\n",
      "[Epoch 76 Batch 20/159] avg loss 0.0594589, throughput 4.10085K wps\n",
      "[Epoch 76 Batch 30/159] avg loss 0.0241367, throughput 4.22008K wps\n",
      "[Epoch 76 Batch 40/159] avg loss 0.0330975, throughput 4.26914K wps\n",
      "[Epoch 76 Batch 50/159] avg loss 0.0266911, throughput 4.27606K wps\n",
      "[Epoch 76 Batch 60/159] avg loss 0.0275819, throughput 4.20384K wps\n",
      "[Epoch 76 Batch 70/159] avg loss 0.0209403, throughput 4.29376K wps\n",
      "[Epoch 76 Batch 80/159] avg loss 0.0412717, throughput 4.53687K wps\n",
      "[Epoch 76 Batch 90/159] avg loss 0.0288674, throughput 4.8495K wps\n",
      "[Epoch 76 Batch 100/159] avg loss 0.0312127, throughput 4.37179K wps\n",
      "[Epoch 76 Batch 110/159] avg loss 0.0520025, throughput 4.73736K wps\n",
      "[Epoch 76 Batch 120/159] avg loss 0.0479706, throughput 4.89068K wps\n",
      "[Epoch 76 Batch 130/159] avg loss 0.0442344, throughput 5.23394K wps\n",
      "[Epoch 76 Batch 140/159] avg loss 0.0649684, throughput 5.22413K wps\n",
      "[Epoch 76 Batch 150/159] avg loss 0.0704864, throughput 5.47094K wps\n",
      "[Epoch 76] train avg loss 0.0498871, train avg r2 -0.431852,throughput 4.55899K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 77 Batch 10/159] avg loss 0.082887, throughput 3.6658K wps\n",
      "[Epoch 77 Batch 20/159] avg loss 0.0534235, throughput 4.00712K wps\n",
      "[Epoch 77 Batch 30/159] avg loss 0.0228849, throughput 4.1407K wps\n",
      "[Epoch 77 Batch 40/159] avg loss 0.0308704, throughput 4.12974K wps\n",
      "[Epoch 77 Batch 50/159] avg loss 0.0260347, throughput 4.09195K wps\n",
      "[Epoch 77 Batch 60/159] avg loss 0.0272438, throughput 4.2163K wps\n",
      "[Epoch 77 Batch 70/159] avg loss 0.0208066, throughput 4.27017K wps\n",
      "[Epoch 77 Batch 80/159] avg loss 0.0406896, throughput 4.56832K wps\n",
      "[Epoch 77 Batch 90/159] avg loss 0.0276172, throughput 4.85091K wps\n",
      "[Epoch 77 Batch 100/159] avg loss 0.0311062, throughput 5.0458K wps\n",
      "[Epoch 77 Batch 110/159] avg loss 0.0500265, throughput 5.04633K wps\n",
      "[Epoch 77 Batch 120/159] avg loss 0.0465019, throughput 5.18804K wps\n",
      "[Epoch 77 Batch 130/159] avg loss 0.0423282, throughput 5.26566K wps\n",
      "[Epoch 77 Batch 140/159] avg loss 0.0620599, throughput 5.43984K wps\n",
      "[Epoch 77 Batch 150/159] avg loss 0.0673177, throughput 5.48343K wps\n",
      "[Epoch 77] train avg loss 0.0476893, train avg r2 -0.372221,throughput 4.61309K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 78 Batch 10/159] avg loss 0.0721584, throughput 3.67863K wps\n",
      "[Epoch 78 Batch 20/159] avg loss 0.0461888, throughput 3.98108K wps\n",
      "[Epoch 78 Batch 30/159] avg loss 0.02119, throughput 4.243K wps\n",
      "[Epoch 78 Batch 40/159] avg loss 0.0281425, throughput 4.25927K wps\n",
      "[Epoch 78 Batch 50/159] avg loss 0.0256001, throughput 4.27864K wps\n",
      "[Epoch 78 Batch 60/159] avg loss 0.026967, throughput 4.19323K wps\n",
      "[Epoch 78 Batch 70/159] avg loss 0.0206471, throughput 4.40935K wps\n",
      "[Epoch 78 Batch 80/159] avg loss 0.0398395, throughput 4.47821K wps\n",
      "[Epoch 78 Batch 90/159] avg loss 0.0265589, throughput 5.03424K wps\n",
      "[Epoch 78 Batch 100/159] avg loss 0.0307504, throughput 4.9583K wps\n",
      "[Epoch 78 Batch 110/159] avg loss 0.0478654, throughput 4.91725K wps\n",
      "[Epoch 78 Batch 120/159] avg loss 0.0451552, throughput 5.21071K wps\n",
      "[Epoch 78 Batch 130/159] avg loss 0.0402855, throughput 5.06421K wps\n",
      "[Epoch 78 Batch 140/159] avg loss 0.0591896, throughput 5.01514K wps\n",
      "[Epoch 78 Batch 150/159] avg loss 0.0641018, throughput 5.05412K wps\n",
      "[Epoch 78] train avg loss 0.0450614, train avg r2 -0.3085,throughput 4.5827K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 79 Batch 10/159] avg loss 0.0587314, throughput 3.66783K wps\n",
      "[Epoch 79 Batch 20/159] avg loss 0.0424285, throughput 4.05108K wps\n",
      "[Epoch 79 Batch 30/159] avg loss 0.0212731, throughput 4.12765K wps\n",
      "[Epoch 79 Batch 40/159] avg loss 0.0259767, throughput 4.23994K wps\n",
      "[Epoch 79 Batch 50/159] avg loss 0.0248867, throughput 4.17196K wps\n",
      "[Epoch 79 Batch 60/159] avg loss 0.0266978, throughput 4.17377K wps\n",
      "[Epoch 79 Batch 70/159] avg loss 0.0200559, throughput 4.33252K wps\n",
      "[Epoch 79 Batch 80/159] avg loss 0.0407805, throughput 4.36848K wps\n",
      "[Epoch 79 Batch 90/159] avg loss 0.0257393, throughput 4.76804K wps\n",
      "[Epoch 79 Batch 100/159] avg loss 0.029854, throughput 4.36473K wps\n",
      "[Epoch 79 Batch 110/159] avg loss 0.0455314, throughput 4.5785K wps\n",
      "[Epoch 79 Batch 120/159] avg loss 0.0439266, throughput 4.86631K wps\n",
      "[Epoch 79 Batch 130/159] avg loss 0.0381718, throughput 4.98949K wps\n",
      "[Epoch 79 Batch 140/159] avg loss 0.0564134, throughput 5.10632K wps\n",
      "[Epoch 79 Batch 150/159] avg loss 0.0609862, throughput 5.28046K wps\n",
      "[Epoch 79] train avg loss 0.0426855, train avg r2 -0.24598,throughput 4.47679K wps\n",
      "learning rate: 7.8125e-06\n",
      "[Epoch 80 Batch 10/159] avg loss 0.0549379, throughput 3.52532K wps\n",
      "[Epoch 80 Batch 20/159] avg loss 0.040817, throughput 3.96447K wps\n",
      "[Epoch 80 Batch 30/159] avg loss 0.0223783, throughput 4.03663K wps\n",
      "[Epoch 80 Batch 40/159] avg loss 0.0247836, throughput 4.16213K wps\n",
      "[Epoch 80 Batch 50/159] avg loss 0.023731, throughput 4.0951K wps\n",
      "[Epoch 80 Batch 60/159] avg loss 0.0262709, throughput 4.1016K wps\n",
      "[Epoch 80 Batch 70/159] avg loss 0.0196934, throughput 4.23375K wps\n",
      "[Epoch 80 Batch 80/159] avg loss 0.040961, throughput 4.31523K wps\n",
      "[Epoch 80 Batch 90/159] avg loss 0.0255089, throughput 4.704K wps\n",
      "[Epoch 80 Batch 100/159] avg loss 0.0288198, throughput 4.86077K wps\n",
      "[Epoch 80 Batch 110/159] avg loss 0.0436499, throughput 4.98429K wps\n",
      "[Epoch 80 Batch 120/159] avg loss 0.0427507, throughput 4.99882K wps\n",
      "[Epoch 80 Batch 130/159] avg loss 0.0365789, throughput 5.06582K wps\n",
      "[Epoch 80 Batch 140/159] avg loss 0.0545926, throughput 5.14143K wps\n",
      "[Epoch 80 Batch 150/159] avg loss 0.0591884, throughput 5.11908K wps\n",
      "[Epoch 80] train avg loss 0.0414671, train avg r2 -0.13195,throughput 4.46916K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 81 Batch 10/159] avg loss 0.0520392, throughput 3.62155K wps\n",
      "[Epoch 81 Batch 20/159] avg loss 0.0401681, throughput 4.06K wps\n",
      "[Epoch 81 Batch 30/159] avg loss 0.0218587, throughput 4.17186K wps\n",
      "[Epoch 81 Batch 40/159] avg loss 0.0244699, throughput 4.15522K wps\n",
      "[Epoch 81 Batch 50/159] avg loss 0.0236884, throughput 4.20986K wps\n",
      "[Epoch 81 Batch 60/159] avg loss 0.0258572, throughput 4.16783K wps\n",
      "[Epoch 81 Batch 70/159] avg loss 0.0194402, throughput 4.31564K wps\n",
      "[Epoch 81 Batch 80/159] avg loss 0.0407239, throughput 4.44668K wps\n",
      "[Epoch 81 Batch 90/159] avg loss 0.0253435, throughput 4.8246K wps\n",
      "[Epoch 81 Batch 100/159] avg loss 0.028014, throughput 5.06993K wps\n",
      "[Epoch 81 Batch 110/159] avg loss 0.0427924, throughput 5.07687K wps\n",
      "[Epoch 81 Batch 120/159] avg loss 0.0424793, throughput 5.16562K wps\n",
      "[Epoch 81 Batch 130/159] avg loss 0.0358052, throughput 5.18837K wps\n",
      "[Epoch 81 Batch 140/159] avg loss 0.0536643, throughput 5.16076K wps\n",
      "[Epoch 81 Batch 150/159] avg loss 0.0581708, throughput 5.5275K wps\n",
      "[Epoch 81] train avg loss 0.0407033, train avg r2 -0.0492598,throughput 4.59423K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 82 Batch 10/159] avg loss 0.0509533, throughput 3.6532K wps\n",
      "[Epoch 82 Batch 20/159] avg loss 0.0397518, throughput 4.11208K wps\n",
      "[Epoch 82 Batch 30/159] avg loss 0.0217829, throughput 4.15686K wps\n",
      "[Epoch 82 Batch 40/159] avg loss 0.0242936, throughput 4.07106K wps\n",
      "[Epoch 82 Batch 50/159] avg loss 0.0236329, throughput 4.32529K wps\n",
      "[Epoch 82 Batch 60/159] avg loss 0.0255563, throughput 4.21479K wps\n",
      "[Epoch 82 Batch 70/159] avg loss 0.0192213, throughput 4.22455K wps\n",
      "[Epoch 82 Batch 80/159] avg loss 0.040492, throughput 4.45954K wps\n",
      "[Epoch 82 Batch 90/159] avg loss 0.0252872, throughput 4.51878K wps\n",
      "[Epoch 82 Batch 100/159] avg loss 0.0273793, throughput 4.3946K wps\n",
      "[Epoch 82 Batch 110/159] avg loss 0.0419692, throughput 4.72131K wps\n",
      "[Epoch 82 Batch 120/159] avg loss 0.0422348, throughput 5.06402K wps\n",
      "[Epoch 82 Batch 130/159] avg loss 0.0350538, throughput 5.0149K wps\n",
      "[Epoch 82 Batch 140/159] avg loss 0.0527239, throughput 5.08568K wps\n",
      "[Epoch 82 Batch 150/159] avg loss 0.0571454, throughput 5.25558K wps\n",
      "[Epoch 82] train avg loss 0.0401195, train avg r2 0.0462011,throughput 4.50057K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 83 Batch 10/159] avg loss 0.0503785, throughput 3.65941K wps\n",
      "[Epoch 83 Batch 20/159] avg loss 0.0393534, throughput 4.1023K wps\n",
      "[Epoch 83 Batch 30/159] avg loss 0.021823, throughput 4.16526K wps\n",
      "[Epoch 83 Batch 40/159] avg loss 0.024161, throughput 4.30268K wps\n",
      "[Epoch 83 Batch 50/159] avg loss 0.0235684, throughput 4.31184K wps\n",
      "[Epoch 83 Batch 60/159] avg loss 0.0253539, throughput 4.30602K wps\n",
      "[Epoch 83 Batch 70/159] avg loss 0.019049, throughput 4.33879K wps\n",
      "[Epoch 83 Batch 80/159] avg loss 0.0403969, throughput 4.46864K wps\n",
      "[Epoch 83 Batch 90/159] avg loss 0.0252626, throughput 4.89894K wps\n",
      "[Epoch 83 Batch 100/159] avg loss 0.0271038, throughput 4.91964K wps\n",
      "[Epoch 83 Batch 110/159] avg loss 0.0412233, throughput 4.95547K wps\n",
      "[Epoch 83 Batch 120/159] avg loss 0.0419259, throughput 4.97613K wps\n",
      "[Epoch 83 Batch 130/159] avg loss 0.0343801, throughput 5.12964K wps\n",
      "[Epoch 83 Batch 140/159] avg loss 0.0518599, throughput 5.36172K wps\n",
      "[Epoch 83 Batch 150/159] avg loss 0.0562434, throughput 5.4241K wps\n",
      "[Epoch 83] train avg loss 0.0396596, train avg r2 0.101204,throughput 4.61162K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 84 Batch 10/159] avg loss 0.0498861, throughput 3.73032K wps\n",
      "[Epoch 84 Batch 20/159] avg loss 0.0389325, throughput 3.96961K wps\n",
      "[Epoch 84 Batch 30/159] avg loss 0.0219019, throughput 4.28254K wps\n",
      "[Epoch 84 Batch 40/159] avg loss 0.0240735, throughput 4.22351K wps\n",
      "[Epoch 84 Batch 50/159] avg loss 0.0234894, throughput 4.23911K wps\n",
      "[Epoch 84 Batch 60/159] avg loss 0.0252315, throughput 4.22665K wps\n",
      "[Epoch 84 Batch 70/159] avg loss 0.0189137, throughput 4.33372K wps\n",
      "[Epoch 84 Batch 80/159] avg loss 0.0402834, throughput 4.62841K wps\n",
      "[Epoch 84 Batch 90/159] avg loss 0.0252204, throughput 4.76926K wps\n",
      "[Epoch 84 Batch 100/159] avg loss 0.0268143, throughput 5.1284K wps\n",
      "[Epoch 84 Batch 110/159] avg loss 0.0405063, throughput 5.03851K wps\n",
      "[Epoch 84 Batch 120/159] avg loss 0.0416367, throughput 5.22447K wps\n",
      "[Epoch 84 Batch 130/159] avg loss 0.0337693, throughput 5.23407K wps\n",
      "[Epoch 84 Batch 140/159] avg loss 0.051073, throughput 5.27121K wps\n",
      "[Epoch 84 Batch 150/159] avg loss 0.0553966, throughput 5.30687K wps\n",
      "[Epoch 84] train avg loss 0.0392545, train avg r2 0.114469,throughput 4.63624K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 85 Batch 10/159] avg loss 0.0494833, throughput 3.71431K wps\n",
      "[Epoch 85 Batch 20/159] avg loss 0.0385505, throughput 4.07573K wps\n",
      "[Epoch 85 Batch 30/159] avg loss 0.0218427, throughput 4.07391K wps\n",
      "[Epoch 85 Batch 40/159] avg loss 0.0239775, throughput 4.23864K wps\n",
      "[Epoch 85 Batch 50/159] avg loss 0.0234824, throughput 4.18405K wps\n",
      "[Epoch 85 Batch 60/159] avg loss 0.0250275, throughput 4.35819K wps\n",
      "[Epoch 85 Batch 70/159] avg loss 0.0187884, throughput 4.43795K wps\n",
      "[Epoch 85 Batch 80/159] avg loss 0.0401501, throughput 4.52179K wps\n",
      "[Epoch 85 Batch 90/159] avg loss 0.025299, throughput 5.13998K wps\n",
      "[Epoch 85 Batch 100/159] avg loss 0.0266053, throughput 4.44787K wps\n",
      "[Epoch 85 Batch 110/159] avg loss 0.0398222, throughput 4.49271K wps\n",
      "[Epoch 85 Batch 120/159] avg loss 0.0412854, throughput 5.14532K wps\n",
      "[Epoch 85 Batch 130/159] avg loss 0.0332066, throughput 5.27124K wps\n",
      "[Epoch 85 Batch 140/159] avg loss 0.0503382, throughput 5.45872K wps\n",
      "[Epoch 85 Batch 150/159] avg loss 0.054523, throughput 5.44015K wps\n",
      "[Epoch 85] train avg loss 0.038869, train avg r2 0.121578,throughput 4.59079K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 86 Batch 10/159] avg loss 0.0491016, throughput 3.6495K wps\n",
      "[Epoch 86 Batch 20/159] avg loss 0.038264, throughput 4.10725K wps\n",
      "[Epoch 86 Batch 30/159] avg loss 0.0218215, throughput 4.28484K wps\n",
      "[Epoch 86 Batch 40/159] avg loss 0.02388, throughput 4.33135K wps\n",
      "[Epoch 86 Batch 50/159] avg loss 0.0234451, throughput 4.37929K wps\n",
      "[Epoch 86 Batch 60/159] avg loss 0.0248989, throughput 4.28704K wps\n",
      "[Epoch 86 Batch 70/159] avg loss 0.0186917, throughput 4.51609K wps\n",
      "[Epoch 86 Batch 80/159] avg loss 0.0400437, throughput 4.63819K wps\n",
      "[Epoch 86 Batch 90/159] avg loss 0.0254142, throughput 5.09772K wps\n",
      "[Epoch 86 Batch 100/159] avg loss 0.0263939, throughput 5.08168K wps\n",
      "[Epoch 86 Batch 110/159] avg loss 0.0391967, throughput 5.29506K wps\n",
      "[Epoch 86 Batch 120/159] avg loss 0.0409291, throughput 5.2743K wps\n",
      "[Epoch 86 Batch 130/159] avg loss 0.0326758, throughput 5.30962K wps\n",
      "[Epoch 86 Batch 140/159] avg loss 0.0496548, throughput 5.28256K wps\n",
      "[Epoch 86 Batch 150/159] avg loss 0.0537115, throughput 5.54003K wps\n",
      "[Epoch 86] train avg loss 0.0385111, train avg r2 0.141235,throughput 4.72084K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 87 Batch 10/159] avg loss 0.0486683, throughput 3.62949K wps\n",
      "[Epoch 87 Batch 20/159] avg loss 0.0381508, throughput 4.04292K wps\n",
      "[Epoch 87 Batch 30/159] avg loss 0.0217126, throughput 4.15298K wps\n",
      "[Epoch 87 Batch 40/159] avg loss 0.0237864, throughput 4.2257K wps\n",
      "[Epoch 87 Batch 50/159] avg loss 0.0233755, throughput 4.14998K wps\n",
      "[Epoch 87 Batch 60/159] avg loss 0.0247556, throughput 4.23858K wps\n",
      "[Epoch 87 Batch 70/159] avg loss 0.0185549, throughput 4.19041K wps\n",
      "[Epoch 87 Batch 80/159] avg loss 0.0399242, throughput 4.49095K wps\n",
      "[Epoch 87 Batch 90/159] avg loss 0.0254875, throughput 4.8317K wps\n",
      "[Epoch 87 Batch 100/159] avg loss 0.0262263, throughput 4.93946K wps\n",
      "[Epoch 87 Batch 110/159] avg loss 0.0385935, throughput 5.34093K wps\n",
      "[Epoch 87 Batch 120/159] avg loss 0.0405518, throughput 5.13153K wps\n",
      "[Epoch 87 Batch 130/159] avg loss 0.03223, throughput 5.32177K wps\n",
      "[Epoch 87 Batch 140/159] avg loss 0.0490331, throughput 5.22001K wps\n",
      "[Epoch 87 Batch 150/159] avg loss 0.0529332, throughput 5.38256K wps\n",
      "[Epoch 87] train avg loss 0.0381641, train avg r2 0.156901,throughput 4.59561K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 88 Batch 10/159] avg loss 0.04845, throughput 3.64475K wps\n",
      "[Epoch 88 Batch 20/159] avg loss 0.0380532, throughput 4.14221K wps\n",
      "[Epoch 88 Batch 30/159] avg loss 0.0215794, throughput 4.2822K wps\n",
      "[Epoch 88 Batch 40/159] avg loss 0.0237264, throughput 4.28349K wps\n",
      "[Epoch 88 Batch 50/159] avg loss 0.0232845, throughput 4.29881K wps\n",
      "[Epoch 88 Batch 60/159] avg loss 0.0246322, throughput 4.3194K wps\n",
      "[Epoch 88 Batch 70/159] avg loss 0.0183879, throughput 4.37262K wps\n",
      "[Epoch 88 Batch 80/159] avg loss 0.0398359, throughput 4.67002K wps\n",
      "[Epoch 88 Batch 90/159] avg loss 0.0255429, throughput 4.84455K wps\n",
      "[Epoch 88 Batch 100/159] avg loss 0.0261736, throughput 4.56053K wps\n",
      "[Epoch 88 Batch 110/159] avg loss 0.0380179, throughput 4.52083K wps\n",
      "[Epoch 88 Batch 120/159] avg loss 0.0401925, throughput 4.74044K wps\n",
      "[Epoch 88 Batch 130/159] avg loss 0.0318187, throughput 5.09447K wps\n",
      "[Epoch 88 Batch 140/159] avg loss 0.0484644, throughput 5.19594K wps\n",
      "[Epoch 88 Batch 150/159] avg loss 0.0522034, throughput 5.37448K wps\n",
      "[Epoch 88] train avg loss 0.0378572, train avg r2 0.172556,throughput 4.55705K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 89 Batch 10/159] avg loss 0.0482125, throughput 3.60784K wps\n",
      "[Epoch 89 Batch 20/159] avg loss 0.0379261, throughput 4.05592K wps\n",
      "[Epoch 89 Batch 30/159] avg loss 0.021493, throughput 4.068K wps\n",
      "[Epoch 89 Batch 40/159] avg loss 0.0236448, throughput 4.1106K wps\n",
      "[Epoch 89 Batch 50/159] avg loss 0.0232143, throughput 4.21571K wps\n",
      "[Epoch 89 Batch 60/159] avg loss 0.024518, throughput 4.2528K wps\n",
      "[Epoch 89 Batch 70/159] avg loss 0.0182555, throughput 4.33866K wps\n",
      "[Epoch 89 Batch 80/159] avg loss 0.0398207, throughput 4.41193K wps\n",
      "[Epoch 89 Batch 90/159] avg loss 0.0255079, throughput 4.77212K wps\n",
      "[Epoch 89 Batch 100/159] avg loss 0.0261485, throughput 5.0778K wps\n",
      "[Epoch 89 Batch 110/159] avg loss 0.0374746, throughput 5.34425K wps\n",
      "[Epoch 89 Batch 120/159] avg loss 0.0398848, throughput 5.19012K wps\n",
      "[Epoch 89 Batch 130/159] avg loss 0.0314816, throughput 5.27333K wps\n",
      "[Epoch 89 Batch 140/159] avg loss 0.0479713, throughput 5.3258K wps\n",
      "[Epoch 89 Batch 150/159] avg loss 0.0514988, throughput 5.51201K wps\n",
      "[Epoch 89] train avg loss 0.0375818, train avg r2 0.175126,throughput 4.61411K wps\n",
      "learning rate: 3.90625e-06\n",
      "[Epoch 90 Batch 10/159] avg loss 0.0479595, throughput 3.69207K wps\n",
      "[Epoch 90 Batch 20/159] avg loss 0.0377295, throughput 4.12451K wps\n",
      "[Epoch 90 Batch 30/159] avg loss 0.0213424, throughput 4.31535K wps\n",
      "[Epoch 90 Batch 40/159] avg loss 0.0233945, throughput 4.322K wps\n",
      "[Epoch 90 Batch 50/159] avg loss 0.023219, throughput 4.24898K wps\n",
      "[Epoch 90 Batch 60/159] avg loss 0.0243219, throughput 4.17914K wps\n",
      "[Epoch 90 Batch 70/159] avg loss 0.0180911, throughput 4.28954K wps\n",
      "[Epoch 90 Batch 80/159] avg loss 0.0395022, throughput 4.54082K wps\n",
      "[Epoch 90 Batch 90/159] avg loss 0.0256511, throughput 4.98718K wps\n",
      "[Epoch 90 Batch 100/159] avg loss 0.0260727, throughput 5.00535K wps\n",
      "[Epoch 90 Batch 110/159] avg loss 0.0368877, throughput 5.02334K wps\n",
      "[Epoch 90 Batch 120/159] avg loss 0.039509, throughput 5.02367K wps\n",
      "[Epoch 90 Batch 130/159] avg loss 0.031024, throughput 5.25094K wps\n",
      "[Epoch 90 Batch 140/159] avg loss 0.0475596, throughput 5.38269K wps\n",
      "[Epoch 90 Batch 150/159] avg loss 0.0510058, throughput 5.60929K wps\n",
      "[Epoch 90] train avg loss 0.037283, train avg r2 0.21767,throughput 4.64466K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 91 Batch 10/159] avg loss 0.0474127, throughput 3.61719K wps\n",
      "[Epoch 91 Batch 20/159] avg loss 0.0377176, throughput 3.99762K wps\n",
      "[Epoch 91 Batch 30/159] avg loss 0.0209612, throughput 4.17802K wps\n",
      "[Epoch 91 Batch 40/159] avg loss 0.0233641, throughput 4.17841K wps\n",
      "[Epoch 91 Batch 50/159] avg loss 0.0231193, throughput 4.23116K wps\n",
      "[Epoch 91 Batch 60/159] avg loss 0.0242969, throughput 4.27209K wps\n",
      "[Epoch 91 Batch 70/159] avg loss 0.0180376, throughput 4.25027K wps\n",
      "[Epoch 91 Batch 80/159] avg loss 0.039284, throughput 4.37779K wps\n",
      "[Epoch 91 Batch 90/159] avg loss 0.025657, throughput 4.75K wps\n",
      "[Epoch 91 Batch 100/159] avg loss 0.0260264, throughput 4.83846K wps\n",
      "[Epoch 91 Batch 110/159] avg loss 0.0366523, throughput 4.49885K wps\n",
      "[Epoch 91 Batch 120/159] avg loss 0.0393845, throughput 4.78521K wps\n",
      "[Epoch 91 Batch 130/159] avg loss 0.0308898, throughput 4.9056K wps\n",
      "[Epoch 91 Batch 140/159] avg loss 0.0473688, throughput 5.21514K wps\n",
      "[Epoch 91 Batch 150/159] avg loss 0.0507261, throughput 5.23153K wps\n",
      "[Epoch 91] train avg loss 0.0371009, train avg r2 0.224123,throughput 4.48915K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 92 Batch 10/159] avg loss 0.0472033, throughput 3.68861K wps\n",
      "[Epoch 92 Batch 20/159] avg loss 0.0376234, throughput 4.09108K wps\n",
      "[Epoch 92 Batch 30/159] avg loss 0.0208806, throughput 4.11754K wps\n",
      "[Epoch 92 Batch 40/159] avg loss 0.0233433, throughput 4.13899K wps\n",
      "[Epoch 92 Batch 50/159] avg loss 0.0230612, throughput 4.1892K wps\n",
      "[Epoch 92 Batch 60/159] avg loss 0.0242538, throughput 4.13503K wps\n",
      "[Epoch 92 Batch 70/159] avg loss 0.017973, throughput 4.17584K wps\n",
      "[Epoch 92 Batch 80/159] avg loss 0.0391678, throughput 4.46765K wps\n",
      "[Epoch 92 Batch 90/159] avg loss 0.025661, throughput 4.73961K wps\n",
      "[Epoch 92 Batch 100/159] avg loss 0.0260125, throughput 4.89283K wps\n",
      "[Epoch 92 Batch 110/159] avg loss 0.0364321, throughput 5.10981K wps\n",
      "[Epoch 92 Batch 120/159] avg loss 0.0392905, throughput 5.02453K wps\n",
      "[Epoch 92 Batch 130/159] avg loss 0.0307469, throughput 4.99631K wps\n",
      "[Epoch 92 Batch 140/159] avg loss 0.0471768, throughput 5.07756K wps\n",
      "[Epoch 92 Batch 150/159] avg loss 0.0504302, throughput 5.12431K wps\n",
      "[Epoch 92] train avg loss 0.0369652, train avg r2 0.229121,throughput 4.52422K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 93 Batch 10/159] avg loss 0.047072, throughput 3.60448K wps\n",
      "[Epoch 93 Batch 20/159] avg loss 0.0375559, throughput 4.04928K wps\n",
      "[Epoch 93 Batch 30/159] avg loss 0.0208466, throughput 4.12573K wps\n",
      "[Epoch 93 Batch 40/159] avg loss 0.0232993, throughput 4.28971K wps\n",
      "[Epoch 93 Batch 50/159] avg loss 0.0230223, throughput 4.28882K wps\n",
      "[Epoch 93 Batch 60/159] avg loss 0.0242157, throughput 4.30571K wps\n",
      "[Epoch 93 Batch 70/159] avg loss 0.017928, throughput 4.38652K wps\n",
      "[Epoch 93 Batch 80/159] avg loss 0.0390202, throughput 4.62245K wps\n",
      "[Epoch 93 Batch 90/159] avg loss 0.0256623, throughput 5.02329K wps\n",
      "[Epoch 93 Batch 100/159] avg loss 0.0260125, throughput 4.91748K wps\n",
      "[Epoch 93 Batch 110/159] avg loss 0.0361807, throughput 5.27417K wps\n",
      "[Epoch 93 Batch 120/159] avg loss 0.0391881, throughput 5.07937K wps\n",
      "[Epoch 93 Batch 130/159] avg loss 0.0305901, throughput 5.13859K wps\n",
      "[Epoch 93 Batch 140/159] avg loss 0.046988, throughput 5.33045K wps\n",
      "[Epoch 93 Batch 150/159] avg loss 0.0501427, throughput 5.25497K wps\n",
      "[Epoch 93] train avg loss 0.0368358, train avg r2 0.233249,throughput 4.63076K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 94 Batch 10/159] avg loss 0.0469658, throughput 3.68181K wps\n",
      "[Epoch 94 Batch 20/159] avg loss 0.0374947, throughput 3.99601K wps\n",
      "[Epoch 94 Batch 30/159] avg loss 0.0208072, throughput 4.28071K wps\n",
      "[Epoch 94 Batch 40/159] avg loss 0.0232589, throughput 4.23969K wps\n",
      "[Epoch 94 Batch 50/159] avg loss 0.0229812, throughput 4.25625K wps\n",
      "[Epoch 94 Batch 60/159] avg loss 0.0241743, throughput 4.20951K wps\n",
      "[Epoch 94 Batch 70/159] avg loss 0.0178907, throughput 4.35354K wps\n",
      "[Epoch 94 Batch 80/159] avg loss 0.0388757, throughput 4.53386K wps\n",
      "[Epoch 94 Batch 90/159] avg loss 0.0256563, throughput 4.9931K wps\n",
      "[Epoch 94 Batch 100/159] avg loss 0.0260284, throughput 5.07539K wps\n",
      "[Epoch 94 Batch 110/159] avg loss 0.0359381, throughput 4.56794K wps\n",
      "[Epoch 94 Batch 120/159] avg loss 0.0390933, throughput 4.76483K wps\n",
      "[Epoch 94 Batch 130/159] avg loss 0.0304428, throughput 4.99934K wps\n",
      "[Epoch 94 Batch 140/159] avg loss 0.0468083, throughput 5.30667K wps\n",
      "[Epoch 94 Batch 150/159] avg loss 0.0498564, throughput 5.38635K wps\n",
      "[Epoch 94] train avg loss 0.0367139, train avg r2 0.2366,throughput 4.57122K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 95 Batch 10/159] avg loss 0.0468645, throughput 3.62667K wps\n",
      "[Epoch 95 Batch 20/159] avg loss 0.0374228, throughput 4.07371K wps\n",
      "[Epoch 95 Batch 30/159] avg loss 0.0207778, throughput 4.16514K wps\n",
      "[Epoch 95 Batch 40/159] avg loss 0.0232124, throughput 4.15044K wps\n",
      "[Epoch 95 Batch 50/159] avg loss 0.0229794, throughput 4.22427K wps\n",
      "[Epoch 95 Batch 60/159] avg loss 0.0241363, throughput 4.26195K wps\n",
      "[Epoch 95 Batch 70/159] avg loss 0.0178498, throughput 4.21844K wps\n",
      "[Epoch 95 Batch 80/159] avg loss 0.0387057, throughput 4.45571K wps\n",
      "[Epoch 95 Batch 90/159] avg loss 0.0256395, throughput 4.75601K wps\n",
      "[Epoch 95 Batch 100/159] avg loss 0.0260508, throughput 4.86396K wps\n",
      "[Epoch 95 Batch 110/159] avg loss 0.0357097, throughput 5.10168K wps\n",
      "[Epoch 95 Batch 120/159] avg loss 0.0390266, throughput 5.14241K wps\n",
      "[Epoch 95 Batch 130/159] avg loss 0.0303064, throughput 5.33117K wps\n",
      "[Epoch 95 Batch 140/159] avg loss 0.0466442, throughput 5.48327K wps\n",
      "[Epoch 95 Batch 150/159] avg loss 0.049582, throughput 5.38241K wps\n",
      "[Epoch 95] train avg loss 0.0365984, train avg r2 0.240682,throughput 4.60465K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 96 Batch 10/159] avg loss 0.0467473, throughput 3.73762K wps\n",
      "[Epoch 96 Batch 20/159] avg loss 0.0373695, throughput 4.1111K wps\n",
      "[Epoch 96 Batch 30/159] avg loss 0.0207442, throughput 4.27067K wps\n",
      "[Epoch 96 Batch 40/159] avg loss 0.0231761, throughput 4.21969K wps\n",
      "[Epoch 96 Batch 50/159] avg loss 0.0229573, throughput 4.25389K wps\n",
      "[Epoch 96 Batch 60/159] avg loss 0.024101, throughput 4.32287K wps\n",
      "[Epoch 96 Batch 70/159] avg loss 0.0178224, throughput 4.4622K wps\n",
      "[Epoch 96 Batch 80/159] avg loss 0.0385392, throughput 4.51168K wps\n",
      "[Epoch 96 Batch 90/159] avg loss 0.0256288, throughput 4.8877K wps\n",
      "[Epoch 96 Batch 100/159] avg loss 0.0260704, throughput 4.9769K wps\n",
      "[Epoch 96 Batch 110/159] avg loss 0.0354786, throughput 4.93301K wps\n",
      "[Epoch 96 Batch 120/159] avg loss 0.0389667, throughput 5.09985K wps\n",
      "[Epoch 96 Batch 130/159] avg loss 0.0301914, throughput 5.15078K wps\n",
      "[Epoch 96 Batch 140/159] avg loss 0.0464831, throughput 5.25256K wps\n",
      "[Epoch 96 Batch 150/159] avg loss 0.0493201, throughput 5.22225K wps\n",
      "[Epoch 96] train avg loss 0.0364871, train avg r2 0.24485,throughput 4.61818K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 97 Batch 10/159] avg loss 0.0466366, throughput 3.63686K wps\n",
      "[Epoch 97 Batch 20/159] avg loss 0.0373282, throughput 4.05678K wps\n",
      "[Epoch 97 Batch 30/159] avg loss 0.0207094, throughput 4.14157K wps\n",
      "[Epoch 97 Batch 40/159] avg loss 0.0231449, throughput 4.12166K wps\n",
      "[Epoch 97 Batch 50/159] avg loss 0.0229197, throughput 4.20676K wps\n",
      "[Epoch 97 Batch 60/159] avg loss 0.0240886, throughput 4.18608K wps\n",
      "[Epoch 97 Batch 70/159] avg loss 0.0177959, throughput 4.30518K wps\n",
      "[Epoch 97 Batch 80/159] avg loss 0.0383101, throughput 4.48399K wps\n",
      "[Epoch 97 Batch 90/159] avg loss 0.0256222, throughput 4.75581K wps\n",
      "[Epoch 97 Batch 100/159] avg loss 0.0260741, throughput 4.9493K wps\n",
      "[Epoch 97 Batch 110/159] avg loss 0.0352526, throughput 4.65698K wps\n",
      "[Epoch 97 Batch 120/159] avg loss 0.0388947, throughput 4.76953K wps\n",
      "[Epoch 97 Batch 130/159] avg loss 0.0300492, throughput 4.90121K wps\n",
      "[Epoch 97 Batch 140/159] avg loss 0.0463196, throughput 5.38602K wps\n",
      "[Epoch 97 Batch 150/159] avg loss 0.0490761, throughput 5.30093K wps\n",
      "[Epoch 97] train avg loss 0.0363714, train avg r2 0.250346,throughput 4.52193K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 98 Batch 10/159] avg loss 0.0465223, throughput 3.67484K wps\n",
      "[Epoch 98 Batch 20/159] avg loss 0.0372913, throughput 3.99849K wps\n",
      "[Epoch 98 Batch 30/159] avg loss 0.0206792, throughput 4.14486K wps\n",
      "[Epoch 98 Batch 40/159] avg loss 0.0231035, throughput 4.29147K wps\n",
      "[Epoch 98 Batch 50/159] avg loss 0.0229184, throughput 4.24771K wps\n",
      "[Epoch 98 Batch 60/159] avg loss 0.0240574, throughput 4.39582K wps\n",
      "[Epoch 98 Batch 70/159] avg loss 0.0177633, throughput 4.43106K wps\n",
      "[Epoch 98 Batch 80/159] avg loss 0.0380369, throughput 4.71052K wps\n",
      "[Epoch 98 Batch 90/159] avg loss 0.0256076, throughput 4.9048K wps\n",
      "[Epoch 98 Batch 100/159] avg loss 0.0260885, throughput 5.07394K wps\n",
      "[Epoch 98 Batch 110/159] avg loss 0.0350434, throughput 5.2266K wps\n",
      "[Epoch 98 Batch 120/159] avg loss 0.0388236, throughput 5.3131K wps\n",
      "[Epoch 98 Batch 130/159] avg loss 0.0299154, throughput 5.11109K wps\n",
      "[Epoch 98 Batch 140/159] avg loss 0.0461601, throughput 5.55041K wps\n",
      "[Epoch 98 Batch 150/159] avg loss 0.0488303, throughput 5.38999K wps\n",
      "[Epoch 98] train avg loss 0.0362556, train avg r2 0.25394,throughput 4.68272K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 99 Batch 10/159] avg loss 0.0463988, throughput 3.7925K wps\n",
      "[Epoch 99 Batch 20/159] avg loss 0.0372516, throughput 4.03357K wps\n",
      "[Epoch 99 Batch 30/159] avg loss 0.0206439, throughput 4.31842K wps\n",
      "[Epoch 99 Batch 40/159] avg loss 0.0230708, throughput 4.32356K wps\n",
      "[Epoch 99 Batch 50/159] avg loss 0.0229023, throughput 4.36911K wps\n",
      "[Epoch 99 Batch 60/159] avg loss 0.0240344, throughput 4.36397K wps\n",
      "[Epoch 99 Batch 70/159] avg loss 0.017739, throughput 4.44242K wps\n",
      "[Epoch 99 Batch 80/159] avg loss 0.0377898, throughput 4.55271K wps\n",
      "[Epoch 99 Batch 90/159] avg loss 0.0256064, throughput 5.04284K wps\n",
      "[Epoch 99 Batch 100/159] avg loss 0.0260955, throughput 5.04212K wps\n",
      "[Epoch 99 Batch 110/159] avg loss 0.0348222, throughput 5.32944K wps\n",
      "[Epoch 99 Batch 120/159] avg loss 0.0387657, throughput 5.2253K wps\n",
      "[Epoch 99 Batch 130/159] avg loss 0.0297959, throughput 5.3139K wps\n",
      "[Epoch 99 Batch 140/159] avg loss 0.0460079, throughput 5.35547K wps\n",
      "[Epoch 99 Batch 150/159] avg loss 0.0485964, throughput 5.62677K wps\n",
      "[Epoch 99] train avg loss 0.0361448, train avg r2 0.258265,throughput 4.7301K wps\n",
      "learning rate: 1.953125e-06\n",
      "[Epoch 100 Batch 10/159] avg loss 0.0462243, throughput 3.71052K wps\n",
      "[Epoch 100 Batch 20/159] avg loss 0.0371236, throughput 4.14569K wps\n",
      "[Epoch 100 Batch 30/159] avg loss 0.0205839, throughput 4.22246K wps\n",
      "[Epoch 100 Batch 40/159] avg loss 0.0229391, throughput 4.37729K wps\n",
      "[Epoch 100 Batch 50/159] avg loss 0.0229479, throughput 4.2715K wps\n",
      "[Epoch 100 Batch 60/159] avg loss 0.0238855, throughput 4.25662K wps\n",
      "[Epoch 100 Batch 70/159] avg loss 0.0176636, throughput 4.34016K wps\n",
      "[Epoch 100 Batch 80/159] avg loss 0.0373405, throughput 4.57931K wps\n",
      "[Epoch 100 Batch 90/159] avg loss 0.0255684, throughput 4.95845K wps\n",
      "[Epoch 100 Batch 100/159] avg loss 0.0260794, throughput 4.85287K wps\n",
      "[Epoch 100 Batch 110/159] avg loss 0.034576, throughput 5.14947K wps\n",
      "[Epoch 100 Batch 120/159] avg loss 0.0386675, throughput 4.70514K wps\n",
      "[Epoch 100 Batch 130/159] avg loss 0.0295624, throughput 4.66555K wps\n",
      "[Epoch 100 Batch 140/159] avg loss 0.0458618, throughput 4.91122K wps\n",
      "[Epoch 100 Batch 150/159] avg loss 0.0483647, throughput 5.40287K wps\n",
      "[Epoch 100] train avg loss 0.0359882, train avg r2 0.265383,throughput 4.58415K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 101 Batch 10/159] avg loss 0.0460886, throughput 3.63748K wps\n",
      "[Epoch 101 Batch 20/159] avg loss 0.037139, throughput 4.22111K wps\n",
      "[Epoch 101 Batch 30/159] avg loss 0.0204242, throughput 4.37842K wps\n",
      "[Epoch 101 Batch 40/159] avg loss 0.0229383, throughput 4.2327K wps\n",
      "[Epoch 101 Batch 50/159] avg loss 0.0228841, throughput 4.28614K wps\n",
      "[Epoch 101 Batch 60/159] avg loss 0.0239043, throughput 4.36146K wps\n",
      "[Epoch 101 Batch 70/159] avg loss 0.0176558, throughput 4.44984K wps\n",
      "[Epoch 101 Batch 80/159] avg loss 0.0372197, throughput 4.54868K wps\n",
      "[Epoch 101 Batch 90/159] avg loss 0.0255737, throughput 4.99651K wps\n",
      "[Epoch 101 Batch 100/159] avg loss 0.0260628, throughput 5.10524K wps\n",
      "[Epoch 101 Batch 110/159] avg loss 0.0344745, throughput 5.19803K wps\n",
      "[Epoch 101 Batch 120/159] avg loss 0.0386383, throughput 5.30391K wps\n",
      "[Epoch 101 Batch 130/159] avg loss 0.0295115, throughput 5.30849K wps\n",
      "[Epoch 101 Batch 140/159] avg loss 0.0457901, throughput 5.44104K wps\n",
      "[Epoch 101 Batch 150/159] avg loss 0.0482601, throughput 5.59063K wps\n",
      "[Epoch 101] train avg loss 0.0359232, train avg r2 0.267541,throughput 4.72301K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 102 Batch 10/159] avg loss 0.0460106, throughput 3.76894K wps\n",
      "[Epoch 102 Batch 20/159] avg loss 0.0371057, throughput 4.18708K wps\n",
      "[Epoch 102 Batch 30/159] avg loss 0.0203959, throughput 4.27619K wps\n",
      "[Epoch 102 Batch 40/159] avg loss 0.0229175, throughput 4.37935K wps\n",
      "[Epoch 102 Batch 50/159] avg loss 0.0228656, throughput 4.41625K wps\n",
      "[Epoch 102 Batch 60/159] avg loss 0.0238899, throughput 4.49206K wps\n",
      "[Epoch 102 Batch 70/159] avg loss 0.0176366, throughput 4.51545K wps\n",
      "[Epoch 102 Batch 80/159] avg loss 0.0370645, throughput 4.62782K wps\n",
      "[Epoch 102 Batch 90/159] avg loss 0.0255699, throughput 5.09938K wps\n",
      "[Epoch 102 Batch 100/159] avg loss 0.0260682, throughput 5.06057K wps\n",
      "[Epoch 102 Batch 110/159] avg loss 0.034378, throughput 5.33186K wps\n",
      "[Epoch 102 Batch 120/159] avg loss 0.0386094, throughput 5.40273K wps\n",
      "[Epoch 102 Batch 130/159] avg loss 0.029465, throughput 5.32043K wps\n",
      "[Epoch 102 Batch 140/159] avg loss 0.0457177, throughput 5.55714K wps\n",
      "[Epoch 102 Batch 150/159] avg loss 0.048158, throughput 5.75185K wps\n",
      "[Epoch 102] train avg loss 0.0358651, train avg r2 0.268144,throughput 4.7896K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 103 Batch 10/159] avg loss 0.0459556, throughput 3.7177K wps\n",
      "[Epoch 103 Batch 20/159] avg loss 0.0370851, throughput 4.07521K wps\n",
      "[Epoch 103 Batch 30/159] avg loss 0.0203879, throughput 4.4026K wps\n",
      "[Epoch 103 Batch 40/159] avg loss 0.0228968, throughput 4.28493K wps\n",
      "[Epoch 103 Batch 50/159] avg loss 0.0228589, throughput 4.29933K wps\n",
      "[Epoch 103 Batch 60/159] avg loss 0.0238871, throughput 4.37109K wps\n",
      "[Epoch 103 Batch 70/159] avg loss 0.0176276, throughput 4.4539K wps\n",
      "[Epoch 103 Batch 80/159] avg loss 0.0369121, throughput 4.78682K wps\n",
      "[Epoch 103 Batch 90/159] avg loss 0.0255769, throughput 4.99861K wps\n",
      "[Epoch 103 Batch 100/159] avg loss 0.0260544, throughput 5.09611K wps\n",
      "[Epoch 103 Batch 110/159] avg loss 0.0342745, throughput 5.50743K wps\n",
      "[Epoch 103 Batch 120/159] avg loss 0.0385787, throughput 5.20039K wps\n",
      "[Epoch 103 Batch 130/159] avg loss 0.0294141, throughput 5.4869K wps\n",
      "[Epoch 103 Batch 140/159] avg loss 0.0456462, throughput 4.73485K wps\n",
      "[Epoch 103 Batch 150/159] avg loss 0.0480492, throughput 4.90108K wps\n",
      "[Epoch 103] train avg loss 0.0358109, train avg r2 0.270146,throughput 4.66115K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 104 Batch 10/159] avg loss 0.0459016, throughput 3.69299K wps\n",
      "[Epoch 104 Batch 20/159] avg loss 0.0370719, throughput 4.06934K wps\n",
      "[Epoch 104 Batch 30/159] avg loss 0.0203634, throughput 4.22657K wps\n",
      "[Epoch 104 Batch 40/159] avg loss 0.0228759, throughput 4.22766K wps\n",
      "[Epoch 104 Batch 50/159] avg loss 0.0228429, throughput 4.33668K wps\n",
      "[Epoch 104 Batch 60/159] avg loss 0.0238596, throughput 4.30929K wps\n",
      "[Epoch 104 Batch 70/159] avg loss 0.0176083, throughput 4.48168K wps\n",
      "[Epoch 104 Batch 80/159] avg loss 0.036754, throughput 4.51838K wps\n",
      "[Epoch 104 Batch 90/159] avg loss 0.0255641, throughput 4.98604K wps\n",
      "[Epoch 104 Batch 100/159] avg loss 0.0260457, throughput 4.93574K wps\n",
      "[Epoch 104 Batch 110/159] avg loss 0.0341756, throughput 5.24756K wps\n",
      "[Epoch 104 Batch 120/159] avg loss 0.0385433, throughput 5.29285K wps\n",
      "[Epoch 104 Batch 130/159] avg loss 0.0293503, throughput 5.07509K wps\n",
      "[Epoch 104 Batch 140/159] avg loss 0.045576, throughput 5.42809K wps\n",
      "[Epoch 104 Batch 150/159] avg loss 0.0479448, throughput 5.43347K wps\n",
      "[Epoch 104] train avg loss 0.0357521, train avg r2 0.270996,throughput 4.65895K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 105 Batch 10/159] avg loss 0.0458444, throughput 3.66649K wps\n",
      "[Epoch 105 Batch 20/159] avg loss 0.0370379, throughput 4.04098K wps\n",
      "[Epoch 105 Batch 30/159] avg loss 0.0203629, throughput 4.31074K wps\n",
      "[Epoch 105 Batch 40/159] avg loss 0.0228544, throughput 4.16861K wps\n",
      "[Epoch 105 Batch 50/159] avg loss 0.0228364, throughput 4.36862K wps\n",
      "[Epoch 105 Batch 60/159] avg loss 0.02385, throughput 4.29647K wps\n",
      "[Epoch 105 Batch 70/159] avg loss 0.0175908, throughput 4.45536K wps\n",
      "[Epoch 105 Batch 80/159] avg loss 0.0366031, throughput 4.5775K wps\n",
      "[Epoch 105 Batch 90/159] avg loss 0.0255529, throughput 4.78843K wps\n",
      "[Epoch 105 Batch 100/159] avg loss 0.026042, throughput 5.03008K wps\n",
      "[Epoch 105 Batch 110/159] avg loss 0.0340796, throughput 5.12997K wps\n",
      "[Epoch 105 Batch 120/159] avg loss 0.0385166, throughput 5.20303K wps\n",
      "[Epoch 105 Batch 130/159] avg loss 0.0293091, throughput 5.16723K wps\n",
      "[Epoch 105 Batch 140/159] avg loss 0.0455057, throughput 5.52423K wps\n",
      "[Epoch 105 Batch 150/159] avg loss 0.0478388, throughput 5.28248K wps\n",
      "[Epoch 105] train avg loss 0.0356982, train avg r2 0.272379,throughput 4.64878K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 106 Batch 10/159] avg loss 0.0457934, throughput 3.66345K wps\n",
      "[Epoch 106 Batch 20/159] avg loss 0.0370186, throughput 4.12208K wps\n",
      "[Epoch 106 Batch 30/159] avg loss 0.0203349, throughput 4.20106K wps\n",
      "[Epoch 106 Batch 40/159] avg loss 0.0228424, throughput 4.25803K wps\n",
      "[Epoch 106 Batch 50/159] avg loss 0.0228185, throughput 4.18744K wps\n",
      "[Epoch 106 Batch 60/159] avg loss 0.0238549, throughput 4.30275K wps\n",
      "[Epoch 106 Batch 70/159] avg loss 0.0175818, throughput 4.31664K wps\n",
      "[Epoch 106 Batch 80/159] avg loss 0.0364989, throughput 4.47953K wps\n",
      "[Epoch 106 Batch 90/159] avg loss 0.0255597, throughput 4.89002K wps\n",
      "[Epoch 106 Batch 100/159] avg loss 0.0260435, throughput 4.89786K wps\n",
      "[Epoch 106 Batch 110/159] avg loss 0.0339836, throughput 5.22527K wps\n",
      "[Epoch 106 Batch 120/159] avg loss 0.0385037, throughput 5.20414K wps\n",
      "[Epoch 106 Batch 130/159] avg loss 0.0292647, throughput 5.20649K wps\n",
      "[Epoch 106 Batch 140/159] avg loss 0.0454346, throughput 5.19272K wps\n",
      "[Epoch 106 Batch 150/159] avg loss 0.0477339, throughput 4.71218K wps\n",
      "[Epoch 106] train avg loss 0.0356492, train avg r2 0.274064,throughput 4.56909K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 107 Batch 10/159] avg loss 0.0457518, throughput 3.59443K wps\n",
      "[Epoch 107 Batch 20/159] avg loss 0.03698, throughput 4.02185K wps\n",
      "[Epoch 107 Batch 30/159] avg loss 0.0203094, throughput 4.14602K wps\n",
      "[Epoch 107 Batch 40/159] avg loss 0.0228184, throughput 4.20667K wps\n",
      "[Epoch 107 Batch 50/159] avg loss 0.022818, throughput 4.22532K wps\n",
      "[Epoch 107 Batch 60/159] avg loss 0.0238356, throughput 4.17495K wps\n",
      "[Epoch 107 Batch 70/159] avg loss 0.0175692, throughput 4.33938K wps\n",
      "[Epoch 107 Batch 80/159] avg loss 0.036366, throughput 4.46933K wps\n",
      "[Epoch 107 Batch 90/159] avg loss 0.0255498, throughput 4.97833K wps\n",
      "[Epoch 107 Batch 100/159] avg loss 0.0260478, throughput 4.95173K wps\n",
      "[Epoch 107 Batch 110/159] avg loss 0.0338798, throughput 5.20765K wps\n",
      "[Epoch 107 Batch 120/159] avg loss 0.0384789, throughput 5.21164K wps\n",
      "[Epoch 107 Batch 130/159] avg loss 0.0291993, throughput 5.16286K wps\n",
      "[Epoch 107 Batch 140/159] avg loss 0.0453666, throughput 5.40844K wps\n",
      "[Epoch 107 Batch 150/159] avg loss 0.0476239, throughput 5.49639K wps\n",
      "[Epoch 107] train avg loss 0.0355938, train avg r2 0.274948,throughput 4.62265K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 108 Batch 10/159] avg loss 0.0456959, throughput 3.59871K wps\n",
      "[Epoch 108 Batch 20/159] avg loss 0.0369591, throughput 4.0443K wps\n",
      "[Epoch 108 Batch 30/159] avg loss 0.0203151, throughput 4.21892K wps\n",
      "[Epoch 108 Batch 40/159] avg loss 0.0227954, throughput 4.20837K wps\n",
      "[Epoch 108 Batch 50/159] avg loss 0.0228106, throughput 4.21517K wps\n",
      "[Epoch 108 Batch 60/159] avg loss 0.0238061, throughput 4.37008K wps\n",
      "[Epoch 108 Batch 70/159] avg loss 0.0175567, throughput 4.2966K wps\n",
      "[Epoch 108 Batch 80/159] avg loss 0.0362354, throughput 4.6205K wps\n",
      "[Epoch 108 Batch 90/159] avg loss 0.0255412, throughput 4.89704K wps\n",
      "[Epoch 108 Batch 100/159] avg loss 0.026049, throughput 5.05825K wps\n",
      "[Epoch 108 Batch 110/159] avg loss 0.0337851, throughput 5.29789K wps\n",
      "[Epoch 108 Batch 120/159] avg loss 0.0384501, throughput 5.25082K wps\n",
      "[Epoch 108 Batch 130/159] avg loss 0.0291455, throughput 5.1175K wps\n",
      "[Epoch 108 Batch 140/159] avg loss 0.0453041, throughput 5.16204K wps\n",
      "[Epoch 108 Batch 150/159] avg loss 0.0475183, throughput 5.25172K wps\n",
      "[Epoch 108] train avg loss 0.0355415, train avg r2 0.275666,throughput 4.62284K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 109 Batch 10/159] avg loss 0.0456407, throughput 3.67137K wps\n",
      "[Epoch 109 Batch 20/159] avg loss 0.0369573, throughput 4.13504K wps\n",
      "[Epoch 109 Batch 30/159] avg loss 0.0203074, throughput 4.15791K wps\n",
      "[Epoch 109 Batch 40/159] avg loss 0.0227691, throughput 4.26434K wps\n",
      "[Epoch 109 Batch 50/159] avg loss 0.0227993, throughput 4.29736K wps\n",
      "[Epoch 109 Batch 60/159] avg loss 0.0237954, throughput 4.33966K wps\n",
      "[Epoch 109 Batch 70/159] avg loss 0.0175409, throughput 4.35825K wps\n",
      "[Epoch 109 Batch 80/159] avg loss 0.0361036, throughput 4.50263K wps\n",
      "[Epoch 109 Batch 90/159] avg loss 0.0255456, throughput 4.94674K wps\n",
      "[Epoch 109 Batch 100/159] avg loss 0.0260431, throughput 4.96096K wps\n",
      "[Epoch 109 Batch 110/159] avg loss 0.0336899, throughput 5.11295K wps\n",
      "[Epoch 109 Batch 120/159] avg loss 0.0384377, throughput 5.11617K wps\n",
      "[Epoch 109 Batch 130/159] avg loss 0.0290993, throughput 5.03817K wps\n",
      "[Epoch 109 Batch 140/159] avg loss 0.0452384, throughput 5.27119K wps\n",
      "[Epoch 109 Batch 150/159] avg loss 0.0474107, throughput 4.64572K wps\n",
      "[Epoch 109] train avg loss 0.0354916, train avg r2 0.277119,throughput 4.56692K wps\n",
      "learning rate: 9.765625e-07\n",
      "[Epoch 110 Batch 10/159] avg loss 0.0455617, throughput 3.61298K wps\n",
      "[Epoch 110 Batch 20/159] avg loss 0.0368938, throughput 4.10984K wps\n",
      "[Epoch 110 Batch 30/159] avg loss 0.0202589, throughput 4.14513K wps\n",
      "[Epoch 110 Batch 40/159] avg loss 0.0226956, throughput 4.20924K wps\n",
      "[Epoch 110 Batch 50/159] avg loss 0.0228347, throughput 4.31676K wps\n",
      "[Epoch 110 Batch 60/159] avg loss 0.0237161, throughput 4.33512K wps\n",
      "[Epoch 110 Batch 70/159] avg loss 0.0175018, throughput 4.28215K wps\n",
      "[Epoch 110 Batch 80/159] avg loss 0.0358801, throughput 4.58209K wps\n",
      "[Epoch 110 Batch 90/159] avg loss 0.0255269, throughput 4.95517K wps\n",
      "[Epoch 110 Batch 100/159] avg loss 0.0260452, throughput 4.93815K wps\n",
      "[Epoch 110 Batch 110/159] avg loss 0.0335798, throughput 5.21229K wps\n",
      "[Epoch 110 Batch 120/159] avg loss 0.0384057, throughput 5.07995K wps\n",
      "[Epoch 110 Batch 130/159] avg loss 0.0289855, throughput 5.13766K wps\n",
      "[Epoch 110 Batch 140/159] avg loss 0.0451794, throughput 5.21269K wps\n",
      "[Epoch 110 Batch 150/159] avg loss 0.0472954, throughput 5.25449K wps\n",
      "[Epoch 110] train avg loss 0.0354171, train avg r2 0.280085,throughput 4.61857K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 111 Batch 10/159] avg loss 0.0455175, throughput 3.62795K wps\n",
      "[Epoch 111 Batch 20/159] avg loss 0.0368795, throughput 4.04396K wps\n",
      "[Epoch 111 Batch 30/159] avg loss 0.0202022, throughput 4.19079K wps\n",
      "[Epoch 111 Batch 40/159] avg loss 0.022697, throughput 4.22408K wps\n",
      "[Epoch 111 Batch 50/159] avg loss 0.0228058, throughput 4.24873K wps\n",
      "[Epoch 111 Batch 60/159] avg loss 0.0237216, throughput 4.32524K wps\n",
      "[Epoch 111 Batch 70/159] avg loss 0.0174991, throughput 4.32015K wps\n",
      "[Epoch 111 Batch 80/159] avg loss 0.035823, throughput 4.52026K wps\n",
      "[Epoch 111 Batch 90/159] avg loss 0.0255287, throughput 4.99006K wps\n",
      "[Epoch 111 Batch 100/159] avg loss 0.0260225, throughput 4.94646K wps\n",
      "[Epoch 111 Batch 110/159] avg loss 0.0335339, throughput 5.22692K wps\n",
      "[Epoch 111 Batch 120/159] avg loss 0.0383939, throughput 4.95303K wps\n",
      "[Epoch 111 Batch 130/159] avg loss 0.0289647, throughput 5.22383K wps\n",
      "[Epoch 111 Batch 140/159] avg loss 0.0451467, throughput 5.34731K wps\n",
      "[Epoch 111 Batch 150/159] avg loss 0.0472506, throughput 5.42512K wps\n",
      "[Epoch 111] train avg loss 0.0353875, train avg r2 0.280679,throughput 4.62818K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 112 Batch 10/159] avg loss 0.0454756, throughput 3.62206K wps\n",
      "[Epoch 112 Batch 20/159] avg loss 0.0368733, throughput 4.02116K wps\n",
      "[Epoch 112 Batch 30/159] avg loss 0.0201788, throughput 4.17786K wps\n",
      "[Epoch 112 Batch 40/159] avg loss 0.0226911, throughput 4.19949K wps\n",
      "[Epoch 112 Batch 50/159] avg loss 0.0227859, throughput 4.29453K wps\n",
      "[Epoch 112 Batch 60/159] avg loss 0.0237338, throughput 4.22808K wps\n",
      "[Epoch 112 Batch 70/159] avg loss 0.0174969, throughput 4.3406K wps\n",
      "[Epoch 112 Batch 80/159] avg loss 0.0357696, throughput 4.54194K wps\n",
      "[Epoch 112 Batch 90/159] avg loss 0.0255285, throughput 4.9303K wps\n",
      "[Epoch 112 Batch 100/159] avg loss 0.0260169, throughput 5.09305K wps\n",
      "[Epoch 112 Batch 110/159] avg loss 0.0334907, throughput 5.20072K wps\n",
      "[Epoch 112 Batch 120/159] avg loss 0.0383817, throughput 5.27078K wps\n",
      "[Epoch 112 Batch 130/159] avg loss 0.0289342, throughput 5.28809K wps\n",
      "[Epoch 112 Batch 140/159] avg loss 0.0451159, throughput 5.19121K wps\n",
      "[Epoch 112 Batch 150/159] avg loss 0.0472043, throughput 5.2278K wps\n",
      "[Epoch 112] train avg loss 0.035362, train avg r2 0.281312,throughput 4.59225K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 113 Batch 10/159] avg loss 0.0454455, throughput 3.47005K wps\n",
      "[Epoch 113 Batch 20/159] avg loss 0.0368604, throughput 3.97935K wps\n",
      "[Epoch 113 Batch 30/159] avg loss 0.0201773, throughput 4.08478K wps\n",
      "[Epoch 113 Batch 40/159] avg loss 0.022681, throughput 4.10987K wps\n",
      "[Epoch 113 Batch 50/159] avg loss 0.0227786, throughput 4.19393K wps\n",
      "[Epoch 113 Batch 60/159] avg loss 0.0237291, throughput 4.21142K wps\n",
      "[Epoch 113 Batch 70/159] avg loss 0.0174941, throughput 4.32328K wps\n",
      "[Epoch 113 Batch 80/159] avg loss 0.0357039, throughput 4.46645K wps\n",
      "[Epoch 113 Batch 90/159] avg loss 0.0255218, throughput 4.95944K wps\n",
      "[Epoch 113 Batch 100/159] avg loss 0.0260198, throughput 4.9847K wps\n",
      "[Epoch 113 Batch 110/159] avg loss 0.0334454, throughput 5.06338K wps\n",
      "[Epoch 113 Batch 120/159] avg loss 0.0383747, throughput 5.19869K wps\n",
      "[Epoch 113 Batch 130/159] avg loss 0.0289052, throughput 5.24901K wps\n",
      "[Epoch 113 Batch 140/159] avg loss 0.0450828, throughput 5.39265K wps\n",
      "[Epoch 113 Batch 150/159] avg loss 0.0471481, throughput 5.36922K wps\n",
      "[Epoch 113] train avg loss 0.0353364, train avg r2 0.282043,throughput 4.5768K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 114 Batch 10/159] avg loss 0.0454122, throughput 3.67721K wps\n",
      "[Epoch 114 Batch 20/159] avg loss 0.0368461, throughput 4.0918K wps\n",
      "[Epoch 114 Batch 30/159] avg loss 0.02017, throughput 4.2013K wps\n",
      "[Epoch 114 Batch 40/159] avg loss 0.0226732, throughput 4.40413K wps\n",
      "[Epoch 114 Batch 50/159] avg loss 0.0227723, throughput 4.51265K wps\n",
      "[Epoch 114 Batch 60/159] avg loss 0.0237163, throughput 4.26625K wps\n",
      "[Epoch 114 Batch 70/159] avg loss 0.0174852, throughput 4.48924K wps\n",
      "[Epoch 114 Batch 80/159] avg loss 0.0356626, throughput 4.59056K wps\n",
      "[Epoch 114 Batch 90/159] avg loss 0.025523, throughput 5.05167K wps\n",
      "[Epoch 114 Batch 100/159] avg loss 0.0260127, throughput 4.96061K wps\n",
      "[Epoch 114 Batch 110/159] avg loss 0.0334016, throughput 5.0033K wps\n",
      "[Epoch 114 Batch 120/159] avg loss 0.0383596, throughput 5.29304K wps\n",
      "[Epoch 114 Batch 130/159] avg loss 0.0288928, throughput 5.19281K wps\n",
      "[Epoch 114 Batch 140/159] avg loss 0.045053, throughput 5.34727K wps\n",
      "[Epoch 114 Batch 150/159] avg loss 0.0470982, throughput 5.53747K wps\n",
      "[Epoch 114] train avg loss 0.0353118, train avg r2 0.282778,throughput 4.6949K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 115 Batch 10/159] avg loss 0.0453927, throughput 3.71314K wps\n",
      "[Epoch 115 Batch 20/159] avg loss 0.0368505, throughput 3.97158K wps\n",
      "[Epoch 115 Batch 30/159] avg loss 0.0201618, throughput 4.18007K wps\n",
      "[Epoch 115 Batch 40/159] avg loss 0.0226622, throughput 4.20903K wps\n",
      "[Epoch 115 Batch 50/159] avg loss 0.0227704, throughput 4.26181K wps\n",
      "[Epoch 115 Batch 60/159] avg loss 0.0237126, throughput 4.26187K wps\n",
      "[Epoch 115 Batch 70/159] avg loss 0.0174797, throughput 4.36242K wps\n",
      "[Epoch 115 Batch 80/159] avg loss 0.0356014, throughput 4.50888K wps\n",
      "[Epoch 115 Batch 90/159] avg loss 0.0255279, throughput 4.82337K wps\n",
      "[Epoch 115 Batch 100/159] avg loss 0.0260164, throughput 5.0061K wps\n",
      "[Epoch 115 Batch 110/159] avg loss 0.0333556, throughput 5.3084K wps\n",
      "[Epoch 115 Batch 120/159] avg loss 0.0383503, throughput 5.18289K wps\n",
      "[Epoch 115 Batch 130/159] avg loss 0.0288641, throughput 5.27715K wps\n",
      "[Epoch 115 Batch 140/159] avg loss 0.0450208, throughput 5.454K wps\n",
      "[Epoch 115 Batch 150/159] avg loss 0.0470455, throughput 5.42568K wps\n",
      "[Epoch 115] train avg loss 0.035289, train avg r2 0.282586,throughput 4.62869K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 116 Batch 10/159] avg loss 0.0453634, throughput 3.40857K wps\n",
      "[Epoch 116 Batch 20/159] avg loss 0.0368344, throughput 3.9687K wps\n",
      "[Epoch 116 Batch 30/159] avg loss 0.0201476, throughput 4.17798K wps\n",
      "[Epoch 116 Batch 40/159] avg loss 0.0226503, throughput 4.25185K wps\n",
      "[Epoch 116 Batch 50/159] avg loss 0.0227646, throughput 4.14654K wps\n",
      "[Epoch 116 Batch 60/159] avg loss 0.0237061, throughput 4.27292K wps\n",
      "[Epoch 116 Batch 70/159] avg loss 0.0174764, throughput 4.2926K wps\n",
      "[Epoch 116 Batch 80/159] avg loss 0.0355519, throughput 4.59795K wps\n",
      "[Epoch 116 Batch 90/159] avg loss 0.0255235, throughput 5.03759K wps\n",
      "[Epoch 116 Batch 100/159] avg loss 0.026009, throughput 4.94596K wps\n",
      "[Epoch 116 Batch 110/159] avg loss 0.0333066, throughput 5.20698K wps\n",
      "[Epoch 116 Batch 120/159] avg loss 0.0383419, throughput 5.15163K wps\n",
      "[Epoch 116 Batch 130/159] avg loss 0.0288354, throughput 5.24978K wps\n",
      "[Epoch 116 Batch 140/159] avg loss 0.0449882, throughput 5.39467K wps\n",
      "[Epoch 116 Batch 150/159] avg loss 0.0469944, throughput 5.48054K wps\n",
      "[Epoch 116] train avg loss 0.0352627, train avg r2 0.283321,throughput 4.6031K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 117 Batch 10/159] avg loss 0.0453437, throughput 3.5857K wps\n",
      "[Epoch 117 Batch 20/159] avg loss 0.0368287, throughput 4.05094K wps\n",
      "[Epoch 117 Batch 30/159] avg loss 0.0201271, throughput 4.2376K wps\n",
      "[Epoch 117 Batch 40/159] avg loss 0.0226413, throughput 4.18736K wps\n",
      "[Epoch 117 Batch 50/159] avg loss 0.0227602, throughput 4.20156K wps\n",
      "[Epoch 117 Batch 60/159] avg loss 0.0237027, throughput 4.26439K wps\n",
      "[Epoch 117 Batch 70/159] avg loss 0.0174673, throughput 4.3276K wps\n",
      "[Epoch 117 Batch 80/159] avg loss 0.0355106, throughput 4.46195K wps\n",
      "[Epoch 117 Batch 90/159] avg loss 0.0255232, throughput 4.82651K wps\n",
      "[Epoch 117 Batch 100/159] avg loss 0.026007, throughput 4.9614K wps\n",
      "[Epoch 117 Batch 110/159] avg loss 0.0332626, throughput 5.12368K wps\n",
      "[Epoch 117 Batch 120/159] avg loss 0.0383302, throughput 5.15925K wps\n",
      "[Epoch 117 Batch 130/159] avg loss 0.0288078, throughput 5.10687K wps\n",
      "[Epoch 117 Batch 140/159] avg loss 0.0449592, throughput 5.05132K wps\n",
      "[Epoch 117 Batch 150/159] avg loss 0.0469422, throughput 5.46969K wps\n",
      "[Epoch 117] train avg loss 0.0352386, train avg r2 0.283637,throughput 4.59786K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 118 Batch 10/159] avg loss 0.0453316, throughput 3.54129K wps\n",
      "[Epoch 118 Batch 20/159] avg loss 0.0368135, throughput 4.0222K wps\n",
      "[Epoch 118 Batch 30/159] avg loss 0.020122, throughput 4.22895K wps\n",
      "[Epoch 118 Batch 40/159] avg loss 0.0226316, throughput 4.14307K wps\n",
      "[Epoch 118 Batch 50/159] avg loss 0.0227575, throughput 4.22639K wps\n",
      "[Epoch 118 Batch 60/159] avg loss 0.0236945, throughput 4.24638K wps\n",
      "[Epoch 118 Batch 70/159] avg loss 0.0174648, throughput 4.1778K wps\n",
      "[Epoch 118 Batch 80/159] avg loss 0.0354457, throughput 4.33953K wps\n",
      "[Epoch 118 Batch 90/159] avg loss 0.0255258, throughput 4.65897K wps\n",
      "[Epoch 118 Batch 100/159] avg loss 0.0260006, throughput 4.73498K wps\n",
      "[Epoch 118 Batch 110/159] avg loss 0.0332142, throughput 5.06757K wps\n",
      "[Epoch 118 Batch 120/159] avg loss 0.03832, throughput 4.9666K wps\n",
      "[Epoch 118 Batch 130/159] avg loss 0.0287876, throughput 5.15334K wps\n",
      "[Epoch 118 Batch 140/159] avg loss 0.0449256, throughput 5.18081K wps\n",
      "[Epoch 118 Batch 150/159] avg loss 0.046891, throughput 5.28078K wps\n",
      "[Epoch 118] train avg loss 0.0352143, train avg r2 0.284024,throughput 4.49485K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 119 Batch 10/159] avg loss 0.045303, throughput 2.87289K wps\n",
      "[Epoch 119 Batch 20/159] avg loss 0.036808, throughput 4.08159K wps\n",
      "[Epoch 119 Batch 30/159] avg loss 0.0201025, throughput 4.11462K wps\n",
      "[Epoch 119 Batch 40/159] avg loss 0.0226211, throughput 4.20574K wps\n",
      "[Epoch 119 Batch 50/159] avg loss 0.0227471, throughput 4.04438K wps\n",
      "[Epoch 119 Batch 60/159] avg loss 0.0236856, throughput 4.13028K wps\n",
      "[Epoch 119 Batch 70/159] avg loss 0.0174592, throughput 4.20276K wps\n",
      "[Epoch 119 Batch 80/159] avg loss 0.0353911, throughput 4.30989K wps\n",
      "[Epoch 119 Batch 90/159] avg loss 0.0255234, throughput 4.66935K wps\n",
      "[Epoch 119 Batch 100/159] avg loss 0.0260015, throughput 4.82126K wps\n",
      "[Epoch 119 Batch 110/159] avg loss 0.0331727, throughput 5.08491K wps\n",
      "[Epoch 119 Batch 120/159] avg loss 0.0383065, throughput 4.99898K wps\n",
      "[Epoch 119 Batch 130/159] avg loss 0.0287604, throughput 5.02589K wps\n",
      "[Epoch 119 Batch 140/159] avg loss 0.0448972, throughput 5.04409K wps\n",
      "[Epoch 119 Batch 150/159] avg loss 0.046839, throughput 5.16197K wps\n",
      "[Epoch 119] train avg loss 0.0351891, train avg r2 0.284725,throughput 4.40105K wps\n",
      "learning rate: 4.8828125e-07\n",
      "[Epoch 120 Batch 10/159] avg loss 0.0452743, throughput 3.43113K wps\n",
      "[Epoch 120 Batch 20/159] avg loss 0.036787, throughput 3.90176K wps\n",
      "[Epoch 120 Batch 30/159] avg loss 0.0200894, throughput 4.16716K wps\n",
      "[Epoch 120 Batch 40/159] avg loss 0.0225853, throughput 4.10637K wps\n",
      "[Epoch 120 Batch 50/159] avg loss 0.022755, throughput 4.1502K wps\n",
      "[Epoch 120 Batch 60/159] avg loss 0.0236407, throughput 4.09145K wps\n",
      "[Epoch 120 Batch 70/159] avg loss 0.0174426, throughput 4.25662K wps\n",
      "[Epoch 120 Batch 80/159] avg loss 0.0352913, throughput 4.46112K wps\n",
      "[Epoch 120 Batch 90/159] avg loss 0.0255084, throughput 4.99036K wps\n",
      "[Epoch 120 Batch 100/159] avg loss 0.026002, throughput 5.07386K wps\n",
      "[Epoch 120 Batch 110/159] avg loss 0.0331252, throughput 5.10973K wps\n",
      "[Epoch 120 Batch 120/159] avg loss 0.0382955, throughput 5.19015K wps\n",
      "[Epoch 120 Batch 130/159] avg loss 0.0286991, throughput 5.0901K wps\n",
      "[Epoch 120 Batch 140/159] avg loss 0.0448703, throughput 5.42802K wps\n",
      "[Epoch 120 Batch 150/159] avg loss 0.0467784, throughput 5.28745K wps\n",
      "[Epoch 120] train avg loss 0.0351541, train avg r2 0.285684,throughput 4.54031K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 121 Batch 10/159] avg loss 0.0452557, throughput 3.49207K wps\n",
      "[Epoch 121 Batch 20/159] avg loss 0.0367854, throughput 3.83958K wps\n",
      "[Epoch 121 Batch 30/159] avg loss 0.0200621, throughput 4.05459K wps\n",
      "[Epoch 121 Batch 40/159] avg loss 0.0225862, throughput 4.08128K wps\n",
      "[Epoch 121 Batch 50/159] avg loss 0.0227426, throughput 4.14015K wps\n",
      "[Epoch 121 Batch 60/159] avg loss 0.0236502, throughput 3.99451K wps\n",
      "[Epoch 121 Batch 70/159] avg loss 0.0174445, throughput 4.20757K wps\n",
      "[Epoch 121 Batch 80/159] avg loss 0.0352562, throughput 4.39877K wps\n",
      "[Epoch 121 Batch 90/159] avg loss 0.0255018, throughput 4.75498K wps\n",
      "[Epoch 121 Batch 100/159] avg loss 0.0259926, throughput 4.83824K wps\n",
      "[Epoch 121 Batch 110/159] avg loss 0.0331018, throughput 4.96272K wps\n",
      "[Epoch 121 Batch 120/159] avg loss 0.0382853, throughput 4.90558K wps\n",
      "[Epoch 121 Batch 130/159] avg loss 0.0286922, throughput 5.05303K wps\n",
      "[Epoch 121 Batch 140/159] avg loss 0.0448565, throughput 5.13877K wps\n",
      "[Epoch 121 Batch 150/159] avg loss 0.0467562, throughput 4.65197K wps\n",
      "[Epoch 121] train avg loss 0.0351401, train avg r2 0.286101,throughput 4.40929K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 122 Batch 10/159] avg loss 0.0452362, throughput 3.49181K wps\n",
      "[Epoch 122 Batch 20/159] avg loss 0.0367803, throughput 3.96201K wps\n",
      "[Epoch 122 Batch 30/159] avg loss 0.0200602, throughput 4.12293K wps\n",
      "[Epoch 122 Batch 40/159] avg loss 0.022586, throughput 4.05454K wps\n",
      "[Epoch 122 Batch 50/159] avg loss 0.0227302, throughput 4.11578K wps\n",
      "[Epoch 122 Batch 60/159] avg loss 0.0236425, throughput 4.22196K wps\n",
      "[Epoch 122 Batch 70/159] avg loss 0.017443, throughput 4.28472K wps\n",
      "[Epoch 122 Batch 80/159] avg loss 0.035236, throughput 4.5376K wps\n",
      "[Epoch 122 Batch 90/159] avg loss 0.025509, throughput 4.85142K wps\n",
      "[Epoch 122 Batch 100/159] avg loss 0.0259925, throughput 4.72723K wps\n",
      "[Epoch 122 Batch 110/159] avg loss 0.03308, throughput 5.07684K wps\n",
      "[Epoch 122 Batch 120/159] avg loss 0.0382829, throughput 5.02397K wps\n",
      "[Epoch 122 Batch 130/159] avg loss 0.0286799, throughput 5.0647K wps\n",
      "[Epoch 122 Batch 140/159] avg loss 0.0448403, throughput 5.18134K wps\n",
      "[Epoch 122 Batch 150/159] avg loss 0.0467335, throughput 5.28929K wps\n",
      "[Epoch 122] train avg loss 0.0351286, train avg r2 0.286216,throughput 4.51513K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 123 Batch 10/159] avg loss 0.0452246, throughput 3.55851K wps\n",
      "[Epoch 123 Batch 20/159] avg loss 0.0367778, throughput 3.8604K wps\n",
      "[Epoch 123 Batch 30/159] avg loss 0.0200518, throughput 4.11186K wps\n",
      "[Epoch 123 Batch 40/159] avg loss 0.0225777, throughput 4.27023K wps\n",
      "[Epoch 123 Batch 50/159] avg loss 0.0227323, throughput 4.21352K wps\n",
      "[Epoch 123 Batch 60/159] avg loss 0.0236372, throughput 4.28203K wps\n",
      "[Epoch 123 Batch 70/159] avg loss 0.0174471, throughput 4.23134K wps\n",
      "[Epoch 123 Batch 80/159] avg loss 0.0352168, throughput 4.5232K wps\n",
      "[Epoch 123 Batch 90/159] avg loss 0.0255075, throughput 4.75112K wps\n",
      "[Epoch 123 Batch 100/159] avg loss 0.0259896, throughput 4.84342K wps\n",
      "[Epoch 123 Batch 110/159] avg loss 0.0330549, throughput 5.09115K wps\n",
      "[Epoch 123 Batch 120/159] avg loss 0.0382734, throughput 5.06373K wps\n",
      "[Epoch 123 Batch 130/159] avg loss 0.0286683, throughput 5.19039K wps\n",
      "[Epoch 123 Batch 140/159] avg loss 0.0448254, throughput 5.16845K wps\n",
      "[Epoch 123 Batch 150/159] avg loss 0.0467088, throughput 5.22611K wps\n",
      "[Epoch 123] train avg loss 0.035117, train avg r2 0.286481,throughput 4.54243K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 124 Batch 10/159] avg loss 0.0452184, throughput 3.50894K wps\n",
      "[Epoch 124 Batch 20/159] avg loss 0.036775, throughput 4.08913K wps\n",
      "[Epoch 124 Batch 30/159] avg loss 0.0200391, throughput 4.18699K wps\n",
      "[Epoch 124 Batch 40/159] avg loss 0.022571, throughput 4.18436K wps\n",
      "[Epoch 124 Batch 50/159] avg loss 0.0227322, throughput 4.13146K wps\n",
      "[Epoch 124 Batch 60/159] avg loss 0.0236393, throughput 4.14013K wps\n",
      "[Epoch 124 Batch 70/159] avg loss 0.0174368, throughput 4.24183K wps\n",
      "[Epoch 124 Batch 80/159] avg loss 0.0351885, throughput 4.47972K wps\n",
      "[Epoch 124 Batch 90/159] avg loss 0.0255046, throughput 4.83598K wps\n",
      "[Epoch 124 Batch 100/159] avg loss 0.02599, throughput 4.96551K wps\n",
      "[Epoch 124 Batch 110/159] avg loss 0.0330277, throughput 5.16936K wps\n",
      "[Epoch 124 Batch 120/159] avg loss 0.0382643, throughput 5.16709K wps\n",
      "[Epoch 124 Batch 130/159] avg loss 0.0286572, throughput 5.07863K wps\n",
      "[Epoch 124 Batch 140/159] avg loss 0.0448134, throughput 4.89231K wps\n",
      "[Epoch 124 Batch 150/159] avg loss 0.046685, throughput 4.59001K wps\n",
      "[Epoch 124] train avg loss 0.0351041, train avg r2 0.286758,throughput 4.48141K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 125 Batch 10/159] avg loss 0.0452099, throughput 3.6402K wps\n",
      "[Epoch 125 Batch 20/159] avg loss 0.0367735, throughput 3.99851K wps\n",
      "[Epoch 125 Batch 30/159] avg loss 0.0200324, throughput 4.14249K wps\n",
      "[Epoch 125 Batch 40/159] avg loss 0.022566, throughput 4.17724K wps\n",
      "[Epoch 125 Batch 50/159] avg loss 0.0227239, throughput 4.3389K wps\n",
      "[Epoch 125 Batch 60/159] avg loss 0.0236431, throughput 4.34854K wps\n",
      "[Epoch 125 Batch 70/159] avg loss 0.0174342, throughput 4.18155K wps\n",
      "[Epoch 125 Batch 80/159] avg loss 0.0351638, throughput 4.49854K wps\n",
      "[Epoch 125 Batch 90/159] avg loss 0.0254998, throughput 4.86002K wps\n",
      "[Epoch 125 Batch 100/159] avg loss 0.0259916, throughput 4.96789K wps\n",
      "[Epoch 125 Batch 110/159] avg loss 0.0330076, throughput 4.98139K wps\n",
      "[Epoch 125 Batch 120/159] avg loss 0.0382649, throughput 5.25155K wps\n",
      "[Epoch 125 Batch 130/159] avg loss 0.0286463, throughput 5.21783K wps\n",
      "[Epoch 125 Batch 140/159] avg loss 0.0447986, throughput 5.3625K wps\n",
      "[Epoch 125 Batch 150/159] avg loss 0.0466608, throughput 5.39997K wps\n",
      "[Epoch 125] train avg loss 0.0350932, train avg r2 0.287497,throughput 4.61056K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 126 Batch 10/159] avg loss 0.0452037, throughput 3.63136K wps\n",
      "[Epoch 126 Batch 20/159] avg loss 0.0367656, throughput 4.11714K wps\n",
      "[Epoch 126 Batch 30/159] avg loss 0.0200159, throughput 4.2263K wps\n",
      "[Epoch 126 Batch 40/159] avg loss 0.0225599, throughput 4.22755K wps\n",
      "[Epoch 126 Batch 50/159] avg loss 0.0227135, throughput 4.29681K wps\n",
      "[Epoch 126 Batch 60/159] avg loss 0.0236359, throughput 4.29754K wps\n",
      "[Epoch 126 Batch 70/159] avg loss 0.0174332, throughput 4.37631K wps\n",
      "[Epoch 126 Batch 80/159] avg loss 0.0351326, throughput 4.54795K wps\n",
      "[Epoch 126 Batch 90/159] avg loss 0.0255035, throughput 5.04656K wps\n",
      "[Epoch 126 Batch 100/159] avg loss 0.025988, throughput 4.90481K wps\n",
      "[Epoch 126 Batch 110/159] avg loss 0.0329851, throughput 5.2168K wps\n",
      "[Epoch 126 Batch 120/159] avg loss 0.0382557, throughput 5.21799K wps\n",
      "[Epoch 126 Batch 130/159] avg loss 0.0286408, throughput 5.30765K wps\n",
      "[Epoch 126 Batch 140/159] avg loss 0.0447836, throughput 5.34737K wps\n",
      "[Epoch 126 Batch 150/159] avg loss 0.0466356, throughput 5.34956K wps\n",
      "[Epoch 126] train avg loss 0.0350799, train avg r2 0.28717,throughput 4.65486K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 127 Batch 10/159] avg loss 0.0451831, throughput 3.68148K wps\n",
      "[Epoch 127 Batch 20/159] avg loss 0.0367581, throughput 4.09402K wps\n",
      "[Epoch 127 Batch 30/159] avg loss 0.0200068, throughput 4.34097K wps\n",
      "[Epoch 127 Batch 40/159] avg loss 0.0225571, throughput 4.32927K wps\n",
      "[Epoch 127 Batch 50/159] avg loss 0.0227083, throughput 4.33237K wps\n",
      "[Epoch 127 Batch 60/159] avg loss 0.0236297, throughput 4.34204K wps\n",
      "[Epoch 127 Batch 70/159] avg loss 0.0174283, throughput 4.31949K wps\n",
      "[Epoch 127 Batch 80/159] avg loss 0.0351161, throughput 4.56534K wps\n",
      "[Epoch 127 Batch 90/159] avg loss 0.0255074, throughput 4.90987K wps\n",
      "[Epoch 127 Batch 100/159] avg loss 0.0259792, throughput 4.99104K wps\n",
      "[Epoch 127 Batch 110/159] avg loss 0.0329615, throughput 5.09223K wps\n",
      "[Epoch 127 Batch 120/159] avg loss 0.0382526, throughput 5.21691K wps\n",
      "[Epoch 127 Batch 130/159] avg loss 0.0286283, throughput 5.28046K wps\n",
      "[Epoch 127 Batch 140/159] avg loss 0.0447689, throughput 5.32885K wps\n",
      "[Epoch 127 Batch 150/159] avg loss 0.0466092, throughput 4.83789K wps\n",
      "[Epoch 127] train avg loss 0.0350669, train avg r2 0.287701,throughput 4.61768K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 128 Batch 10/159] avg loss 0.0451862, throughput 3.54838K wps\n",
      "[Epoch 128 Batch 20/159] avg loss 0.036759, throughput 4.18837K wps\n",
      "[Epoch 128 Batch 30/159] avg loss 0.0199949, throughput 4.16665K wps\n",
      "[Epoch 128 Batch 40/159] avg loss 0.0225506, throughput 4.27448K wps\n",
      "[Epoch 128 Batch 50/159] avg loss 0.0227033, throughput 4.20557K wps\n",
      "[Epoch 128 Batch 60/159] avg loss 0.0236292, throughput 4.31428K wps\n",
      "[Epoch 128 Batch 70/159] avg loss 0.0174305, throughput 4.1892K wps\n",
      "[Epoch 128 Batch 80/159] avg loss 0.0350932, throughput 4.5227K wps\n",
      "[Epoch 128 Batch 90/159] avg loss 0.025504, throughput 4.99621K wps\n",
      "[Epoch 128 Batch 100/159] avg loss 0.0259804, throughput 4.85667K wps\n",
      "[Epoch 128 Batch 110/159] avg loss 0.0329395, throughput 5.16665K wps\n",
      "[Epoch 128 Batch 120/159] avg loss 0.0382485, throughput 5.10553K wps\n",
      "[Epoch 128 Batch 130/159] avg loss 0.0286127, throughput 5.18069K wps\n",
      "[Epoch 128 Batch 140/159] avg loss 0.0447541, throughput 5.46108K wps\n",
      "[Epoch 128 Batch 150/159] avg loss 0.0465836, throughput 5.22902K wps\n",
      "[Epoch 128] train avg loss 0.0350561, train avg r2 0.28789,throughput 4.61515K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 129 Batch 10/159] avg loss 0.0451782, throughput 3.56438K wps\n",
      "[Epoch 129 Batch 20/159] avg loss 0.0367546, throughput 3.95731K wps\n",
      "[Epoch 129 Batch 30/159] avg loss 0.0199935, throughput 4.08372K wps\n",
      "[Epoch 129 Batch 40/159] avg loss 0.022543, throughput 4.11676K wps\n",
      "[Epoch 129 Batch 50/159] avg loss 0.0226952, throughput 4.23812K wps\n",
      "[Epoch 129 Batch 60/159] avg loss 0.0236304, throughput 4.23682K wps\n",
      "[Epoch 129 Batch 70/159] avg loss 0.0174268, throughput 4.36867K wps\n",
      "[Epoch 129 Batch 80/159] avg loss 0.0350648, throughput 4.57431K wps\n",
      "[Epoch 129 Batch 90/159] avg loss 0.0255067, throughput 4.86014K wps\n",
      "[Epoch 129 Batch 100/159] avg loss 0.0259774, throughput 5.00005K wps\n",
      "[Epoch 129 Batch 110/159] avg loss 0.0329158, throughput 5.07108K wps\n",
      "[Epoch 129 Batch 120/159] avg loss 0.0382504, throughput 5.10075K wps\n",
      "[Epoch 129 Batch 130/159] avg loss 0.0286015, throughput 5.11891K wps\n",
      "[Epoch 129 Batch 140/159] avg loss 0.0447409, throughput 5.36929K wps\n",
      "[Epoch 129 Batch 150/159] avg loss 0.0465586, throughput 5.4384K wps\n",
      "[Epoch 129] train avg loss 0.035045, train avg r2 0.288234,throughput 4.58327K wps\n",
      "learning rate: 2.44140625e-07\n",
      "[Epoch 130 Batch 10/159] avg loss 0.0451619, throughput 3.74918K wps\n",
      "[Epoch 130 Batch 20/159] avg loss 0.0367517, throughput 3.97765K wps\n",
      "[Epoch 130 Batch 30/159] avg loss 0.0199833, throughput 4.16892K wps\n",
      "[Epoch 130 Batch 40/159] avg loss 0.022528, throughput 4.22945K wps\n",
      "[Epoch 130 Batch 50/159] avg loss 0.0227092, throughput 4.24641K wps\n",
      "[Epoch 130 Batch 60/159] avg loss 0.023598, throughput 4.20516K wps\n",
      "[Epoch 130 Batch 70/159] avg loss 0.0174179, throughput 4.27073K wps\n",
      "[Epoch 130 Batch 80/159] avg loss 0.035007, throughput 4.47176K wps\n",
      "[Epoch 130 Batch 90/159] avg loss 0.025499, throughput 4.85973K wps\n",
      "[Epoch 130 Batch 100/159] avg loss 0.0259763, throughput 4.98814K wps\n",
      "[Epoch 130 Batch 110/159] avg loss 0.0328899, throughput 5.1814K wps\n",
      "[Epoch 130 Batch 120/159] avg loss 0.0382399, throughput 4.97473K wps\n",
      "[Epoch 130 Batch 130/159] avg loss 0.0285739, throughput 5.1806K wps\n",
      "[Epoch 130 Batch 140/159] avg loss 0.0447295, throughput 5.3199K wps\n",
      "[Epoch 130 Batch 150/159] avg loss 0.0465258, throughput 4.66466K wps\n",
      "[Epoch 130] train avg loss 0.0350269, train avg r2 0.288789,throughput 4.54344K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 131 Batch 10/159] avg loss 0.0451548, throughput 3.53597K wps\n",
      "[Epoch 131 Batch 20/159] avg loss 0.0367421, throughput 4.16605K wps\n",
      "[Epoch 131 Batch 30/159] avg loss 0.0199817, throughput 4.24248K wps\n",
      "[Epoch 131 Batch 40/159] avg loss 0.0225233, throughput 4.28751K wps\n",
      "[Epoch 131 Batch 50/159] avg loss 0.0227026, throughput 4.12136K wps\n",
      "[Epoch 131 Batch 60/159] avg loss 0.0236009, throughput 4.29975K wps\n",
      "[Epoch 131 Batch 70/159] avg loss 0.017419, throughput 4.39087K wps\n",
      "[Epoch 131 Batch 80/159] avg loss 0.0350029, throughput 4.58693K wps\n",
      "[Epoch 131 Batch 90/159] avg loss 0.0254963, throughput 5.02305K wps\n",
      "[Epoch 131 Batch 100/159] avg loss 0.0259731, throughput 5.07354K wps\n",
      "[Epoch 131 Batch 110/159] avg loss 0.0328806, throughput 5.12615K wps\n",
      "[Epoch 131 Batch 120/159] avg loss 0.0382364, throughput 5.20461K wps\n",
      "[Epoch 131 Batch 130/159] avg loss 0.028566, throughput 5.05582K wps\n",
      "[Epoch 131 Batch 140/159] avg loss 0.0447217, throughput 5.46545K wps\n",
      "[Epoch 131 Batch 150/159] avg loss 0.0465168, throughput 5.45834K wps\n",
      "[Epoch 131] train avg loss 0.0350206, train avg r2 0.289147,throughput 4.64449K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 132 Batch 10/159] avg loss 0.0451454, throughput 3.61204K wps\n",
      "[Epoch 132 Batch 20/159] avg loss 0.036743, throughput 4.09565K wps\n",
      "[Epoch 132 Batch 30/159] avg loss 0.0199736, throughput 4.27119K wps\n",
      "[Epoch 132 Batch 40/159] avg loss 0.0225218, throughput 4.24736K wps\n",
      "[Epoch 132 Batch 50/159] avg loss 0.0226989, throughput 4.26569K wps\n",
      "[Epoch 132 Batch 60/159] avg loss 0.0235941, throughput 4.23727K wps\n",
      "[Epoch 132 Batch 70/159] avg loss 0.0174203, throughput 4.38349K wps\n",
      "[Epoch 132 Batch 80/159] avg loss 0.0349997, throughput 4.52543K wps\n",
      "[Epoch 132 Batch 90/159] avg loss 0.0255019, throughput 4.96921K wps\n",
      "[Epoch 132 Batch 100/159] avg loss 0.0259693, throughput 5.10685K wps\n",
      "[Epoch 132 Batch 110/159] avg loss 0.0328709, throughput 5.22799K wps\n",
      "[Epoch 132 Batch 120/159] avg loss 0.0382311, throughput 5.25137K wps\n",
      "[Epoch 132 Batch 130/159] avg loss 0.0285613, throughput 5.1917K wps\n",
      "[Epoch 132 Batch 140/159] avg loss 0.044715, throughput 5.25097K wps\n",
      "[Epoch 132 Batch 150/159] avg loss 0.0465035, throughput 5.30877K wps\n",
      "[Epoch 132] train avg loss 0.035015, train avg r2 0.289192,throughput 4.65421K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 133 Batch 10/159] avg loss 0.0451392, throughput 3.72069K wps\n",
      "[Epoch 133 Batch 20/159] avg loss 0.0367344, throughput 4.25801K wps\n",
      "[Epoch 133 Batch 30/159] avg loss 0.0199699, throughput 4.24761K wps\n",
      "[Epoch 133 Batch 40/159] avg loss 0.0225209, throughput 4.25583K wps\n",
      "[Epoch 133 Batch 50/159] avg loss 0.0226891, throughput 4.35652K wps\n",
      "[Epoch 133 Batch 60/159] avg loss 0.0235977, throughput 4.36652K wps\n",
      "[Epoch 133 Batch 70/159] avg loss 0.0174192, throughput 4.21417K wps\n",
      "[Epoch 133 Batch 80/159] avg loss 0.0349856, throughput 4.4712K wps\n",
      "[Epoch 133 Batch 90/159] avg loss 0.0254921, throughput 4.84385K wps\n",
      "[Epoch 133 Batch 100/159] avg loss 0.0259692, throughput 4.81876K wps\n",
      "[Epoch 133 Batch 110/159] avg loss 0.0328592, throughput 5.0991K wps\n",
      "[Epoch 133 Batch 120/159] avg loss 0.038232, throughput 5.11108K wps\n",
      "[Epoch 133 Batch 130/159] avg loss 0.0285539, throughput 5.1249K wps\n",
      "[Epoch 133 Batch 140/159] avg loss 0.0447065, throughput 5.32931K wps\n",
      "[Epoch 133 Batch 150/159] avg loss 0.0464929, throughput 5.33985K wps\n",
      "[Epoch 133] train avg loss 0.0350079, train avg r2 0.289456,throughput 4.60588K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 134 Batch 10/159] avg loss 0.0451295, throughput 3.42941K wps\n",
      "[Epoch 134 Batch 20/159] avg loss 0.0367374, throughput 3.98436K wps\n",
      "[Epoch 134 Batch 30/159] avg loss 0.0199708, throughput 4.16897K wps\n",
      "[Epoch 134 Batch 40/159] avg loss 0.0225218, throughput 4.17217K wps\n",
      "[Epoch 134 Batch 50/159] avg loss 0.0226881, throughput 4.1573K wps\n",
      "[Epoch 134 Batch 60/159] avg loss 0.0235953, throughput 4.16263K wps\n",
      "[Epoch 134 Batch 70/159] avg loss 0.0174159, throughput 4.45831K wps\n",
      "[Epoch 134 Batch 80/159] avg loss 0.0349723, throughput 4.44819K wps\n",
      "[Epoch 134 Batch 90/159] avg loss 0.0254951, throughput 4.84489K wps\n",
      "[Epoch 134 Batch 100/159] avg loss 0.0259716, throughput 5.04016K wps\n",
      "[Epoch 134 Batch 110/159] avg loss 0.0328488, throughput 5.02599K wps\n",
      "[Epoch 134 Batch 120/159] avg loss 0.0382298, throughput 5.12248K wps\n",
      "[Epoch 134 Batch 130/159] avg loss 0.0285511, throughput 5.08091K wps\n",
      "[Epoch 134 Batch 140/159] avg loss 0.0447009, throughput 5.22307K wps\n",
      "[Epoch 134 Batch 150/159] avg loss 0.0464797, throughput 5.3187K wps\n",
      "[Epoch 134] train avg loss 0.035003, train avg r2 0.289082,throughput 4.55168K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 135 Batch 10/159] avg loss 0.0451268, throughput 3.64174K wps\n",
      "[Epoch 135 Batch 20/159] avg loss 0.0367289, throughput 3.95014K wps\n",
      "[Epoch 135 Batch 30/159] avg loss 0.0199735, throughput 4.14758K wps\n",
      "[Epoch 135 Batch 40/159] avg loss 0.0225133, throughput 4.1178K wps\n",
      "[Epoch 135 Batch 50/159] avg loss 0.0226995, throughput 4.36057K wps\n",
      "[Epoch 135 Batch 60/159] avg loss 0.0235987, throughput 4.36881K wps\n",
      "[Epoch 135 Batch 70/159] avg loss 0.017413, throughput 4.35083K wps\n",
      "[Epoch 135 Batch 80/159] avg loss 0.0349675, throughput 4.5457K wps\n",
      "[Epoch 135 Batch 90/159] avg loss 0.0254999, throughput 4.81203K wps\n",
      "[Epoch 135 Batch 100/159] avg loss 0.0259694, throughput 4.98263K wps\n",
      "[Epoch 135 Batch 110/159] avg loss 0.0328392, throughput 5.16071K wps\n",
      "[Epoch 135 Batch 120/159] avg loss 0.0382249, throughput 5.26676K wps\n",
      "[Epoch 135 Batch 130/159] avg loss 0.0285428, throughput 5.31592K wps\n",
      "[Epoch 135 Batch 140/159] avg loss 0.0446931, throughput 5.28597K wps\n",
      "[Epoch 135 Batch 150/159] avg loss 0.0464675, throughput 5.50286K wps\n",
      "[Epoch 135] train avg loss 0.0349981, train avg r2 0.289648,throughput 4.63562K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 136 Batch 10/159] avg loss 0.0451224, throughput 3.62671K wps\n",
      "[Epoch 136 Batch 20/159] avg loss 0.0367305, throughput 3.99089K wps\n",
      "[Epoch 136 Batch 30/159] avg loss 0.0199497, throughput 4.11009K wps\n",
      "[Epoch 136 Batch 40/159] avg loss 0.0225167, throughput 4.15069K wps\n",
      "[Epoch 136 Batch 50/159] avg loss 0.0226892, throughput 4.21149K wps\n",
      "[Epoch 136 Batch 60/159] avg loss 0.0235953, throughput 4.31926K wps\n",
      "[Epoch 136 Batch 70/159] avg loss 0.0174121, throughput 4.43362K wps\n",
      "[Epoch 136 Batch 80/159] avg loss 0.0349533, throughput 4.54727K wps\n",
      "[Epoch 136 Batch 90/159] avg loss 0.0254965, throughput 5.02662K wps\n",
      "[Epoch 136 Batch 100/159] avg loss 0.0259704, throughput 4.82616K wps\n",
      "[Epoch 136 Batch 110/159] avg loss 0.0328302, throughput 5.07898K wps\n",
      "[Epoch 136 Batch 120/159] avg loss 0.0382243, throughput 5.13954K wps\n",
      "[Epoch 136 Batch 130/159] avg loss 0.0285413, throughput 5.13788K wps\n",
      "[Epoch 136 Batch 140/159] avg loss 0.0446859, throughput 5.29627K wps\n",
      "[Epoch 136 Batch 150/159] avg loss 0.0464542, throughput 5.35265K wps\n",
      "[Epoch 136] train avg loss 0.034991, train avg r2 0.289826,throughput 4.57009K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 137 Batch 10/159] avg loss 0.0451176, throughput 3.41094K wps\n",
      "[Epoch 137 Batch 20/159] avg loss 0.0367308, throughput 3.94686K wps\n",
      "[Epoch 137 Batch 30/159] avg loss 0.0199526, throughput 4.17569K wps\n",
      "[Epoch 137 Batch 40/159] avg loss 0.0225149, throughput 4.21165K wps\n",
      "[Epoch 137 Batch 50/159] avg loss 0.0226833, throughput 4.23939K wps\n",
      "[Epoch 137 Batch 60/159] avg loss 0.0235948, throughput 4.17369K wps\n",
      "[Epoch 137 Batch 70/159] avg loss 0.0174114, throughput 4.12517K wps\n",
      "[Epoch 137 Batch 80/159] avg loss 0.0349271, throughput 4.39984K wps\n",
      "[Epoch 137 Batch 90/159] avg loss 0.0254988, throughput 4.91962K wps\n",
      "[Epoch 137 Batch 100/159] avg loss 0.025968, throughput 4.95811K wps\n",
      "[Epoch 137 Batch 110/159] avg loss 0.0328187, throughput 5.1253K wps\n",
      "[Epoch 137 Batch 120/159] avg loss 0.0382199, throughput 5.10362K wps\n",
      "[Epoch 137 Batch 130/159] avg loss 0.0285318, throughput 5.06193K wps\n",
      "[Epoch 137 Batch 140/159] avg loss 0.044676, throughput 5.0678K wps\n",
      "[Epoch 137 Batch 150/159] avg loss 0.0464441, throughput 5.36655K wps\n",
      "[Epoch 137] train avg loss 0.0349844, train avg r2 0.289573,throughput 4.53418K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 138 Batch 10/159] avg loss 0.0451159, throughput 3.64258K wps\n",
      "[Epoch 138 Batch 20/159] avg loss 0.0367297, throughput 3.97381K wps\n",
      "[Epoch 138 Batch 30/159] avg loss 0.0199463, throughput 4.20039K wps\n",
      "[Epoch 138 Batch 40/159] avg loss 0.022512, throughput 4.17998K wps\n",
      "[Epoch 138 Batch 50/159] avg loss 0.0226814, throughput 4.09381K wps\n",
      "[Epoch 138 Batch 60/159] avg loss 0.0235904, throughput 4.27134K wps\n",
      "[Epoch 138 Batch 70/159] avg loss 0.0174131, throughput 4.20669K wps\n",
      "[Epoch 138 Batch 80/159] avg loss 0.034916, throughput 4.49622K wps\n",
      "[Epoch 138 Batch 90/159] avg loss 0.0254953, throughput 4.7778K wps\n",
      "[Epoch 138 Batch 100/159] avg loss 0.0259693, throughput 4.84577K wps\n",
      "[Epoch 138 Batch 110/159] avg loss 0.0328094, throughput 5.00209K wps\n",
      "[Epoch 138 Batch 120/159] avg loss 0.0382192, throughput 5.11093K wps\n",
      "[Epoch 138 Batch 130/159] avg loss 0.0285288, throughput 5.1875K wps\n",
      "[Epoch 138 Batch 140/159] avg loss 0.0446704, throughput 5.09695K wps\n",
      "[Epoch 138 Batch 150/159] avg loss 0.0464313, throughput 5.15515K wps\n",
      "[Epoch 138] train avg loss 0.034979, train avg r2 0.289939,throughput 4.54364K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 139 Batch 10/159] avg loss 0.0451132, throughput 3.56537K wps\n",
      "[Epoch 139 Batch 20/159] avg loss 0.0367338, throughput 3.95473K wps\n",
      "[Epoch 139 Batch 30/159] avg loss 0.0199494, throughput 4.08825K wps\n",
      "[Epoch 139 Batch 40/159] avg loss 0.0225095, throughput 4.10528K wps\n",
      "[Epoch 139 Batch 50/159] avg loss 0.0226844, throughput 4.23437K wps\n",
      "[Epoch 139 Batch 60/159] avg loss 0.0235911, throughput 4.2323K wps\n",
      "[Epoch 139 Batch 70/159] avg loss 0.0174123, throughput 4.3788K wps\n",
      "[Epoch 139 Batch 80/159] avg loss 0.0349008, throughput 4.41434K wps\n",
      "[Epoch 139 Batch 90/159] avg loss 0.0254964, throughput 4.8278K wps\n",
      "[Epoch 139 Batch 100/159] avg loss 0.025966, throughput 4.87031K wps\n",
      "[Epoch 139 Batch 110/159] avg loss 0.0327977, throughput 5.38864K wps\n",
      "[Epoch 139 Batch 120/159] avg loss 0.0382166, throughput 5.10988K wps\n",
      "[Epoch 139 Batch 130/159] avg loss 0.0285279, throughput 5.12219K wps\n",
      "[Epoch 139 Batch 140/159] avg loss 0.0446599, throughput 5.31174K wps\n",
      "[Epoch 139 Batch 150/159] avg loss 0.0464207, throughput 5.23151K wps\n",
      "[Epoch 139] train avg loss 0.0349744, train avg r2 0.290064,throughput 4.54K wps\n",
      "learning rate: 1.220703125e-07\n",
      "[Epoch 140 Batch 10/159] avg loss 0.0451052, throughput 3.42471K wps\n",
      "[Epoch 140 Batch 20/159] avg loss 0.0367208, throughput 4.04549K wps\n",
      "[Epoch 140 Batch 30/159] avg loss 0.019935, throughput 4.25396K wps\n",
      "[Epoch 140 Batch 40/159] avg loss 0.022503, throughput 4.21188K wps\n",
      "[Epoch 140 Batch 50/159] avg loss 0.0226836, throughput 4.2275K wps\n",
      "[Epoch 140 Batch 60/159] avg loss 0.0235789, throughput 4.20077K wps\n",
      "[Epoch 140 Batch 70/159] avg loss 0.0174091, throughput 4.2866K wps\n",
      "[Epoch 140 Batch 80/159] avg loss 0.0348831, throughput 4.39592K wps\n",
      "[Epoch 140 Batch 90/159] avg loss 0.0254902, throughput 4.85056K wps\n",
      "[Epoch 140 Batch 100/159] avg loss 0.0259677, throughput 5.00157K wps\n",
      "[Epoch 140 Batch 110/159] avg loss 0.0327855, throughput 5.02815K wps\n",
      "[Epoch 140 Batch 120/159] avg loss 0.0382147, throughput 5.27447K wps\n",
      "[Epoch 140 Batch 130/159] avg loss 0.0285083, throughput 5.14341K wps\n",
      "[Epoch 140 Batch 140/159] avg loss 0.0446518, throughput 5.46585K wps\n",
      "[Epoch 140 Batch 150/159] avg loss 0.0464057, throughput 5.42287K wps\n",
      "[Epoch 140] train avg loss 0.0349645, train avg r2 0.290283,throughput 4.59769K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 141 Batch 10/159] avg loss 0.0450973, throughput 3.63283K wps\n",
      "[Epoch 141 Batch 20/159] avg loss 0.0367213, throughput 3.86393K wps\n",
      "[Epoch 141 Batch 30/159] avg loss 0.0199344, throughput 4.1686K wps\n",
      "[Epoch 141 Batch 40/159] avg loss 0.0224954, throughput 4.26376K wps\n",
      "[Epoch 141 Batch 50/159] avg loss 0.0226815, throughput 4.2051K wps\n",
      "[Epoch 141 Batch 60/159] avg loss 0.0235786, throughput 4.26255K wps\n",
      "[Epoch 141 Batch 70/159] avg loss 0.01741, throughput 4.19252K wps\n",
      "[Epoch 141 Batch 80/159] avg loss 0.0348777, throughput 4.44277K wps\n",
      "[Epoch 141 Batch 90/159] avg loss 0.025495, throughput 4.74885K wps\n",
      "[Epoch 141 Batch 100/159] avg loss 0.0259696, throughput 5.0653K wps\n",
      "[Epoch 141 Batch 110/159] avg loss 0.0327801, throughput 5.15971K wps\n",
      "[Epoch 141 Batch 120/159] avg loss 0.0382099, throughput 5.14344K wps\n",
      "[Epoch 141 Batch 130/159] avg loss 0.0285091, throughput 4.97416K wps\n",
      "[Epoch 141 Batch 140/159] avg loss 0.0446494, throughput 5.44492K wps\n",
      "[Epoch 141 Batch 150/159] avg loss 0.046399, throughput 5.22621K wps\n",
      "[Epoch 141] train avg loss 0.0349617, train avg r2 0.290401,throughput 4.57361K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 142 Batch 10/159] avg loss 0.0450934, throughput 3.63812K wps\n",
      "[Epoch 142 Batch 20/159] avg loss 0.0367276, throughput 3.94944K wps\n",
      "[Epoch 142 Batch 30/159] avg loss 0.0199315, throughput 4.18514K wps\n",
      "[Epoch 142 Batch 40/159] avg loss 0.0224999, throughput 4.14308K wps\n",
      "[Epoch 142 Batch 50/159] avg loss 0.0226796, throughput 4.21023K wps\n",
      "[Epoch 142 Batch 60/159] avg loss 0.0235789, throughput 4.2288K wps\n",
      "[Epoch 142 Batch 70/159] avg loss 0.0174059, throughput 4.23201K wps\n",
      "[Epoch 142 Batch 80/159] avg loss 0.0348875, throughput 4.5091K wps\n",
      "[Epoch 142 Batch 90/159] avg loss 0.0254905, throughput 4.68006K wps\n",
      "[Epoch 142 Batch 100/159] avg loss 0.0259659, throughput 4.83525K wps\n",
      "[Epoch 142 Batch 110/159] avg loss 0.032776, throughput 5.10167K wps\n",
      "[Epoch 142 Batch 120/159] avg loss 0.03821, throughput 5.07683K wps\n",
      "[Epoch 142 Batch 130/159] avg loss 0.0285042, throughput 5.10431K wps\n",
      "[Epoch 142 Batch 140/159] avg loss 0.0446468, throughput 5.19968K wps\n",
      "[Epoch 142 Batch 150/159] avg loss 0.0463928, throughput 5.02563K wps\n",
      "[Epoch 142] train avg loss 0.0349598, train avg r2 0.290899,throughput 4.50353K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 143 Batch 10/159] avg loss 0.0450892, throughput 3.46618K wps\n",
      "[Epoch 143 Batch 20/159] avg loss 0.0367174, throughput 3.9337K wps\n",
      "[Epoch 143 Batch 30/159] avg loss 0.0199258, throughput 4.25452K wps\n",
      "[Epoch 143 Batch 40/159] avg loss 0.0224971, throughput 4.25392K wps\n",
      "[Epoch 143 Batch 50/159] avg loss 0.0226797, throughput 4.38648K wps\n",
      "[Epoch 143 Batch 60/159] avg loss 0.0235783, throughput 4.29496K wps\n",
      "[Epoch 143 Batch 70/159] avg loss 0.0174097, throughput 4.36727K wps\n",
      "[Epoch 143 Batch 80/159] avg loss 0.0348837, throughput 4.51962K wps\n",
      "[Epoch 143 Batch 90/159] avg loss 0.0254959, throughput 5.11561K wps\n",
      "[Epoch 143 Batch 100/159] avg loss 0.0259655, throughput 5.02811K wps\n",
      "[Epoch 143 Batch 110/159] avg loss 0.0327709, throughput 5.25295K wps\n",
      "[Epoch 143 Batch 120/159] avg loss 0.0382057, throughput 5.32323K wps\n",
      "[Epoch 143 Batch 130/159] avg loss 0.0285009, throughput 5.24698K wps\n",
      "[Epoch 143 Batch 140/159] avg loss 0.0446423, throughput 5.35451K wps\n",
      "[Epoch 143 Batch 150/159] avg loss 0.0463875, throughput 5.44118K wps\n",
      "[Epoch 143] train avg loss 0.0349563, train avg r2 0.291497,throughput 4.65767K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 144 Batch 10/159] avg loss 0.0450872, throughput 3.71832K wps\n",
      "[Epoch 144 Batch 20/159] avg loss 0.0367164, throughput 4.0762K wps\n",
      "[Epoch 144 Batch 30/159] avg loss 0.0199212, throughput 4.24143K wps\n",
      "[Epoch 144 Batch 40/159] avg loss 0.0224954, throughput 4.25137K wps\n",
      "[Epoch 144 Batch 50/159] avg loss 0.0226788, throughput 4.18759K wps\n",
      "[Epoch 144 Batch 60/159] avg loss 0.023578, throughput 4.23522K wps\n",
      "[Epoch 144 Batch 70/159] avg loss 0.0174071, throughput 4.24333K wps\n",
      "[Epoch 144 Batch 80/159] avg loss 0.034874, throughput 4.64729K wps\n",
      "[Epoch 144 Batch 90/159] avg loss 0.0254984, throughput 4.91542K wps\n",
      "[Epoch 144 Batch 100/159] avg loss 0.0259655, throughput 4.82563K wps\n",
      "[Epoch 144 Batch 110/159] avg loss 0.032766, throughput 5.31742K wps\n",
      "[Epoch 144 Batch 120/159] avg loss 0.0382045, throughput 5.08621K wps\n",
      "[Epoch 144 Batch 130/159] avg loss 0.0284974, throughput 5.383K wps\n",
      "[Epoch 144 Batch 140/159] avg loss 0.0446405, throughput 5.31691K wps\n",
      "[Epoch 144 Batch 150/159] avg loss 0.0463796, throughput 5.37976K wps\n",
      "[Epoch 144] train avg loss 0.034953, train avg r2 0.290961,throughput 4.64608K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 145 Batch 10/159] avg loss 0.0450826, throughput 3.70803K wps\n",
      "[Epoch 145 Batch 20/159] avg loss 0.036717, throughput 4.15362K wps\n",
      "[Epoch 145 Batch 30/159] avg loss 0.0199139, throughput 4.27946K wps\n",
      "[Epoch 145 Batch 40/159] avg loss 0.022493, throughput 4.22929K wps\n",
      "[Epoch 145 Batch 50/159] avg loss 0.022675, throughput 4.27872K wps\n",
      "[Epoch 145 Batch 60/159] avg loss 0.023578, throughput 4.27K wps\n",
      "[Epoch 145 Batch 70/159] avg loss 0.017408, throughput 4.28563K wps\n",
      "[Epoch 145 Batch 80/159] avg loss 0.0348664, throughput 4.53611K wps\n",
      "[Epoch 145 Batch 90/159] avg loss 0.0254977, throughput 4.97233K wps\n",
      "[Epoch 145 Batch 100/159] avg loss 0.0259621, throughput 4.91829K wps\n",
      "[Epoch 145 Batch 110/159] avg loss 0.0327614, throughput 5.0305K wps\n",
      "[Epoch 145 Batch 120/159] avg loss 0.0382048, throughput 4.72178K wps\n",
      "[Epoch 145 Batch 130/159] avg loss 0.0284969, throughput 5.01874K wps\n",
      "[Epoch 145 Batch 140/159] avg loss 0.0446358, throughput 5.09697K wps\n",
      "[Epoch 145 Batch 150/159] avg loss 0.0463791, throughput 5.28235K wps\n",
      "[Epoch 145] train avg loss 0.03495, train avg r2 0.290784,throughput 4.57019K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 146 Batch 10/159] avg loss 0.0450804, throughput 3.38206K wps\n",
      "[Epoch 146 Batch 20/159] avg loss 0.0367212, throughput 3.82317K wps\n",
      "[Epoch 146 Batch 30/159] avg loss 0.0199132, throughput 4.10831K wps\n",
      "[Epoch 146 Batch 40/159] avg loss 0.0224933, throughput 3.92127K wps\n",
      "[Epoch 146 Batch 50/159] avg loss 0.0226753, throughput 4.09148K wps\n",
      "[Epoch 146 Batch 60/159] avg loss 0.0235745, throughput 4.00375K wps\n",
      "[Epoch 146 Batch 70/159] avg loss 0.0174059, throughput 4.04761K wps\n",
      "[Epoch 146 Batch 80/159] avg loss 0.0348596, throughput 4.38906K wps\n",
      "[Epoch 146 Batch 90/159] avg loss 0.0254982, throughput 4.66243K wps\n",
      "[Epoch 146 Batch 100/159] avg loss 0.0259592, throughput 4.45488K wps\n",
      "[Epoch 146 Batch 110/159] avg loss 0.0327512, throughput 5.07073K wps\n",
      "[Epoch 146 Batch 120/159] avg loss 0.0382033, throughput 5.07751K wps\n",
      "[Epoch 146 Batch 130/159] avg loss 0.0284924, throughput 5.16275K wps\n",
      "[Epoch 146 Batch 140/159] avg loss 0.0446335, throughput 5.17299K wps\n",
      "[Epoch 146 Batch 150/159] avg loss 0.0463731, throughput 5.03073K wps\n",
      "[Epoch 146] train avg loss 0.0349468, train avg r2 0.290829,throughput 4.39814K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 147 Batch 10/159] avg loss 0.0450819, throughput 3.5164K wps\n",
      "[Epoch 147 Batch 20/159] avg loss 0.0367173, throughput 3.84479K wps\n",
      "[Epoch 147 Batch 30/159] avg loss 0.0199129, throughput 3.88793K wps\n",
      "[Epoch 147 Batch 40/159] avg loss 0.0224899, throughput 3.94262K wps\n",
      "[Epoch 147 Batch 50/159] avg loss 0.0226739, throughput 4.14741K wps\n",
      "[Epoch 147 Batch 60/159] avg loss 0.0235753, throughput 4.11547K wps\n",
      "[Epoch 147 Batch 70/159] avg loss 0.0174052, throughput 4.06203K wps\n",
      "[Epoch 147 Batch 80/159] avg loss 0.0348423, throughput 4.33012K wps\n",
      "[Epoch 147 Batch 90/159] avg loss 0.0254958, throughput 4.78086K wps\n",
      "[Epoch 147 Batch 100/159] avg loss 0.025961, throughput 4.52883K wps\n",
      "[Epoch 147 Batch 110/159] avg loss 0.0327481, throughput 4.83719K wps\n",
      "[Epoch 147 Batch 120/159] avg loss 0.0382047, throughput 4.96968K wps\n",
      "[Epoch 147 Batch 130/159] avg loss 0.0284855, throughput 4.68979K wps\n",
      "[Epoch 147 Batch 140/159] avg loss 0.0446298, throughput 5.03584K wps\n",
      "[Epoch 147 Batch 150/159] avg loss 0.046367, throughput 5.02391K wps\n",
      "[Epoch 147] train avg loss 0.0349434, train avg r2 0.291164,throughput 4.3781K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 148 Batch 10/159] avg loss 0.0450788, throughput 3.54341K wps\n",
      "[Epoch 148 Batch 20/159] avg loss 0.036722, throughput 3.99455K wps\n",
      "[Epoch 148 Batch 30/159] avg loss 0.0199136, throughput 4.00159K wps\n",
      "[Epoch 148 Batch 40/159] avg loss 0.0224909, throughput 4.02984K wps\n",
      "[Epoch 148 Batch 50/159] avg loss 0.0226683, throughput 4.08181K wps\n",
      "[Epoch 148 Batch 60/159] avg loss 0.0235744, throughput 4.12876K wps\n",
      "[Epoch 148 Batch 70/159] avg loss 0.0174063, throughput 4.16485K wps\n",
      "[Epoch 148 Batch 80/159] avg loss 0.0348325, throughput 4.42004K wps\n",
      "[Epoch 148 Batch 90/159] avg loss 0.025495, throughput 4.51086K wps\n",
      "[Epoch 148 Batch 100/159] avg loss 0.0259639, throughput 4.7756K wps\n",
      "[Epoch 148 Batch 110/159] avg loss 0.0327437, throughput 4.99099K wps\n",
      "[Epoch 148 Batch 120/159] avg loss 0.0381993, throughput 4.81949K wps\n",
      "[Epoch 148 Batch 130/159] avg loss 0.0284871, throughput 4.90041K wps\n",
      "[Epoch 148 Batch 140/159] avg loss 0.0446268, throughput 4.61935K wps\n",
      "[Epoch 148 Batch 150/159] avg loss 0.0463575, throughput 4.87472K wps\n",
      "[Epoch 148] train avg loss 0.0349405, train avg r2 0.291233,throughput 4.38042K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 149 Batch 10/159] avg loss 0.0450751, throughput 3.63793K wps\n",
      "[Epoch 149 Batch 20/159] avg loss 0.0367158, throughput 4.06858K wps\n",
      "[Epoch 149 Batch 30/159] avg loss 0.0199119, throughput 4.19663K wps\n",
      "[Epoch 149 Batch 40/159] avg loss 0.0224877, throughput 4.1762K wps\n",
      "[Epoch 149 Batch 50/159] avg loss 0.0226669, throughput 4.16476K wps\n",
      "[Epoch 149 Batch 60/159] avg loss 0.0235763, throughput 4.3146K wps\n",
      "[Epoch 149 Batch 70/159] avg loss 0.0174063, throughput 4.29247K wps\n",
      "[Epoch 149 Batch 80/159] avg loss 0.0348273, throughput 4.64489K wps\n",
      "[Epoch 149 Batch 90/159] avg loss 0.0254965, throughput 5.01543K wps\n",
      "[Epoch 149 Batch 100/159] avg loss 0.025964, throughput 4.95753K wps\n",
      "[Epoch 149 Batch 110/159] avg loss 0.0327394, throughput 5.21604K wps\n",
      "[Epoch 149 Batch 120/159] avg loss 0.0382029, throughput 5.07295K wps\n",
      "[Epoch 149 Batch 130/159] avg loss 0.0284832, throughput 5.04223K wps\n",
      "[Epoch 149 Batch 140/159] avg loss 0.044624, throughput 5.19015K wps\n",
      "[Epoch 149 Batch 150/159] avg loss 0.0463532, throughput 5.2529K wps\n",
      "[Epoch 149] train avg loss 0.0349379, train avg r2 0.291109,throughput 4.60902K wps\n",
      "learning rate: 6.103515625e-08\n",
      "[Epoch 150 Batch 10/159] avg loss 0.0450728, throughput 3.62968K wps\n",
      "[Epoch 150 Batch 20/159] avg loss 0.0367122, throughput 4.08655K wps\n",
      "[Epoch 150 Batch 30/159] avg loss 0.0199075, throughput 4.27277K wps\n",
      "[Epoch 150 Batch 40/159] avg loss 0.0224873, throughput 4.15624K wps\n",
      "[Epoch 150 Batch 50/159] avg loss 0.0226666, throughput 4.29811K wps\n",
      "[Epoch 150 Batch 60/159] avg loss 0.0235703, throughput 4.11652K wps\n",
      "[Epoch 150 Batch 70/159] avg loss 0.0174042, throughput 4.15803K wps\n",
      "[Epoch 150 Batch 80/159] avg loss 0.0348205, throughput 4.29853K wps\n",
      "[Epoch 150 Batch 90/159] avg loss 0.0254956, throughput 4.68397K wps\n",
      "[Epoch 150 Batch 100/159] avg loss 0.0259611, throughput 4.86301K wps\n",
      "[Epoch 150 Batch 110/159] avg loss 0.032732, throughput 5.00575K wps\n",
      "[Epoch 150 Batch 120/159] avg loss 0.0382011, throughput 5.09245K wps\n",
      "[Epoch 150 Batch 130/159] avg loss 0.028472, throughput 5.10417K wps\n",
      "[Epoch 150 Batch 140/159] avg loss 0.0446209, throughput 5.18071K wps\n",
      "[Epoch 150 Batch 150/159] avg loss 0.0463463, throughput 5.39664K wps\n",
      "[Epoch 150] train avg loss 0.0349334, train avg r2 0.291352,throughput 4.54126K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 151 Batch 10/159] avg loss 0.0450742, throughput 3.62718K wps\n",
      "[Epoch 151 Batch 20/159] avg loss 0.0367145, throughput 4.04749K wps\n",
      "[Epoch 151 Batch 30/159] avg loss 0.0199082, throughput 4.10614K wps\n",
      "[Epoch 151 Batch 40/159] avg loss 0.0224834, throughput 4.03838K wps\n",
      "[Epoch 151 Batch 50/159] avg loss 0.0226667, throughput 4.32736K wps\n",
      "[Epoch 151 Batch 60/159] avg loss 0.023567, throughput 4.12053K wps\n",
      "[Epoch 151 Batch 70/159] avg loss 0.0174041, throughput 4.33729K wps\n",
      "[Epoch 151 Batch 80/159] avg loss 0.0348069, throughput 4.50385K wps\n",
      "[Epoch 151 Batch 90/159] avg loss 0.0254935, throughput 4.8354K wps\n",
      "[Epoch 151 Batch 100/159] avg loss 0.025964, throughput 4.83672K wps\n",
      "[Epoch 151 Batch 110/159] avg loss 0.0327292, throughput 5.08078K wps\n",
      "[Epoch 151 Batch 120/159] avg loss 0.0381966, throughput 4.81341K wps\n",
      "[Epoch 151 Batch 130/159] avg loss 0.0284664, throughput 4.78159K wps\n",
      "[Epoch 151 Batch 140/159] avg loss 0.0446189, throughput 4.54966K wps\n",
      "[Epoch 151 Batch 150/159] avg loss 0.0463424, throughput 4.79852K wps\n",
      "[Epoch 151] train avg loss 0.034931, train avg r2 0.291204,throughput 4.45244K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 152 Batch 10/159] avg loss 0.0450737, throughput 3.6255K wps\n",
      "[Epoch 152 Batch 20/159] avg loss 0.0367193, throughput 4.03374K wps\n",
      "[Epoch 152 Batch 30/159] avg loss 0.0199002, throughput 4.16114K wps\n",
      "[Epoch 152 Batch 40/159] avg loss 0.022481, throughput 4.23087K wps\n",
      "[Epoch 152 Batch 50/159] avg loss 0.0226633, throughput 4.19765K wps\n",
      "[Epoch 152 Batch 60/159] avg loss 0.0235699, throughput 4.26492K wps\n",
      "[Epoch 152 Batch 70/159] avg loss 0.0174035, throughput 4.33643K wps\n",
      "[Epoch 152 Batch 80/159] avg loss 0.0348059, throughput 4.50865K wps\n",
      "[Epoch 152 Batch 90/159] avg loss 0.0254961, throughput 5.07846K wps\n",
      "[Epoch 152 Batch 100/159] avg loss 0.0259618, throughput 4.94073K wps\n",
      "[Epoch 152 Batch 110/159] avg loss 0.0327256, throughput 5.18733K wps\n",
      "[Epoch 152 Batch 120/159] avg loss 0.0382008, throughput 5.12906K wps\n",
      "[Epoch 152 Batch 130/159] avg loss 0.028473, throughput 5.2341K wps\n",
      "[Epoch 152 Batch 140/159] avg loss 0.0446175, throughput 5.25873K wps\n",
      "[Epoch 152 Batch 150/159] avg loss 0.046339, throughput 5.29749K wps\n",
      "[Epoch 152] train avg loss 0.0349304, train avg r2 0.291303,throughput 4.62263K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 153 Batch 10/159] avg loss 0.0450683, throughput 3.61954K wps\n",
      "[Epoch 153 Batch 20/159] avg loss 0.0367016, throughput 3.8058K wps\n",
      "[Epoch 153 Batch 30/159] avg loss 0.0199018, throughput 4.20521K wps\n",
      "[Epoch 153 Batch 40/159] avg loss 0.0224796, throughput 4.24217K wps\n",
      "[Epoch 153 Batch 50/159] avg loss 0.0226669, throughput 4.21008K wps\n",
      "[Epoch 153 Batch 60/159] avg loss 0.0235745, throughput 4.29421K wps\n",
      "[Epoch 153 Batch 70/159] avg loss 0.0174024, throughput 4.24893K wps\n",
      "[Epoch 153 Batch 80/159] avg loss 0.034801, throughput 4.43016K wps\n",
      "[Epoch 153 Batch 90/159] avg loss 0.025501, throughput 4.83697K wps\n",
      "[Epoch 153 Batch 100/159] avg loss 0.0259632, throughput 4.83731K wps\n",
      "[Epoch 153 Batch 110/159] avg loss 0.0327199, throughput 4.94256K wps\n",
      "[Epoch 153 Batch 120/159] avg loss 0.0381991, throughput 5.10498K wps\n",
      "[Epoch 153 Batch 130/159] avg loss 0.028472, throughput 5.17079K wps\n",
      "[Epoch 153 Batch 140/159] avg loss 0.044615, throughput 5.38783K wps\n",
      "[Epoch 153 Batch 150/159] avg loss 0.046337, throughput 5.46795K wps\n",
      "[Epoch 153] train avg loss 0.0349283, train avg r2 0.291755,throughput 4.56452K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 154 Batch 10/159] avg loss 0.0450715, throughput 3.67089K wps\n",
      "[Epoch 154 Batch 20/159] avg loss 0.0367083, throughput 4.05914K wps\n",
      "[Epoch 154 Batch 30/159] avg loss 0.0198957, throughput 4.15497K wps\n",
      "[Epoch 154 Batch 40/159] avg loss 0.022481, throughput 4.22986K wps\n",
      "[Epoch 154 Batch 50/159] avg loss 0.0226591, throughput 4.26601K wps\n",
      "[Epoch 154 Batch 60/159] avg loss 0.0235717, throughput 4.15853K wps\n",
      "[Epoch 154 Batch 70/159] avg loss 0.0173984, throughput 4.26632K wps\n",
      "[Epoch 154 Batch 80/159] avg loss 0.0348048, throughput 4.57113K wps\n",
      "[Epoch 154 Batch 90/159] avg loss 0.025498, throughput 4.90789K wps\n",
      "[Epoch 154 Batch 100/159] avg loss 0.02596, throughput 5.03486K wps\n",
      "[Epoch 154 Batch 110/159] avg loss 0.0327171, throughput 5.11302K wps\n",
      "[Epoch 154 Batch 120/159] avg loss 0.038192, throughput 5.13925K wps\n",
      "[Epoch 154 Batch 130/159] avg loss 0.0284708, throughput 5.37229K wps\n",
      "[Epoch 154 Batch 140/159] avg loss 0.0446136, throughput 4.57237K wps\n",
      "[Epoch 154 Batch 150/159] avg loss 0.0463344, throughput 4.8288K wps\n",
      "[Epoch 154] train avg loss 0.0349263, train avg r2 0.291727,throughput 4.53878K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 155 Batch 10/159] avg loss 0.0450709, throughput 3.48645K wps\n",
      "[Epoch 155 Batch 20/159] avg loss 0.0367084, throughput 3.95173K wps\n",
      "[Epoch 155 Batch 30/159] avg loss 0.0198962, throughput 4.02446K wps\n",
      "[Epoch 155 Batch 40/159] avg loss 0.0224789, throughput 4.16801K wps\n",
      "[Epoch 155 Batch 50/159] avg loss 0.0226653, throughput 4.14228K wps\n",
      "[Epoch 155 Batch 60/159] avg loss 0.0235697, throughput 4.22832K wps\n",
      "[Epoch 155 Batch 70/159] avg loss 0.0173993, throughput 4.27915K wps\n",
      "[Epoch 155 Batch 80/159] avg loss 0.034799, throughput 4.38208K wps\n",
      "[Epoch 155 Batch 90/159] avg loss 0.0254963, throughput 4.84402K wps\n",
      "[Epoch 155 Batch 100/159] avg loss 0.0259589, throughput 4.65895K wps\n",
      "[Epoch 155 Batch 110/159] avg loss 0.0327129, throughput 4.67545K wps\n",
      "[Epoch 155 Batch 120/159] avg loss 0.0381941, throughput 4.82107K wps\n",
      "[Epoch 155 Batch 130/159] avg loss 0.0284654, throughput 4.93486K wps\n",
      "[Epoch 155 Batch 140/159] avg loss 0.0446119, throughput 5.09895K wps\n",
      "[Epoch 155 Batch 150/159] avg loss 0.0463312, throughput 5.19385K wps\n",
      "[Epoch 155] train avg loss 0.0349248, train avg r2 0.291863,throughput 4.46189K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 156 Batch 10/159] avg loss 0.0450724, throughput 3.67819K wps\n",
      "[Epoch 156 Batch 20/159] avg loss 0.0367106, throughput 4.1128K wps\n",
      "[Epoch 156 Batch 30/159] avg loss 0.0198963, throughput 4.24903K wps\n",
      "[Epoch 156 Batch 40/159] avg loss 0.0224799, throughput 4.33103K wps\n",
      "[Epoch 156 Batch 50/159] avg loss 0.0226653, throughput 4.35025K wps\n",
      "[Epoch 156 Batch 60/159] avg loss 0.0235692, throughput 4.30019K wps\n",
      "[Epoch 156 Batch 70/159] avg loss 0.0173985, throughput 4.4306K wps\n",
      "[Epoch 156 Batch 80/159] avg loss 0.0347987, throughput 4.41038K wps\n",
      "[Epoch 156 Batch 90/159] avg loss 0.0254954, throughput 4.85326K wps\n",
      "[Epoch 156 Batch 100/159] avg loss 0.0259571, throughput 4.82214K wps\n",
      "[Epoch 156 Batch 110/159] avg loss 0.0327111, throughput 4.99375K wps\n",
      "[Epoch 156 Batch 120/159] avg loss 0.0381926, throughput 5.05461K wps\n",
      "[Epoch 156 Batch 130/159] avg loss 0.0284642, throughput 4.97472K wps\n",
      "[Epoch 156 Batch 140/159] avg loss 0.0446114, throughput 5.35618K wps\n",
      "[Epoch 156 Batch 150/159] avg loss 0.0463279, throughput 5.36096K wps\n",
      "[Epoch 156] train avg loss 0.034924, train avg r2 0.291905,throughput 4.61147K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 157 Batch 10/159] avg loss 0.0450635, throughput 3.6054K wps\n",
      "[Epoch 157 Batch 20/159] avg loss 0.0367017, throughput 4.06828K wps\n",
      "[Epoch 157 Batch 30/159] avg loss 0.0198993, throughput 4.1679K wps\n",
      "[Epoch 157 Batch 40/159] avg loss 0.0224779, throughput 3.9734K wps\n",
      "[Epoch 157 Batch 50/159] avg loss 0.0226631, throughput 4.21031K wps\n",
      "[Epoch 157 Batch 60/159] avg loss 0.0235677, throughput 4.31833K wps\n",
      "[Epoch 157 Batch 70/159] avg loss 0.017399, throughput 4.32934K wps\n",
      "[Epoch 157 Batch 80/159] avg loss 0.0348006, throughput 4.55216K wps\n",
      "[Epoch 157 Batch 90/159] avg loss 0.0254965, throughput 4.92025K wps\n",
      "[Epoch 157 Batch 100/159] avg loss 0.0259581, throughput 5.08173K wps\n",
      "[Epoch 157 Batch 110/159] avg loss 0.0327087, throughput 5.2297K wps\n",
      "[Epoch 157 Batch 120/159] avg loss 0.0381911, throughput 5.07985K wps\n",
      "[Epoch 157 Batch 130/159] avg loss 0.0284657, throughput 5.18841K wps\n",
      "[Epoch 157 Batch 140/159] avg loss 0.0446097, throughput 4.56036K wps\n",
      "[Epoch 157 Batch 150/159] avg loss 0.0463258, throughput 4.84304K wps\n",
      "[Epoch 157] train avg loss 0.0349222, train avg r2 0.291876,throughput 4.53334K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 158 Batch 10/159] avg loss 0.0450679, throughput 3.59418K wps\n",
      "[Epoch 158 Batch 20/159] avg loss 0.0367032, throughput 4.08527K wps\n",
      "[Epoch 158 Batch 30/159] avg loss 0.0198982, throughput 4.0907K wps\n",
      "[Epoch 158 Batch 40/159] avg loss 0.0224793, throughput 4.24739K wps\n",
      "[Epoch 158 Batch 50/159] avg loss 0.0226673, throughput 4.2401K wps\n",
      "[Epoch 158 Batch 60/159] avg loss 0.0235697, throughput 4.30315K wps\n",
      "[Epoch 158 Batch 70/159] avg loss 0.0173995, throughput 4.17091K wps\n",
      "[Epoch 158 Batch 80/159] avg loss 0.0348, throughput 4.59266K wps\n",
      "[Epoch 158 Batch 90/159] avg loss 0.0254965, throughput 4.8672K wps\n",
      "[Epoch 158 Batch 100/159] avg loss 0.0259592, throughput 4.81849K wps\n",
      "[Epoch 158 Batch 110/159] avg loss 0.0327061, throughput 5.05885K wps\n",
      "[Epoch 158 Batch 120/159] avg loss 0.0381959, throughput 4.95337K wps\n",
      "[Epoch 158 Batch 130/159] avg loss 0.0284618, throughput 5.00485K wps\n",
      "[Epoch 158 Batch 140/159] avg loss 0.044608, throughput 5.31888K wps\n",
      "[Epoch 158 Batch 150/159] avg loss 0.0463218, throughput 5.34212K wps\n",
      "[Epoch 158] train avg loss 0.0349222, train avg r2 0.291818,throughput 4.57376K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "[Epoch 159 Batch 10/159] avg loss 0.0450631, throughput 3.59299K wps\n",
      "[Epoch 159 Batch 20/159] avg loss 0.0367042, throughput 3.98114K wps\n",
      "[Epoch 159 Batch 30/159] avg loss 0.019899, throughput 4.20352K wps\n",
      "[Epoch 159 Batch 40/159] avg loss 0.022479, throughput 4.18395K wps\n",
      "[Epoch 159 Batch 50/159] avg loss 0.022666, throughput 4.22037K wps\n",
      "[Epoch 159 Batch 60/159] avg loss 0.0235715, throughput 4.22801K wps\n",
      "[Epoch 159 Batch 70/159] avg loss 0.0173996, throughput 4.31942K wps\n",
      "[Epoch 159 Batch 80/159] avg loss 0.0347971, throughput 4.52406K wps\n",
      "[Epoch 159 Batch 90/159] avg loss 0.0254941, throughput 5.10991K wps\n",
      "[Epoch 159 Batch 100/159] avg loss 0.0259609, throughput 4.86207K wps\n",
      "[Epoch 159 Batch 110/159] avg loss 0.0327034, throughput 5.06286K wps\n",
      "[Epoch 159 Batch 120/159] avg loss 0.0381933, throughput 5.126K wps\n",
      "[Epoch 159 Batch 130/159] avg loss 0.0284558, throughput 5.28281K wps\n",
      "[Epoch 159 Batch 140/159] avg loss 0.0446061, throughput 5.24676K wps\n",
      "[Epoch 159 Batch 150/159] avg loss 0.0463183, throughput 5.22629K wps\n",
      "[Epoch 159] train avg loss 0.0349204, train avg r2 0.291878,throughput 4.59722K wps\n",
      "learning rate: 3.0517578125e-08\n",
      "Total time cost 685.32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.03492036357834293, 0.2918775577668546)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs, learning_rate = 160,0.001                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "max_len = 100\n",
    "train(net, train_dataloader, train_batch_size, learning_rate, ctx, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross valid avg train loss 0.016455638436026907\n",
    "K-fold cross valid avg train r2 0.9453854297535305\n",
    "K-fold cross valid avg test loss 0.03263264888688952\n",
    "K-fold cross valid avg test r2 0.8201571726037102\n",
    "\n",
    "K-fold cross valid avg train loss 0.0171923089297629\n",
    "K-fold cross valid avg train r2 0.9405329953971364\n",
    "K-fold cross valid avg test loss 0.03243816043205651\n",
    "K-fold cross valid avg test r2 0.8260102191026778\n",
    "\n",
    "K-fold cross valid avg train loss 0.027404396245807428\n",
    "K-fold cross valid avg train r2 0.8223432808939917\n",
    "K-fold cross valid avg test loss 0.06469722731355083\n",
    "K-fold cross valid avg test r2 -0.3676552963800685"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
